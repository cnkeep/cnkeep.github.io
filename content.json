[{"title":"JVM参数配置打印gc日志","date":"2019-02-04T18:32:00.000Z","path":"2019/02/05/02-JVM参数配置打印gc日志/","text":"JVM参数配置设置打印gc日志 我们在排查内存问题时，经常需要知道系统的gc情况，如果需要输入gc信息到日志中，需要我们配置相关的jvm参数： 12-XX:+PrintGCDetails-Xloggc:gc1.log","tags":[{"name":"JVM","slug":"JVM","permalink":"https://cnkeep.github.io/tags/JVM/"}]},{"title":"jstack命令","date":"2019-02-04T16:09:00.000Z","path":"2019/02/05/02-jstack命令/","text":"JDK命令-jstack1. 介绍jstack用于生成当前java虚拟机当前时刻的线程快照。线程快照是当前虚拟机中每一个线程的执行情况，通过观察这些信息可以为我们分析程序异常提供帮助，例如：线程死锁等。 线程出现停顿的时候通过jstack来查看各个线程的调用堆栈，就可以知道没有响应的线程到底在后台做什么事情，或者等待什么资源。 2. 参数介绍1234567Usage: jstack [-options] &lt;pid&gt;Options: -F 当’jstack [-l] pid’没有响应的时候强制打印栈信息 -m 打印java和native c/c++框架的所有栈信息. -h | -help打印帮助信息 -l 长列表. 打印关于锁的附加信息,例如属于java.util.concurrent的ownable synchronizers列表 4. 案例使用4.1 案例1-cpu飙升问题排查 情景模拟12345678910111213141516171819202122232425import java.util.concurrent.Executor;import java.util.concurrent.Executors;public class CpuTask &#123; public static final Executor executor = Executors.newFixedThreadPool(2); public static final Object lock = new Object(); public static void main(String[] args) &#123; executor.execute(new Task()); executor.execute(new Task()); &#125; public static class Task implements Runnable &#123; @Override public void run() &#123; synchronized (lock) &#123; int i = 0; while (true) &#123; i++; &#125; &#125; &#125; &#125;&#125; 上面的程序会开启2个子线程，其中一个子线程中存在一个死循环(占用CPU), 另外一个线程会阻塞。12#javac CpuTask.java#java CpuTask &amp; 查看CPU占用 可以看到有一个pid为11775的Java进程高占CPU, 我们来看看进程内的线程情况： 我们发现是pid：11787的线程一直在占用CPU, 接着我们通过jstack看看线程的相关情况。 发现其中存在2个用户线程，其中一个阻塞，一个在运行，pid=0X2e0b(通过printf %d 0x2e0b转换为十进制), 与我们上一步发现高占CPU的线程是同一个，至此定位到了该线程，接下来就是反查代码了。","tags":[{"name":"JVM","slug":"JVM","permalink":"https://cnkeep.github.io/tags/JVM/"}]},{"title":"Jconsole无法连接程序","date":"2019-02-02T16:54:00.000Z","path":"2019/02/03/01-Jconsole无法连接程序/","text":"JConsole无法连接程序介绍我们在检测程序内存和线程使用情况时会使用到提供的jconsole工具，但是会出现连接不上的问题，需要我们设置启动参数：1234-Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.port=8011-Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.authenticate=false -XX:MaxPermSize=192M-Xms1000M-Xmx2000M-XX:+HeapDumpOnOutOfMemoryError","tags":[{"name":"JVM","slug":"JVM","permalink":"https://cnkeep.github.io/tags/JVM/"}]},{"title":"jps命令","date":"2019-02-02T14:58:00.000Z","path":"2019/02/02/01-jps命令/","text":"JDK命令-jps命令1. 介绍jps(Java Virtual Machine Process Status Tool)是JDK 1.5提供的一个显示当前所有java进程pid的命令，简单实用，非常适合在linux/unix平台上简单察看当前java进程的一些简单情况。 2. 参数介绍1234567# jps [options] [&lt;pid&gt;]Options: -q: 只显示pid，不显示class名称,jar文件名和传递给main 方法的参数 -m: 输出传递给main 方法的参数，在嵌入式jvm上可能是null -l: 输出应用程序main class的完整package名 或者 应用程序的jar文件完整路径名 -v: 输出传递给JVM的参数 3. 使用案例 查看当前运行的程序123[root@localhost conf]# jps11539 Jps11132 WebApplication","tags":[{"name":"JVM","slug":"JVM","permalink":"https://cnkeep.github.io/tags/JVM/"}]},{"title":"看懂UML类图","date":"2019-01-03T15:51:00.000Z","path":"2019/01/03/看懂UML类图/","text":"看懂UML类图前言&nbsp;&nbsp;作为一个程序员，除了基础的编码工作外，经常还需要输出各种各样的文档，例如：接口说明，系统设计等。其中作为系统设计的一部分，UML类图能直观的表达出各个类之间的关系，尤为重要，那么UML类图中中的图形，线条都是什么意思呢，今天我们就来学习一下，会看然后再用。 从实例开始举个例子，一家公司，有好几个开发部门，每个开发部门有多个程序员，每个程序员都有自己的身份证，通过PC工作。其UMl类图如下： 在这其中包含了如下关系： 实现关系，例如Person继承了Entity接口，即接口实现，采用空心箭头表示 泛化关系，例如Programmer是Person中的一类，即子类继承，采用实心箭头表示 关联关系，例如Programmer与IdCard的关联关系，即作为成员变量，采用实线箭头表示 依赖关系，例如Programmer工作依赖于Computer,但是可以随便换电脑， 即作为参数依赖，采用虚线箭头表示 聚合关系，例如Programmer聚合在一起成了部门，但是部门散了，程序员还在，部分不完全依赖于全局，可单独存在，用空心菱形表示 组合关系，例如部门组合成了公司，当公司倒闭，部门也就不在了，部分依赖于全局的存在, 不能单独存在，用实心菱形表示","tags":[{"name":"UML","slug":"UML","permalink":"https://cnkeep.github.io/tags/UML/"}]},{"title":"Redis设置开机自启","date":"2018-11-14T00:29:00.000Z","path":"2018/11/14/03-Redis设置开机自启/","text":"Redis设置开机自启 我们将Redis作为DB服务时，有时候会碰上关机的情况，及时这样我们也希望可以开机自启,本次我们就来学习如何是实现。 从docker引发的思考 1.设置docker开机自启12$ systemctl enable dockerCreated symlink from /etc/systemd/system/multi-user.target.wants/docker.service to /usr/lib/systemd/system/docker.service. 发现这里有两个目录：/etc/systemd/system/multi-user.target.wants/, /usr/lib/systemd/system/ 2./etc/systemd/system/multi-user.target.wants/ 该目录下存在许多软连接，指向/usr/lib/systemd/system/，开机时会扫描执行该目录下的脚本 3./usr/lib/systemd/system/ 真正的service单元配置文件都在这个目录下面，其中存在docker.service的文件：123456789101112131415161718192021222324252627282930313233[Unit]Description=Docker Application Container EngineDocumentation=https://docs.docker.comAfter=network-online.target firewalld.serviceWants=network-online.target[Service]Type=notify# the default is not to use systemd for cgroups because the delegate issues still# exists and systemd currently does not support the cgroup feature set required# for containers run by dockerExecStart=/usr/bin/dockerdExecReload=/bin/kill -s HUP $MAINPID# Having non-zero Limit*s causes performance problems due to accounting overhead# in the kernel. We recommend using cgroups to do container-local accounting.LimitNOFILE=infinityLimitNPROC=infinityLimitCORE=infinity# Uncomment TasksMax if your systemd version supports it.# Only systemd 226 and above support this version.#TasksMax=infinityTimeoutStartSec=0# set delegate yes so that systemd does not reset the cgroups of docker containersDelegate=yes# kill only the docker process, not all processes in the cgroupKillMode=process# restart the docker process if it exits prematurelyRestart=on-failureStartLimitBurst=3StartLimitInterval=60s[Install]WantedBy=multi-user.target 4.猜想 既然已经摸清了docker开机自启的猫腻，那么完全可以模仿实现redis的开机自启，按照以下步骤即可： 编写/user/lib/systemd/system/redis.service脚本 建立软连接至/etc/systemd/system/multi-user.target.wants/ 刷新配置，开启开启自启即可 设置开机自启 1.编写脚本1234567891011121314$ vi /lib/systemd/system/redis.service#写入以下内容[Unit]Description=Redis_5.0.1After=network.target[Service]#redis安装绝对路径ExecStart=/usr/local/app/redis-5.0.3/src/redis-server /usr/local/app/redis-5.0.3/redis.conf --daemonize noExecStop=/usr/local/app/redis-5.0.3/src/redis-cli -h 127.0.0.1 -p 6379 shutdown[Install]WantedBy=multi-user.target [Unit] 表示这是基础信息 Description 是描述 After 是在那个服务后面启动，一般是网络服务启动后启动 [Service] 表示这里是服务信息 ExecStart 是启动服务的命令 ExecStop 是停止服务的指令 [Install] 表示这是是安装相关信息 WantedBy 是以哪种方式启动：multi-user.target表明当系统以多用户方式（默认的运行级别）启动时，这个服务需要被自动运行。 详细请移步至：CoreOS实践指南（八）：Unit文件详解 2.设置开机启动12345678910#配置软连接$ ln -s /lib/systemd/system/redis.service /etc/systemd/system/multi-user.target.wants/redis.service#刷新配置$ systemctl daemon-reload#开启开机自启功能$ systemctl enable redis$ systemctl [start|stop|restart|status] redis `","tags":[{"name":"Redis","slug":"Redis","permalink":"https://cnkeep.github.io/tags/Redis/"}]},{"title":"Redis设置访问ip限制","date":"2018-11-12T19:55:00.000Z","path":"2018/11/13/02-Redis设置访问ip限制/","text":"Redis设置访问ip限制介绍鉴于Redis没有用户的概念，我们在实际生产中不能直接将其暴露在外网的环境下，普遍都是通过反向代理限制在内网访问，那么如何设置限制ip访问呢？ 1.仅限部署Redis服务器的客户端连接：12345#永久生效配置redis.conf, 设置protected-mode=true#当次生效，重启失效config set protected-mode=true 2.指定客户端信任ip列表：1配置redis.conf, 设置bind &lt;ipList 多个采用空格分隔&gt; 3.开启密码保护：1配置redis.conf, 设置requirepass &lt;pwd&gt; 4.效果：123456789101112131415161718192021222324#重启客户端访问后无法操作： 172.17.0.2:6379&gt; config get mode*(error) DENIED Redis is running in protected mode because protected mode is enabled, no bind address was specified, no authentication password is requested to clients. In this mode connections are only accepted from the loopback interface. If you want to connect from external computers to Redis you may adopt one of the following solutions: 1) Just disable protected mode sending the command &apos;CONFIG SET protected-mode no&apos; from the loopback interface by connecting to Redis from the same host the server is running, however MAKE SURE Redis is not publicly accessible from internet if you do so. Use CONFIG REWRITE to make this change permanent. 2) Alternatively you can just disable the protected mode by editing the Redis configuration file, and setting the protected mode option to &apos;no&apos;, and then restarting the server. 3) If you started the server manually just for testing, restart it with the &apos;--protected-mode no&apos; option. 4) Setup a bind address or an authentication password.NOTE: You only need to do one of the above things in order for the server to start accepting connections from the outside.#需要密码登录[root@localhost conf]# redis-cli 127.0.0.1:6379&gt; keys *(error) NOAUTH Authentication required.[root@localhost conf]# redis-cli127.0.0.1:6379&gt; auth java1024 #auth命令验证密码OK","tags":[{"name":"Redis","slug":"Redis","permalink":"https://cnkeep.github.io/tags/Redis/"}]},{"title":"Redis介绍与安装","date":"2018-11-12T16:22:00.000Z","path":"2018/11/13/01-Redis介绍与安装/","text":"Redis介绍与安装【目录】Redis介绍Redis安装&nbsp;&nbsp;压缩包安装&nbsp;&nbsp;Docker安装Redis Redis介绍&nbsp;&nbsp;Redis是当前最火的Nosql系统之一，它是一个key-value的存储系统，与其他Nosql不同的是它的优良性能以及多种数据结构的支持(只会在理论篇做对比介绍)。 Redis安装 相信大家已经对Redis做过相应了解了，那现在就来动手安装实践一下，本次安装分为:压缩包安装和Docker安装（推荐）。 压缩包安装 1.环境介绍12Centos 7Redis压缩包：http://download.redis.io/releases/redis-5.0.3.tar.gz 2.资源获取 参见Redis官网 3.配置分组和用户，默认目录(非必须)123456789# 这里我采用redis独立用户去操作，可以直接跳过$groupadd redis$useradd redis -g redis -d /home/redis -s /bin/bash$passwd redis更改用户 redis 的密码 。新的 密码：无效的密码： 密码少于 8 个字符重新输入新的 密码：passwd：所有的身份验证令牌已经成功更新。 4.安装编译12345678910111213141516171819$cd /home/redis$tar -zxvf redis-5.0.3.tar.gz $cd /home/redis/redis-5.0.3$make #编译$cd src$make install CC Makefile.depHint: It&apos;s a good idea to run &apos;make test&apos; ;) INSTALL install INSTALL install INSTALL install INSTALL install INSTALL install#建立软连接方便执行启动命令$ln -s /home/redis/redis-5.0.3/src/redis-cli /usr/sbin/redis-cli$ln -s /home/redis/redis-5.0.3/src/redis-server /usr/sbin/redis-server 5.修改配置12345678910111213141516$mkdir /home/redis/redis-5.0.3/data #用于存储数据$mkdir /home/redis/redis-5.0.3/conf #用于存储配置文件$mkdir /home/redis/redis-5.0.3/log #用于存储日志#copy一份配置文件$cp /home/redis/redis-5.0.3/redis.conf /home/redis/redis-5.0.3/conf/redis_template.conf #修改端口，可以跳过$sed s/6379/6389/g /home/redis/redis-5.0.3/conf/redis_template.conf&gt;/home/redis/redis-5.0.3/conf/redis_6389.conf$vi /home/redis/redis-5.0.3/conf/redis_6389.conf#配置日志项：logfile &quot;/home/redis/redis-5.0.3/logs/log&quot;#配置数据项：dir /home/redis/redis-5.0.3/data/#关闭保护：protected-mode no#配置信任ip：bind 172.16.*.*#配置密码：require redis@Pwd11 6.启动1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950#设置加载的配置文件，设置后台启动$redis-server /home/redis/redis-5.0.3/conf/redis_6389.conf &amp;#查看进程信息$ps -ef|grep redisroot 6550 2150 0 17:15 pts/0 00:00:00 redis-server 127.0.0.1:6389root 6555 2150 0 17:15 pts/0 00:00:00 grep --color=auto redis#查看启动日志$tail -n 100 -f /home/redis/redis-5.0.3/logs/log6483:C 28 Dec 2018 16:53:55.715 # oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo6483:C 28 Dec 2018 16:53:55.715 # Redis version=5.0.3, bits=64, commit=00000000, modified=0, pid=6483, just started6483:C 28 Dec 2018 16:53:55.715 # Configuration loaded6483:M 28 Dec 2018 16:53:55.716 * Increased maximum number of open files to 10032 (it was originally set to 1024). _._ _.-``__ &apos;&apos;-._ _.-`` `. `_. &apos;&apos;-._ Redis 5.0.3 (00000000/0) 64 bit .-`` .-```. ```\\/ _.,_ &apos;&apos;-._ ( &apos; , .-` | `, ) Running in standalone mode |`-._`-...-` __...-.``-._|&apos;` _.-&apos;| Port: 6389 | `-._ `._ / _.-&apos; | PID: 6550 `-._ `-._ `-./ _.-&apos; _.-&apos; |`-._`-._ `-.__.-&apos; _.-&apos;_.-&apos;| | `-._`-._ _.-&apos;_.-&apos; | http://redis.io `-._ `-._`-.__.-&apos;_.-&apos; _.-&apos; |`-._`-._ `-.__.-&apos; _.-&apos;_.-&apos;| | `-._`-._ _.-&apos;_.-&apos; | `-._ `-._`-.__.-&apos;_.-&apos; _.-&apos; `-._ `-.__.-&apos; _.-&apos; `-._ _.-&apos; `-.__.-&apos; 6483:M 28 Dec 2018 16:53:55.716 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.6483:M 28 Dec 2018 16:53:55.716 # Server initialized6483:M 28 Dec 2018 16:53:55.716 # WARNING overcommit_memory is set to 0! Background save may fail under low memory condition. To fix this issue add &apos;vm.overcommit_memory = 1&apos; to /etc/sysctl.conf and then reboot or run the command &apos;sysctl vm.overcommit_memory=1&apos; for this to take effect.6483:M 28 Dec 2018 16:53:55.717 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command &apos;echo never &gt; /sys/kernel/mm/transparent_hugepage/enabled&apos; as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.6483:M 28 Dec 2018 16:53:55.717 * DB loaded from disk: 0.000 seconds6483:M 28 Dec 2018 16:53:55.717 * Ready to accept connections6483:M 28 Dec 2018 16:56:31.287 # User requested shutdown...6483:M 28 Dec 2018 16:56:31.287 * Saving the final RDB snapshot before exiting.6483:M 28 Dec 2018 16:56:31.288 * DB saved on disk6483:M 28 Dec 2018 16:56:31.288 * Removing the pid file.6483:M 28 Dec 2018 16:56:31.288 # Redis is now ready to exit, bye bye...``` &gt; 7.客户端连接测试 ```text$redis-cli -h &lt;ip&gt; -p &lt;port&gt;127.0.0.1:6389&gt;keys *(empty list or set) 8.关闭redis1$redis-cli shutdown 以上redis就安装完毕了，快去使用吧！ Docker安装Redis 1.环境介绍1234567Centos 7Docker``` &gt; 2.获取资源 ```text$docker search redis$docker pull redis:latest 3.配置基础环境1234567891011121314151617181920212223242526272829303132$mkdir /home/redis/redis-5.0.3/data #用于存储数据$mkdir /home/redis/redis-5.0.3/conf #用于存储配置文件$mkdir /home/redis/redis-5.0.3/log #用于存储日志#copy一份配置文件, 配置文件可以从官方获取$cp /home/redis/redis-5.0.3/redis.conf /home/redis/redis-5.0.3/conf/redis_template.conf #修改端口，可以跳过$sed s/6379/6389/g /home/redis/redis-5.0.3/conf/redis_template.conf&gt;/home/redis/redis-5.0.3/conf/redis_6389.conf$vi /home/redis/redis-5.0.3/conf/redis_6389.conf#配置日志项：logfile &quot;/home/redis/redis-5.0.3/logs/log&quot;#配置数据项：dir /home/redis/redis-5.0.3/data/#关闭保护：protected-mode no#配置信任ip：bind 172.16.*.*#配置密码：require redis@Pwd11``` &gt; 4.启动 ```text$docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEredis latest c188f257942c 6 weeks ago 94.9MB#配置启动参数$docker run \\ #创建并启动容器--name redis_6379 \\#指定容器名称-p 6379:6379 \\#指定端口映射，宿主机：容器-privileged=true \\#给与权限-v /home/redis/redis-5.0.3/conf/redis_6389.conf:/etc/redis/redis.conf \\#配置文件映射-v /home/redis/redis-5.0.3/data:/etc/data \\#数据目录映射-d redis redis-server /etc/redis/redis.conf --appendonly yes#-d后台启动，并指定配置文件，AOF开启 5.验证启动1234567891011121314151617181920212223242526272829303132333435363738#查看日志$docker logs redis_63791:C 28 Dec 2018 17:40:44.775 # oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo1:C 28 Dec 2018 17:40:44.776 # Redis version=5.0.1, bits=64, commit=00000000, modified=0, pid=1, just started1:C 28 Dec 2018 17:40:44.776 # Configuration loaded _._ _.-``__ &apos;&apos;-._ _.-`` `. `_. &apos;&apos;-._ Redis 5.0.1 (00000000/0) 64 bit .-`` .-```. ```\\/ _.,_ &apos;&apos;-._ ( &apos; , .-` | `, ) Running in standalone mode |`-._`-...-` __...-.``-._|&apos;` _.-&apos;| Port: 6379 | `-._ `._ / _.-&apos; | PID: 1 `-._ `-._ `-./ _.-&apos; _.-&apos; |`-._`-._ `-.__.-&apos; _.-&apos;_.-&apos;| | `-._`-._ _.-&apos;_.-&apos; | http://redis.io `-._ `-._`-.__.-&apos;_.-&apos; _.-&apos; |`-._`-._ `-.__.-&apos; _.-&apos;_.-&apos;| | `-._`-._ _.-&apos;_.-&apos; | `-._ `-._`-.__.-&apos;_.-&apos; _.-&apos; `-._ `-.__.-&apos; _.-&apos; `-._ _.-&apos; `-.__.-&apos; 1:M 28 Dec 2018 17:40:44.780 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.1:M 28 Dec 2018 17:40:44.780 # Server initialized1:M 28 Dec 2018 17:40:44.780 # WARNING overcommit_memory is set to 0! Background save may fail under low memory condition. To fix this issue add &apos;vm.overcommit_memory = 1&apos; to /etc/sysctl.conf and then reboot or run the command &apos;sysctl vm.overcommit_memory=1&apos; for this to take effect.1:M 28 Dec 2018 17:40:44.780 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command &apos;echo never &gt; /sys/kernel/mm/transparent_hugepage/enabled&apos; as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.1:M 28 Dec 2018 17:40:44.792 * DB loaded from disk: 0.012 seconds1:M 28 Dec 2018 17:40:44.792 * Ready to accept connections#进入容器查看 [root@localhost conf]# docker exec -it redis_6379 /bin/bashroot@c6c3bf9cb27c:/data# redis-cli127.0.0.1:6379&gt; keys *(empty list or set) 127.0.0.1:6379&gt; info memory......127.0.0.1:6379&gt; info replication 结束语自此redis的安装就完成了，其中关于Docker的安装，以及防火墙的配置不做介绍，读者可以自行解决。Over!","tags":[{"name":"Redis","slug":"Redis","permalink":"https://cnkeep.github.io/tags/Redis/"}]},{"title":"explain查看执行计划","date":"2018-10-10T11:59:00.000Z","path":"2018/10/10/04-explain查看执行计划/","text":"Explain关键字查看执行计划1.是什么explain命令是mysql提供的一个查看sql执行计划的命令。 2.使用场景&nbsp;&nbsp;在工作中我们免不了会遇到各种SQL调优，SQL调优的第一步就是定位执行效率差的SQL，并判断为何执行效率差，这时候explain命令就派上用场了，可以帮我们查看sql的执行计划，例如是否使用了索引，是全表扫描还是索引扫描，这些都可以通过执行计划判断。 3.如何使用&nbsp;&nbsp;使用方式在sql语句前加上explain关键词即可。 4.参数详解&nbsp;&nbsp;知道用法当然没什么作用，我们得知道执行计划中各个参数表达的含义，才能对症下药，有针对性的优化SQL，接下来我们来看重要参数的含义：1234567891011121314151617181920212223242526272829303132mysql&gt; select version();+-----------+| version() |+-----------+| 5.6.26 |+-----------+drop database if exists 'test';create database test;use test;-- 用户表drop table if exisits `user`;create table `user`( id int primary key auto_increment, `name` varchar(20) not null comment '用户名')engine=innodb charset=utf8 comment '用户表';mysql&gt; explain select * from user where id=1\\G;*************************** 1. row *************************** id: 1 select_type: SIMPLE table: user partitions: NULL type: constpossible_keys: PRIMARY key: PRIMARY key_len: 4 ref: const rows: 1 filtered: 100.00 Extra: NULL explain命令输出的记过包含：id, select_type, table, type, possible_keys, key, key_len, ref, rows, filtered, extra 4.1 id 为一组数字，表示查询中执行select字句或者操作表的顺序。 123456789101112131415161718192021222324252627mysql&gt; explain select * from user a where a.id in (select id from user b where b.name like 'user%')\\G;*************************** 1. row *************************** id: 1 select_type: SIMPLE table: b partitions: NULL type: ALLpossible_keys: PRIMARY key: NULL key_len: NULL ref: NULL rows: 5 filtered: 20.00 Extra: Using where*************************** 2. row *************************** id: 1 select_type: SIMPLE table: a partitions: NULL type: eq_refpossible_keys: PRIMARY key: PRIMARY key_len: 4 ref: test_1.b.id rows: 1 filtered: 100.00 Extra: NULL 4.2select_type 表示select查询的类型 select_type属性有一下几种类型： SIMPLE：简单查询，该查询不包含 UNION 或子查询 PRIMARY：如果查询包含UNION 或子查询，则最外层的查询被标识为PRIMARY UNION：表示此查询是 UNION 中的第二个或者随后的查询 DEPENDENT：UNION 满足 UNION 中的第二个或者随后的查询，其次取决于外面的查询 UNION RESULT：UNION 的结果 SUBQUERY：子查询中的第一个select语句(该子查询不在from子句中) DEPENDENT SUBQUERY：子查询中的 第一个 select，同时取决于外面的查询 DERIVED：包含在from子句中子查询(也称为派生表) UNCACHEABLE SUBQUERY：满足是子查询中的第一个 select 语句，同时意味着 select 中的某些特性阻止结果被缓存于一个 Item_cache 中 UNCACHEABLE UNION：满足此查询是 UNION 中的第二个或者随后的查询，同时意味着 select 中的某些特性阻止结果被缓存于一个 Item_cache 中 4.3table 该列表示对应行正在访问的数据库表，存在别名时显示别名。 4.4type 这个参数表示关联类型或者访问类型，即MySQL决定采用何种策略查找表中的行，这也是我们调优时重点关注的列。 执行效率： const &gt; eq_ref &gt; ref &gt; range &gt; index &gt; ALL以下为常见取值： ALL: 全表扫描，这个类型是性能最差的查询之一，通常来讲我们的查询不应该出现ALL类型，因为这样的查询在数据量大的情况下对数据库的性能损耗是巨大的。 index: 全索引扫描，和ALl类型类似，只不过ALL类型是全表扫描。而 index 类型是扫描全部的索引，主要优点是避免了排序，但是开销仍然非常大。如果在 Extra 列看到 Using index，说明正在使用覆盖索引，只扫描索引的数据，它比按索引次序全表扫描的开销要少很多。 range: 范围查找，就是一个有限制的索引扫描，它开始于索引里的某一点，返回匹配这个值域的行。这个类型通常出现在 =、&lt;&gt;、&gt;、&gt;=、&lt;、&lt;=、ISNULL、&lt;=&gt;、BETWEEN、IN() 的操作中，key 列显示使用了哪个索引，当 type 为该值时，则输出的 ref 列为 NULL，并且 key_len列是此次查询中使用到的索引最长的那个。 ref：一种索引访问，也称索引查找，它返回所有匹配某个单个值的行。此类型通常出现在多表的 join 查询, 针对于非唯一或非主键索引, 或者是使用了最左前缀规则索引的查询。 eq_ref：使用这种索引查找，最多只返回一条符合条件的记录。在使用唯一性索引或主键查找时会出现该值，非常高效。 const、system：该表至多有一个匹配行，在查询开始时读取，或者该表是系统表，只有一行匹配。其中 const 用于在和 primary key 或 unique索引中有固定值比较的情形。 NULL: 在执行阶段不需要访问表。 4.5possible_keys 表示查询可能使用哪些索引来查找 4.6key 表示MySQL实际决定使用的索引。如果没有选择索引，键是NULL。 4.7key_len 表示在所引力使用的字节数，当key列为NULL时，此值为NULL 4.8ref 表示哪些字段或者常量被用来和key配合从表中查询记录 4.9rows 表示估计要找到所需的行而要读取的行数，这个值是个估计值，原则上值越小越好。 4.10extra 附加信息 常见的取值如下： Using index：使用覆盖索引，表示查询索引就可查到所需数据，不用扫描表数据文件，往往说明性能不错。 Using Where：在存储引擎检索行后再进行过滤，使用了where从句来限制哪些行将与下一张表匹配或者是返回给用户。 Using temporary：在查询结果排序时会使用一个临时表，一般出现于排序、分组和多表 join 的情况，查询效率不高，建议优化。 Using filesort：对结果使用一个外部索引排序，而不是按索引次序从表里读取行，一般有出现该值，都建议优化去掉，因为这样的查询 CPU 资源消耗大。","tags":[{"name":"mysql","slug":"mysql","permalink":"https://cnkeep.github.io/tags/mysql/"}]},{"title":"02_Mysql执行顺序","date":"2018-10-08T11:22:00.000Z","path":"2018/10/08/03-02_Mysql执行顺序/","text":"Mysql的执行顺序 Mysql的查询解析器，会对我们的sql进行解析生成一颗语法树, 进而判断语法错误，优化调整执行顺序，我们来看看Mysql是按照什么样的顺序来执行各个关键字的： (8) SELECT (9) DISTINCT &lt;select_list&gt; (1) FROM &lt;left_table&gt; (3) &lt;join_type&gt; JOIN &lt;right_table&gt; (2) ON &lt;join_condition&gt; (4) WHERE &lt;where_condition&gt; (5) GROUP BY &lt;group_by_list&gt; (6) WITH {CUBE|ROLLUP} (7) HAVING &lt;having_condition&gt; (10) ORDER BY &lt;order_by_list&gt; (11) LIMIT &lt;limit_number&gt; 可以看到执行的顺序如上面的编号，from，join, on, where, group by, having, select,distinct, order by, limit.每一个操作都会产生一张虚拟表，该虚拟表作为一个处理的输入。这些虚拟表对用户是透明的，只有最后一步生成的虚拟表才会返回给用户。","tags":[{"name":"mysql","slug":"mysql","permalink":"https://cnkeep.github.io/tags/mysql/"}]},{"title":"01_Mysql联接查询算法","date":"2018-10-07T06:47:00.000Z","path":"2018/10/07/03-01_Mysql联接查询算法/","text":"Mysql联接查询算法 转载：运维那点事-MySQL联接查询算法（NLJ、BNL、BKA、HashJoin） 前言&nbsp;&nbsp;这几天偶然发现了关于MySQL联接查询算法的文章，受益匪浅，在此转载记录，原文已置顶说明。 级联查询算法联接算法是MySQL数据库用于处理联接的物理策略。目前MySQL数据库仅支持Nested-Loops Join算法。而MySQL的分支版本MariaDB除了支持Nested-Loops Join算法外，还支持Classic Hash Join算法。当联接的表上有索引时，Nested-Loops Join是非常高效的算法。根据B+树的特性，其联接的时间复杂度为O(N)，若没有索引，则可视为最坏的情况，时间复杂度为O(N²)。 MySQL数据库根据不同的使用场合，支持两种Nested-Loops Join算法，一种是Simple Nested-Loops Join（NLJ）算法，另一种是Block Nested-Loops Join（BNL）算法。 上图的Fetch阶段是指当内表关联的列是辅助索引时，但是需要访问表中的数据，这是就需要回表(辅助索引叶子节点存储的是主键索引，需要回表查询)，无论表的存储引擎是Innodb还是MyISAM，这都是无法避免的，只是MyISAM的回表速度要快点，因为其辅助索引存放的就是指向记录的指针，而InnoDB存储引擎是索引组织表，需要再次通过索引查找才能定位数据。Fetch阶段也不是必须存在的，如果是聚集索引，那么子节点就存储着数据，无需回表。另外上述给出了两张表之间的join成本，多张表的join就是继续上述这个过程。 接着计算两张表的Join成本，这里有下列几种概念： 外表的扫描次数，记为O。通常外表（驱动表，数据量小）的扫描次数都是1，即Join时扫描一次驱动表的数据即可内表的扫描次数，记为I。根据不同的Join算法，内表的扫描次数不同读取表的记录数，记为R。根据不同Join算法，读取记录的数量可能不同Join的比较次数，记为M。根据不同Join算法，比较次数不同回表的读取记录的数，记为F。若Join的是辅助索引，可能需要回表取得最终的数据 评判一个Join算法是否优劣，就是查看上述这些操作的开销是否比较小。当让，这还要考虑I/O的访问方式，顺序 还是随机。总之Join的调优是门艺术。 Simple Nested-Loops Join (SNLJ)算法Simple Nested-Loops Join算法相当简单、直接。即外表（驱动表）中的每一条记录与内表中的记录进行比较判断。算法如下：12345678910111213141516171819202122232425For each row r in R do -- 扫描R表 For each row s in S do -- 扫描S表 If r and s satisfy the join condition -- 如果r和s满足join条件 Then output the tuple &lt;r, s&gt; -- 返回结果集``` 下图能更好地显示整个SNLJ的过程： ![](images/join_002.jpg) 其中R表为外部表（Outer Table），S表为内部表（Inner Table）。这是一个最简单的算法，这个算法的开销其实非常大。假设在两张表R和S上进行联接的列都不含有索引，外表的记录数为RN，内表的记录数位SN。根据上一节对于Join算法的评判标准来看，SNLJ的开销如下表所示： ![](images/join_003.png) 可以看到读取记录数的成本和比较次数的成本都是SN*RN，也就是笛卡儿积。假设外表内表都是1万条记录，那么其读取的记录数量和Join的比较次数都需要上亿。实际上数据库并不会使用到SNLJ算法。 ### Index Nested-Loops Join（INLJ）算法 SNLJ算法虽然简单明了，但是也是相当的粗暴。因此，在Join的优化时候，通常都会建议在内表建立索引，以此降低Nested-Loop Join算法的开销，MySQL数据库中使用较多的就是这种算法，以下称为INLJ。来看这种算法的伪代码： ```textFor each row r in R do -- 扫描R表 lookup s in S index -- 查询S表的索引（固定3~4次IO，B+树高度） If find s == r -- 如果r匹配了索引s Then output the tuple &lt;r, s&gt; -- 返回结果集 由于内表上有索引，所以比较的时候不再需要一条条记录进行比较，而可以通过索引来减少比较，从而加速查询。整个过程如下图所示： 可以可看到外表中的每一条记录通过内表的索引进行访问，即读取外行表的一行数据，然后去内部表索引进行二分法匹配；而一般的B+数的高度为3~4层，也就是或匹配一次的IO也就是3~4次，因此索引查询的成本是比较固定的，故优化器都倾向于使用记录数少的表作为外表。故INLJ的算法成本如下表所示： 上表Smatch表示通过索引找到匹配的记录数量。同时可以发现，通过索引可以大幅降低内表的Join的比较次数，每次比较1条外表的记录，其实就是一次indexlookup（索引查找），而每次index lookup的成本就是树的高度，即IndexHeight。 INLJ的算法并不复杂，也算简单易懂。但是效率是否能达到用户的预期呢？其实如果是通过表的主键索引进行Join，即使是大数据量的情况下，INLJ的效率亦是相当不错的。因为索引查找的开销非常小，并且访问模式也是顺序的（假设大多数聚集索引的访问都是比较顺序的）。 大部分人诟病MySQL的INLJ慢，主要是因为在进行Join的时候可能用到的索引并不是主键的聚集索引，而是辅助索引，这时INLJ的过程又需要多一步Fetch的过程，而且这个过程开销会相当的大： 由于访问的是辅助索引，如果查询需要访问聚集索引上的列，那么必要需要进行回表取数据，看似每条记录只是多了一次回表操作，但这才是INLJ算法 最大的弊端。首先，辅助索引的index lookup是比较随机I/O访问操作。其次，根据index lookup再进行回表又是一个随机的I/O操作。所以说， INLJ最大的弊端是其可能需要大量的离散操作，这在SSD出现之前是最大的瓶颈。而即使SSD的出现大幅提升了随机的访问性能， 但是对比顺序I/O，其还是慢了很多，依然不在一个数量级上。 另外，在INNER JOIN中，两张联接表的顺序是可以变换的，即R INNER JOIN S ON Condition P等效于S INNER JOIN R ON Condition P。 根据前面描述的Simple Nested-Loops Join算法，优化器在一般情况下总是选择将联接列含有索引的表作为内部表。如果两张表R和S在联接列上都有索引， 并且索引的高度相同，那么优化器会选择记录数少的表作为外部表，这是因为内部表的扫描次数总是索引的高度，与记录的数量无关。所以，联接列只要 有一个字段有索引即可，但最好是数据集多的表有索引；但是，但有WHERE条件的时候又另当别论了。 Block Nested-Loops Join（BNL）算法 在有索引的情况下，MySQL会尝试去使用Index Nested-Loop Join算法，在有些情况下，可能Join的列就是没有索引，那么这时MySQL的选择绝对不会 是最先介绍的Simple Nested-Loop Join算法，因为那个算法太粗暴，不忍直视。数据量大些的复杂SQL估计几年都可能跑不出结果。 而Block Nested-Loop Join算法较Simple Nested-Loop Join的改进就在于可以减少内表的扫描次数，甚至可以和Hash Join算法一样， 仅需扫描内表一次。其使用Join Buffer（联接缓冲）来减少内部循环读取表的次数。 12345For each tuple r in R do -- 扫描外表R store used columns as p from R in Join Buffer -- 将部分或者全部R的记录保存到Join Buffer中，记为p For each tuple s in S do -- 扫描内标S If p and s satisfy the join condition -- p与s满足join条件 Then output the tuple -- 返回为结果集 可以看到相比Simple Nested-Loop Join算法，Block Nested-LoopJoin算法仅多了一个所谓的Join Buffer，为什么这样就能减少内表的扫描次数呢？下图相比更好地解释了Block Nested-Loop Join算法的运行过程： 可以看到Join Buffer用以缓存联接需要的列，然后以Join Buffer批量的形式和内表中的数据进行联接比较。就上图来看，记录r1，r2 … rT的联接仅需扫内表一次，如果join buffer可以缓存所有的外表列，那么联接仅需扫描内外表各一次，从而大幅提升Join的性能。 Mysql数据库使用Join Buffer的原则如下： 系统变量Join_buffer_size决定了Join Buffer的大小。 Join Buffer可被用于联接是ALL、index、和range的类型。 每次联接使用一个Join Buffer，因此多表的联接可以使用多个Join Buffer。 Join Buffer在联接发生之前进行分配，在SQL语句执行完后进行释放。 Join Buffer只存储要进行查询操作的相关列数据，而不是整行的记录。 Batched Key Access Join（BKA）算法Index Nested-Loop Join虽好，但是通过辅助索引进行联接后需要回表，这里需要大量的随机I/O操作。若能优化随机I/O，那么就能极大的提升Join的性能。为此，MySQL 5.6（MariaDB 5.3）开始支持Batched Key Access Join算法（简称BKA），该算法通过常见的空间换时间，随机I/O转顺序I/O，以此来极大的提升Join的性能。 在说明Batched Key Access Join前，首先介绍下MySQL 5.6的新特性mrr——multi range read。因为这个特性也是BKA的重要支柱。MRR优化的目的就是为了减少磁盘的随机访问，InnoDB由于索引组织表的特性，如果你的查询是使用辅助索引，并且有用到表中非索引列（投影非索引字段，及条件有非索引字段），因此需要回表读取数据做后续处理，过于随机的回表会伴随着大量的随机I/O。这个过程如下图所示： 而mrr的优化在于，并不是每次通过辅助索引读取到数据就回表去取记录，范围扫描（range access）中MySQL将扫描到的数据存入read_rnd_buffer_size，默认256K。然后对其按照Primary Key（RowID）排序，然后使用排序好的数据进行顺序回表，因为我们知道InnoDB中叶子节点数据是按照PRIMARY KEY（ROWID）进行排列的，那么这样就转换随机读取为顺序读取了。这对于IO-bound类型的SQL查询语句带来性能极大的提升。 MRR优化可用于range，ref，eq_ref类型的查询，工作方式如下图： 即增加一次排序，将回表的随机IO改为顺序IO,从而提升性能，其算法伪代码如下：123456For each tuple r in R do -- 扫描外部表R store used columns as p from R in join buffer -- 将部分或者全部R的记录保存到Join Buffer中，记为p For each tuple s in S do -- If p and s satisfy the join condition use mrr inter face to sort row Id Then output the tuple &lt;p,s&gt; 总结 Join时，扫描外表(驱动表)一次，然后和内表进行匹配 当关联键为主键索引时，与内表匹配走索引(非全表扫描)，减少IO,提升性能 当关联表为辅助索引时，引入缓存区，将关联查询的列存储在缓存区，批量与内表匹配，减少IO,提升性能（但是依旧无法避免因为回表而导致的随机IO问题） 当关联表为辅助索引时，引入缓存区，在上面的基础上在回表之前将主键索引排序后，随机IO转为顺序IO提升性能 一般选取记录数量小的的表做外表，关联键为索引的表做内表(最好是主键索引，比较次数就会稳定在3~4次)","tags":[{"name":"mysql","slug":"mysql","permalink":"https://cnkeep.github.io/tags/mysql/"}]},{"title":"03_Innodb引擎RR下如何避免幻读","date":"2018-10-07T03:13:00.000Z","path":"2018/10/07/02-03_Innodb引擎RR下如何避免幻读/","text":"Innodb引擎RR下如何避免幻读前言&nbsp;&nbsp;我们知道Innodb默认的事务隔离级别是Repeatable-Read, 这种隔离级别下解决了不可重复读的问题，同时部分解决了幻读的问题，这次我们就来谈论它是如何避免幻读的。关键词：锁，MVCC, Next-key Lock 基础概念排它锁(X锁)和共享锁(S锁) 所谓X锁,是事务T对数据A加上X锁时,只允许事务T读取和修改数据A,类似于写锁； 1select * from tableName where ... for update; 所谓S锁,是事务T对数据A加上S锁时,其他事务只能再对数据A加S锁,而不能加X锁,直到T释放A上的S锁，类似于读锁；1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253select * from tableName where ... + lock in share mode;``` ### 行锁和表锁 &gt; 行锁，小粒度锁，只锁部分数据行 &gt; 表锁，大粒度锁，锁定整张数据表 **对比**： | 对比 | 行锁 | 表锁 | | :--: | :--: | :--: | | 优势 | 粒度小，并发能力强，锁冲突概率低|操作简单，开销小 | | 劣势 | 开销大，加锁满，会出现死锁|并发能力弱 | ### 行锁(Record Lock)和间隙锁(Gap Lock)，Next-Key Lock * **行锁**：锁定当前行，行锁锁定的是``索引``，而不是行数据，也就是针对索引加的锁，不是针对记录加的锁。所以当做更新删除操作时，检索条件不是索引，则都会从行锁升级为表锁。**特例，及时使用了索引，但是索引值不存在时会升级为间隙锁。** * **间隙锁**：锁定索引记录间隙，确保索引记录的间隙不变。间隙锁是针对事务隔离级别为可重复读或以上级别而已的，（无论是S还是X）``只会阻塞insert操作。``另一个事务通过范围条件加锁时，如果使用相等条件请求给一个不存在的记录加锁都会使用间隙锁。* **Next-Key Lock**: 行锁和间隙锁和组合，首先对选中的索引记录加上行锁（Record Lock），再对索引记录两边的间隙（向左扫描扫到第一个比给定参数小的值， 向右扫描扫描到第一个比给定参数大的值， 然后以此为界，构建一个区间）加上间隙锁（Gap Lock）。如果一个间隙被事务T1加了锁，其它事务是不能在这个间隙内插入记录的。Innodb引擎采用这种方式来避免当前出现的幻读。**当查询的索引含有唯一属性时，将next-key lock降级为record key** ### 快照读和当前读 * 快照读：即普通的select操作,读取的是历史快照数据,利用MVCC模式实现。 * 当前读：select时指定了锁，for update，lock in share mode。读取的是最新的数据。 ### MVCC(多版本控制) &amp;nbsp;&amp;nbsp;Mysql的Innodb引擎在Repeatable-Read隔离级别下采用MVCC不加锁的方式来代替加锁操作。 #### 实现方式 &amp;nbsp;&amp;nbsp;Innodb的MVCC, 是通过在每行记录后面保存两个隐藏的列来实现的，这两个列分别保存了这行记录的创建时间和删除时间。但在实际的实现里存储的当前的系统版本号(可以理解为事务的ID)。一旦开始一个新的事务，系统的版本号就会递增，事务开始时刻的系统版本号会作为事务的ID,为了避免幻读查询时只会查询创建版本号小于当前事务版本号，删除版本号大于当前事务版本号(保证事务开始前该记录未删除)的记录。 ### 实例分析 ```sqlmysql&gt; select version();+-----------+| version() |+-----------+| 5.6.26 |+-----------+drop database if exists 'test';create database test;use test;-- 用户表drop table if exisits `user`;create table `user`( id int primary key auto_increment, `name` varchar(20) not null comment '用户名')engine=innodb charset=utf8 comment '用户表'; insert&nbsp;&nbsp;Innodb为插入的每一行记录保存当前系统的版本号作为创建版本号，查询只会查询出创建版本号小于当前事务id的记录。12345678910111213141516171819begin;insert into user(name) values('user_01');insert into user(name) values('user_02');insert into user(name) values('user_03');commit;``` 我们看看实际的存储情况： | id | name | 创建版本号 | 删除版本号 | |:--:|:--:|:--:|:--:| | 1 | user_01 | 1 | undefined | | 2 | user_02 | 1 | undefined | | 3 | user_03 | 1 | undefined | #### delete &amp;nbsp;&amp;nbsp;InnoDB会为删除的每一行保存当前系统的版本号(事务的ID)作为删除标识，只会查询出删除版本号大于当前事务版本号(保证事务开始前该记录未删除)的记录。 ![delete](理论类/images/tx_mvcc_delete.png) 步骤2：事务1查询结果为三条记录；步骤3：事务2删除id=2的记录；步骤4：事务1查询结果为三条记录；这是事务2未提交步骤5：事务2提交；步骤6：事务1查询结果依旧为三条记录；事务1提交后查询结果为最新数据1234567891011121314151617181920212223我们看看事务2提交后实际的存储情况： | id | name | 创建版本号 | 删除版本号 | |:--:|:--:|:--:|:--:| | 1 | user_01 | 1 | undefined | | 2 | user_02 | 1 | 3 | | 3 | user_03 | 1 | undefined | 因为查询时会查询出来删除版本号大于当前事务号的记录，所以事务2提交了删除，事务1还是保证了重复读。#### select InnoDB会根据以下两个条件检查每行记录: a.InnoDB只会查找版本早于当前事务版本的数据行(也就是,行的系统版本号小于或等于事务的系统版本号)，这样可以确保事务读取的行，要么是在事务开始前已经存在的，要么是事务自身插入或者修改过的. b.行的删除版本要么未定义,要么大于当前事务版本号,这可以确保事务读取到的行，在事务开始之前未被删除. 只有a,b同时满足的记录，才能返回作为查询结果. #### update InnoDB执行UPDATE，实际上是新插入了一行记录，并保存其创建时间为当前事务的ID，同时保存当前事务ID到要UPDATE的行的删除时间. ```sqlbegin; //当前事务id=4update user set name='user_3' where id=3;commit; 实际的存储情况如下所示： id name 创建版本号 删除版本号 1 user_01 1 undefined 2 user_02 1 3 3 user_03 1 4 3 user_3 5 undefined 结论有了MVCC就可以在不加锁的情况下读取数据，同时采用乐观更新的策略提高并发度。 Innodb在Repeatable-Read隔离级别下如何避免幻读 在快照读情况下，mysql通过mvcc来避免幻读。 在当前读情况下，mysql通过next-key lock来避免幻读。","tags":[{"name":"mysql","slug":"mysql","permalink":"https://cnkeep.github.io/tags/mysql/"}]},{"title":"02_Innodb存储引擎事务的隔离级别","date":"2018-10-06T00:58:00.000Z","path":"2018/10/06/02-02_Innodb存储引擎事务的隔离级别/","text":"Innodb存储引擎事务的隔离级别","tags":[{"name":"mysql","slug":"mysql","permalink":"https://cnkeep.github.io/tags/mysql/"}]},{"title":"Mysql主从同步","date":"2018-10-05T23:37:00.000Z","path":"2018/10/06/04-Mysql主从同步/","text":"Mysql主从同步 标签：Mysql, 主从同步 1. 介绍1.1 主从复制概念Mysql主从复制是指数据可以从一个Mysql数据库主节点复制到所有的从节点中，从而保证所有节点的数据一致性。 1.2 主从复制主要的用途 读写分离 业务中大部分的场景是读多写少，然而写操作需要锁表，导致读操作阻塞，从而降低了系统的吞吐量，假如可以读写分离，写操作在主节点，读操作在从节点，在利用同步机制保证数据的同步，这样便可以提高系统的处理能力。 高可用 2. 原理介绍当master服务器的数据发生改变时，会将其记录进二进制文件binlog日志中，salve服务器会在一定时间间隔内对master二进制日志进行探测其是否发生改变，如果发生改变，则开始一个I/OThread请求master二进制事件，同时主节点为每个I/O线程启动一个dump线程，用于向其发送二进制事件，并保存至从节点本地的中继日志中，从节点将启动SQL线程从中继日志中读取二进制日志，在本地重放，使得其数据和主节点的保持一致，最后I/OThread和SQLThread将进入睡眠状态，等待下一次被唤醒。流程图如下： 2.1 复制方式mysql复制主要有三种方式： 基于SQL语句的复制(statement-based replication) 基于行的复制(row-based replication) 混合模式复制(mixed-based replication)。 对应的，binlog的格式也有三种：STATEMENT，ROW，MIXED。 STATEMENT模式 每一条会修改数据的sql语句会记录到binlog中。优点是并不需要记录每一条sql语句和每一行的数据变化，减少了binlog日志量，节约IO，提高性能。缺点是在某些情况下会导致master-slave中的数据不一致(如sleep()函数， last_insert_id()，以及user-defined functions(udf)等会出现问题) ROW模式（RBR） 不记录每条sql语句的上下文信息，仅需记录哪条数据被修改了，修改成什么样了。而且不会出现某些特定情况下的存储过程、或function、或trigger的调用和触发无法被正确复制的问题。缺点是会产生大量的日志，尤其是alter table的时候会让日志暴涨。 MIXED模式（MBR） 以上两种模式的混合使用，一般的复制使用STATEMENT模式保存binlog，对于STATEMENT模式无法复制的操作使用ROW模式保存binlog，MySQL会根据执行的SQL语句选择日志保存方式。 3. 环境搭建3.1 服务器配置 Ip 配置 172.16.22.135 Centos7 2CPU 2G内存 40G硬盘, Mysql5.7.17 172.16.22.136 Centos7 2CPU 2G内存 40G硬盘, Mysql5.7.17 本次模拟搭建一主一从结构 3.2 Master配置3.2.1 配置文件修改12345678910111213141516171819202122232425262728293031323334353637# vi /etc/my.cnf #修改配置文件[mysqld]#数据存储目录datadir=/var/lib/mysqlsocket=/var/lib/mysql/mysql.sock#忽略大小写lower_case_table_names=1port = 3306#error日志log-error=/var/log/mysqld.logpid-file=/var/run/mysqld/mysqld.pid#生成binlog日志log-bin=/var/lib/mysql/mysql-bin#服务ID，用于区分服务，范围1~2^32-1,需要与从服务器不同server_id=135#MySQL 磁盘写入策略以及数据安全性#每次事务提交时MySQL都会把log buffer的数据写入log file，并且flush(刷到磁盘)中去innodb_flush_log_at_trx_commit=1#当sync_binlog =N (N&gt;0) ，MySQL 在每写 N次 二进制日志binary log时，会使用fdatasync()函数将它的写二进制日志binary log同步到磁盘中去。#sync_binlog 的默认值是0，像操作系统刷其他文件的机制一样，MySQL不会同步到磁盘中去而是依赖操作系统来刷新binary log。sync_binlog=1#同步数据库，如果多库，就以此格式另写几行即可binlog-do-db=test_0001#无需同步的数据库binlog-ignore-db=mysqlbinlog-ignore-db=performance_schemabinlog-ignore-db=information_schema#mysql复制模式binlog_format=ROW#binlog过期清理时间expire_logs_days=7#binlog每个日志文件大小max_binlog_size=20M 3.2.2 重启服务1234567# systemctl restart mysqld# mysql -uroot -p123456 -e &apos;show master status&apos;+------------------+----------+--------------+------------------+-------------------+| File | Position | Binlog_Do_DB | Binlog_Ignore_DB | Executed_Gtid_Set |+------------------+----------+--------------+------------------+-------------------+| mysql-bin.000001 | 573 | | | |+------------------+----------+--------------+------------------+-------------------+ 记录下上面查询出来的File和Position 3.2.3 创建并授权用户123456# mysql -uroot -ppassword:mysql&gt; GRANT REPLICATION SLAVE ON *.* TO &apos;root&apos;@&apos;172.16.22.%&apos; IDENTIFIED BY &apos;123456&apos; WITH GRANT OPTION;mysql&gt; FLUSH PRIVILEGES;mysql&gt; FLUSH tables with read lock; #锁定数据库为只读，确保备份数据一致性 3.2.4 备份数据库123# mysqldump -u root -p --master-data test_0001&gt; dbdump.sql #备份test_0001数据库# 备份所有数据库采用mysqldump -u root -p --all-databases --master-data &gt; dbdump.sql 将dbdump.sql在从服务器执行，然后在主服务器执行mysql -uroot -p123456 -e &#39;unlock tables&#39; 3.3 Slave配置3.3.1 配置文件修改12345678910111213141516171819202122232425262728293031#从库日志记录文件位置或名称前缀log-bin=/var/lib/mysql/mysql-binrelay_log=/var/lib/mysql/mysql-relay-bin#同步日志记录的频率，1为每条都记录，安全但效率低sync_binlog=1#server的id，不能与相同id的mysql主从连接server-id=136#从库日志忽略的数据库名称，不记录#这里记录从库的binlog是为了安全，如果觉得没必要，可以去掉从库binlog的配置binlog-ignore-db = mysqlbinlog-ignore-db = performance_schemabinlog-ignore-db = information_schema#此处添加需要同步的数据库名称，那么它会只接收这个数据库的信息，多个数据库需同步按照此格式另写几行即可#这里同步数据有两种思路，一种是主服务器只发从库需要的，在主库指定；一种是主服务器把所有数据同步过来，从库按需过滤接收#为了让配置更详细些，此处配置了从库过滤接收的配置replicate-do-db=test_0001#忽略接收的库名replicate-ignore-db=mysqlreplicate-ignore-db=performance_schemareplicate-ignore-db=information_schema#跳过所有错误继续slave-skip-errors=all#设置延时时间slave-net-timeout=60#mysql复制模式binlog_format=ROW#binlog过期清理时间expire_logs_days=7#binlog每个日志文件大小max_binlog_size=20M 3.3.2 重启1# systemctl restart mysqld 3.3.3 配置Master位置12345678910# mysql -uroot -ppassword:mysql&gt; CHANGE MASTER TO MASTER_HOST=&apos;172.16.22.135&apos;, MASTER_USER=&apos;repl&apos;, MASTER_PASSWORD=&apos;repl&apos;, MASTER_PORT=3306, MASTER_LOG_FILE=&apos;mysql-bin.000001&apos;, MASTER_LOG_POS= 573, MASTER_CONNECT_RETRY=30;# 注意上面的master二进制文件名和pos要和master信息一致 3.3.4 启动slave123#启动slave进程mysql&gt; slave start;mysql&gt; show slave statue\\G; 可以看到启动成功，如果此步失败，请排查网络问题，或者查看出错日志/var/log/mysqld.log, /var/log/message 3.4 测试在master上新建表，插入数据，观察slave数据库中是否同步了数据。123456#master上执行mysql&gt; show slave hosts;mysql&gt; show mater status;#slave上执行，对比master上的binlog信息mysql&gt; show slave status\\G; 4. 彩蛋4.1 Error-UUID重复发现原来是Mysql的一个配置文件auto.cnf里面记录了mysql服务器的uuid。 server_uuid:服务器身份ID。在第一次启动Mysql时，会自动生成一个server_uuid并写入到数据目录下auto.cnf文件里。原来是这个uuid和主服务器的uuid重复了。经过修改auto.cnf文件中的server-uuid，重启mysql服务器，再查看mysql从节点的状态，终于成功了。 4.2 命令拓展123mysql&gt; stop slave; #停止mysql&gt; reset slave; #重置mysql&gt; delete from user Where user=&apos;repl&apos; and host=&apos;172.16.22.%&apos;; #删除主服务器配置的连接slave用户","tags":[{"name":"mysql","slug":"mysql","permalink":"https://cnkeep.github.io/tags/mysql/"}]},{"title":"Mysql解决断电启动失败问题","date":"2018-10-05T23:24:00.000Z","path":"2018/10/06/03-Mysql解决断电启动失败问题/","text":"Mysql解决断电启动失败的问题现象还原 Windows上运行着Mysql, 而且是开机自启 有一天正在写代码，突然断电了…………… 电来了，开机，MySQL启动失败，mmp………123C:\\WINDOWS\\system32&gt;net start mysqlMySQL 服务正在启动 ........MySQL 服务无法启动。 问题排查1. 查看启动日志启动日志在数据存放目录下(配置文件中指定的datadir目录)，一个后缀为.err的文件, 查看发现了如下的错误: 发现原来是我之前手动删除过data目录下的数据(不要学我), 2. 解决方案 1.停止mysql12345C:\\WINDOWS\\system32&gt;tasklist|findstr &quot;mysql&quot;mysqld.exe 6744 Services 0 378,652 KC:\\WINDOWS\\system32&gt;taskkill /f -pid 6744成功: 已终止 PID 为 6744 的进程。 3.设置recoveryhttps://blog.csdn.net/lyhdream/article/details/787464392.删除文件 删除data目录下的ib_logfile0, ib_logfile1, ibtmp1三个文件，重启数据库123C:\\WINDOWS\\system32&gt;net start mysqlMySQL 服务正在启动 ...MySQL 服务已经启动成功。 3.刷新数据1mysqladmin -u root -p flush-tables","tags":[{"name":"mysql","slug":"mysql","permalink":"https://cnkeep.github.io/tags/mysql/"}]},{"title":"隐藏版本号","date":"2018-10-05T01:23:00.000Z","path":"2018/10/05/03-隐藏版本号/","text":"隐藏Nginx的版本号介绍处于安全考虑，在系统中的Nginx需要隐藏其自身的版本号，防止被不法分子获取后进行攻击。本次我们就来了解下如何隐藏版本号。 操作12345678# vi /usr/local/nginx/conf/nginx.conf# 在http模块配置中增加server_tokens off;即可http &#123; #隐藏nginx版本信息 server_tokens off;&#125;# nginx -s reload 重载配置后，访问原有页面就发现版本号隐藏了。","tags":[{"name":"Nginx","slug":"Nginx","permalink":"https://cnkeep.github.io/tags/Nginx/"}]},{"title":"设置开机启动项","date":"2018-10-04T21:51:00.000Z","path":"2018/10/05/02-设置开机启动项/","text":"设置开机启动项要是想让nginx开机自启我们就需要进行相关配置了，通过yum安装的无需配置，只需要设置systemctl enable nginx即可，本次我们讨论的是通过压缩包安装的方式实现开机自启的方法。 编写nginx.service脚本 1234567891011121314151617$ vi /user/lib/systemc/system/nginx.service[Unit]Description=Nginx ServiceAfter=network.target[Service]#后台形式运行Type=forking#nginx安装绝对路径ExecStart=/usr/local/nginx/sbin/nginxExecStop=/usr/local/nginx/sbin/nginx -s stop#分配独立的临时空间PrivateTmp=true[Install]WantedBy=multi-user.target 2.建立软连接，映射到启动项目录1234567$ ln -s /usr/lib/systemd/system/nginx.service /etc/systemd/system/multi-user.target.wants/nginx.service ``` &gt; 3.系统配置刷新，开启启动项 ```text$ systemctl daemon-reload$ systemctl [start|stop|status] nginx","tags":[{"name":"Nginx","slug":"Nginx","permalink":"https://cnkeep.github.io/tags/Nginx/"}]},{"title":"安装nginx","date":"2018-10-04T18:27:00.000Z","path":"2018/10/05/01-安装nginx/","text":"Nginx是一款轻量级的网页服务器、反向代理服务器。相较于Apache、lighttpd具有占有内存少，稳定性高等优势。它最常的用途是提供反向代理服务。 在Centos下，yum源不提供nginx的安装，可以通过切换yum源的方法获取安装。目前很多像centos7系统已经自带这几个库，所以安装前可以先查看一下本地是否已经存在。存在可直接跳至第四步骤。需要使用安装包编译安装的，如下。以下命令均需root权限执行：首先安装必要的库（nginx 中gzip模块需要 zlib 库，rewrite模块需要 pcre 库，ssl 功能需要openssl库）。选定/usr/local为安装目录，以下具体版本号根据实际改变。 1.安装依赖库12# cd /usr/local/# yum -y install make gcc gcc-c++ zlib zlib-devel openssl openssl-devel pcre pcre-devel 2.安装nginx1234567# cd /usr/local/# sudo tar -zxvf nginx-1.8.0.tar.gz# cd nginx-1.8.0 # sudo ./configure --prefix=/usr/local/nginx #prefix指明安装位置,如果是使用安装包编译的上面几个依赖，需要在在--prefix后面接以下命令:--with-pcre=**** --with-zlib。# sudo make# sudo make install# sudo ln -s /usr/local/nginx/sbin/* /usr/local/sbin 设置软连接，使得可以再任意路径执行nginx命令 3.启动先测试一下配置文件是否正确：12345678910111213141516# /usr/local/nginx/sbin/nginx -t无问题可以启动：# /usr/local/nginx/sbin/nginx检查是否启动成功：打开浏览器访问此机器的 IP，如果浏览器出现 Welcome to nginx! 则表示 Nginx 已经安装并运行成功。部分命令如下：重启：# /usr/local/nginx/sbin/nginx –s reload停止：# /usr/local/nginx/sbin/nginx –s stop测试配置文件是否正常：# /usr/local/nginx/sbin/nginx –t强制关闭：# pkill nginx配置 以上安装方法nginx的配置文件位于123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051## 4.简单的示例 ```text server &#123; listen 5100; server_name localhost; set $root_path &quot;/home/wwwroot&quot;; #charset koi8-r; access_log /home/wwwlogs/host.access.log main; error_log /home/wwwlogs/error.log; location / &#123; root $root_path; index index.html index.htm; &#125; # proxy the Java scripts to JDK listening on 127.0.0.1:9000 location ~ ^/service/ &#123; proxy_set_header Host $http_host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header Cookie $http_cookie; proxy_cookie_path /service/ /; proxy_buffering off; proxy_connect_timeout 300s; proxy_send_timeout 300s; proxy_read_timeout 300s; proxy_pass http://127.0.0.1:9000; &#125; #error_page 404 /404.html; # redirect server error pages to the static page /50x.html # error_page 500 502 503 504 /50x.html; location = /50x.html &#123; root /usr/share/nginx/html; &#125; # deny access to .htaccess files, if Apache&apos;s document root # concurs with nginx&apos;s one # #location ~ /\\.ht &#123; # deny all; #&#125; &#125;","tags":[{"name":"Nginx","slug":"Nginx","permalink":"https://cnkeep.github.io/tags/Nginx/"}]},{"title":"01_存储引擎的分类","date":"2018-10-03T21:47:00.000Z","path":"2018/10/04/02-01_存储引擎的分类/","text":"","tags":[{"name":"mysql","slug":"mysql","permalink":"https://cnkeep.github.io/tags/mysql/"}]},{"title":"03_索引的建立&优化","date":"2018-10-03T20:19:00.000Z","path":"2018/10/04/01-03_索引的建立&优化/","text":"索引的建立&amp;优化建索引的几大规则 查询中与其他表关联的字段，外键关系建立索引; 频繁更新的字段不适合创建索引，因为每次更新还需要更新索引文件; 数据量大于500W或者单表数据大于2G时，此时索引的效果已经不明显了，这是就改考虑分库分表了; 尽量选择区分度高的列作为索引，区分度的公式为：count(distinct &lt;col&gt;)/count(*), 表示字段不重复的额比例，比例越大扫描的记录数越少; 尽量扩展索引，而不要新建索引。例如表中已经有a的索引，现在要加(a,b)的索引，那么只需要修改原有的a索引即可，索引维护也需要代价; 字符串建立索引时可指定长度，不一定非要将整个字段列作为索引; 索引优化 应尽量避免在 where 子句中使用!=或&lt;&gt;操作符，否则将引擎放弃使用索引而进行全表扫描; 能用between…and…就不要用in; 避免使用or来连接条件，否则将导致放弃索引采用索引扫描(使用union代替or); 依据最左前缀匹配原则，like字段使用时避免使用全模糊匹配(%word%)，而应尽量采用右模糊匹配(word%); 避免在where字句中对字段进行函数操作，这将导致放弃使用索引而使用全表扫描;","tags":[{"name":"mysql","slug":"mysql","permalink":"https://cnkeep.github.io/tags/mysql/"}]},{"title":"Mysql5.7-ONLY_FULL_GROUP_BY问题","date":"2018-10-03T18:57:00.000Z","path":"2018/10/04/02-Mysql5.7-ONLY_FULL_GROUP_BY问题/","text":"Mysql5.7-ONLY_FULL_GROUP_BY问题问题复现mysql 5.7之后，进行一些group by查询时，比如:1234567SELECT *, count(id) as count FROM `news` GROUP BY `group_id` ORDER BY `inputtime` DESC LIMIT 20;``` 就会报错： ```sql SELECT list is not in GROUP BY clause and contains nonaggregated column ‘news.id' which is not functionally dependent on columns in GROUP BY clause; this is incompatible with sql_mode=only_full_group_by. 分析与解决分析原因是mysql 5.7模式中，默认启用了ONLY_FULL_GROUP_BY。ONLY_FULL_GROUP_BY是Mysql提供的一个sql_mode,通过这个sql_mode来提供group by合法性的检查。其要求查询的字段必须全部都应该在group by分组条件内。 解决方案123SET @@sql_mode ='STRICT_TRANS_TABLES,NO_ZERO_IN_DATE,NO_ZERO_DATE,ERROR_FOR_DIVISION_BY_ZERO,NO_AUTO_CREATE_USER,NO_ENGINE_SUBSTITUTION';SET @@global.sql_mode ='STRICT_TRANS_TABLES,NO_ZERO_IN_DATE,NO_ZERO_DATE,ERROR_FOR_DIVISION_BY_ZERO,NO_AUTO_CREATE_USER,NO_ENGINE_SUBSTITUTION';SELECT @@global.sql_mode;","tags":[{"name":"mysql","slug":"mysql","permalink":"https://cnkeep.github.io/tags/mysql/"}]},{"title":"02_索引类型&种类","date":"2018-10-02T20:12:00.000Z","path":"2018/10/03/01-02_索引类型&种类/","text":"索引的类型 &amp; 种类 之前已经讨论过关于Innodb索引的实现原理了，那么我们今天就来了解一下mysql的索引到底有哪些 1.索引的类型 Mysql目前主要有以下几种索引类型： FullText(全文索引) Hash(哈希索引) B-Tree(B数索引) R-Tree(空间数据索引) 1.1FullText(全文索引) 介绍 即为全文索引，它的出现是为了解决WHERE name LIKE “%word%”这类针对文本的模糊查询效率较低的问题。它有如下的限制条件： 目前只有MyISAM引擎支持(Innodb5.6以上也支持); 只有 CHAR, VARCHAR, TEXT列上可以创建全文索引; 必须采用指定的函数才会生效:MATCH()…AGAINST 示例`sql– 模拟数据库DROP DATABASE IF EXISTS myisam_db;CREATE DATABASE myisam_db;USE myisam_db; – 文章表DROP TABLE IF EXISTS article;CREATE TABLE article (id INT ( 10 ) PRIMARY KEY AUTO_INCREMENT,title VARCHAR(100) NOT NULL COMMENT ‘标题’,content TEXT COMMENT ‘文章内容’) ENGINE = INNODB DEFAULT CHARSET = utf8 COMMENT ‘文章表’; – 在title，content列上建立全文索引ALTER TABLE article ADD FULLTEXT INDEX fulltext_article(title,content); – 插入模拟数据TRUNCATE TABLE article;INSERT INTO article(title,content) VALUES(‘title001标题’,’content内容1’);INSERT INTO article(title,content) VALUES(‘title002标题’,’content内容2’);INSERT INTO article(title,content) VALUES(‘title003标题’,’content内容3’);INSERT INTO article(title,content) VALUES(‘title004标题’,’content内容4’);INSERT INTO article(title,content) VALUES(‘title005标题’,’content内容5’); – 查询EXPLAIN SELECT FROM article WHERE title LIKE ‘t%’;** 1. ROW *** id: 1 select_type: SIMPLE TABLE: article PARTITIONS: NULL TYPE: ALLpossible_keys: fulltext_article KEY: NULL key_len: NULL ref: NULL ROWS: 5 filtered: 20.00 Extra: USING WHERE EXPLAIN SELECT FROM article WHERE MATCH(title,content) AGAINST(‘t c’);** 1. ROW *** id: 1 select_type: SIMPLE TABLE: article PARTITIONS: NULL TYPE: FULLTEXTpossible_keys: fulltext_article KEY: fulltext_article key_len: 0 ref: const ROWS: 1 filtered: 100.00 Extra: USING WHERE` 1.2Hash(哈希索引) 介绍 通过计算hash至一次定位; 由InnoDB存储引擎自己控制的, DBA无法外界干预; 只能用于等值查找，不能用于范围查找; 不支持排序; 无法避免全表扫描; 1.3B-Tree(B数索引) 介绍 在01-01_索引中已经介绍过索引的BTree结构，如下： 特点： 适合等值查找，范围顺序查找; 适合左前缀模糊查找; 1.4R-Tree(空间数据索引)不常用，忽略 2.索引的种类2.1按照使用类型分以下几种索引种类： 主键索引 单表中只能存在一个主键索引，因为其聚集顺序存储，查询效率非常高(最好保持主键为递增序列，减少分页)。 唯一索引 可以存在多个，列值唯一，不能为null, 加快查询效率。 普通索引 可以存在多个，加快查询效率(可能需要回表, 下文非聚集索引会提及)。 组合索引 多列值组成一个索引，专门用于组合搜索，其效率大于索引合并。 全文索引 分词搜索。 2.2按照存储实现分为2种： 聚集索引 如上图：聚集索引就是按照每张表的主键构造一颗B+树，同时叶子结点存放的即为整张表的行纪录数据（聚集索引的叶子结点也称为数据页）。聚集索引的这个特性也决定了索引组织表中数据也是索引的一部分，和B+树数据结构一样，每个数据页之间都通过一个双向链表来进行链接。 特点： 基于主键的查找速度非常快，范围查找快 非聚集索引 非聚集索引与聚集索引相比： 叶子结点并非数据结点 叶子结点为每一真正的数据行存储一个“键-指针”对 叶子结点中还存储了一个指针偏移量，根据页指针及指针偏移量可以定位到具体的数据行 类似的，在除叶结点外的其它索引结点，存储的也是类似的内容，只不过它是指向下一级的索引页的 当查找的列不在非聚集索引中时，需要再回表一次，这属于随机IO,性能较差","tags":[{"name":"mysql","slug":"mysql","permalink":"https://cnkeep.github.io/tags/mysql/"}]},{"title":"01_索引","date":"2018-10-02T19:37:00.000Z","path":"2018/10/03/01-01_索引/","text":"索引 参考：linhaifeng-第八篇：索引原理与慢查询优化参考：张洋-MySQL索引背后的数据结构及算法原理 【目录】1.介绍2.索引的原理 2.1索引的理念 2.2磁盘IO与预读取 2.3索引的数据结构 1.介绍 为什么要用索引 &nbsp;&nbsp;在数据库操作中，占据主要地位的当属Select操作了，系统80%的操作都来自于系统的查询操作。当数据量大之后，查询的性能将成为系统的瓶颈，这时候就需要一种快速且使用的规则去满足我们查询的需求，所以用索引就是为了提高我们系统的查询效率。索引主要做了两件事：查询和排序. 索引是什么 &nbsp;&nbsp;索引是存储引擎用于快速查找数据记录的一种数据结构。 索引就像字典中的音序表(MYSQL实际实现不是)，要查找某个字，先查找音序表就能快速找到页数，不必一页一页的查找。索引对于性能的影响尤为重要，索引查询优化也是一个程序员的必修课。 2.索引的原理 接下来我们就来了解一下Mysql的Innodb引擎是如何实现索引的 2.1索引的理念&nbsp;&nbsp;以前在学习数据结构时，关于查找算法，我们知道有Hash,二分法，其实质都是通过不断的缩小数据范围，减少需要扫描的数据和次数，数据库查询也是这种理念，但是是结合查询需求和IO考量而实现的。 2.2磁盘IO与预读取 &nbsp;&nbsp;说道数据存储我们不能不考量磁盘IO的影响，那我们来看看我们的磁盘是如何加载数据的。 &nbsp;&nbsp;上图是操作系统读取不同数据的性能对比，可以看到磁盘读取和内存读取差了好几个数据量，那么数据库几百万的数据读写，磁盘的性能读写优化也很重要。&nbsp;&nbsp;考虑到磁盘IO是非常昂贵的操作，计算机系统做了一些优化-预读取。当一次IO操作是，不止加载当前磁盘地址的数据进内存，而且会把相邻地址的数据也会读取到内存中，因为局部预读性原理告诉我们，当计算机访问一个地址的数据的时候，与其相邻的数据也会很快被访问到。每一次IO读取的数据我们称之为一页(page)。具体一页有多大数据跟操作系统有关，一般为4k或8k，也就是我们读取一页内的数据时候，实际上才发生了一次IO，这个理论对于索引的数据结构设计非常有帮助。 2.3索引的数据结构 &nbsp;&nbsp;Innodb存储引擎采用B+Tree`实现索引结构，Innodb的数据文件本身就是索引文件(MyISAM索引和数据文件是分离的)。 如图是Innodb主索引的示意图，它有如下特点： 每一个节点都对应着一个磁盘块，一个磁盘块的大小与操作系统的页大小有关，可以一次加载整个数据块到内存中; 这棵树的叶子节点data域保存了完整的数据记录，叶子节点之间连接为一条链。这种索引交聚集索引，因为InnoDB的数据文件本身要按主键聚集，所以InnoDB要求表必须有主键（MyISAM可以没有），如果没有显式指定，则MySQL系统会自动选择一个可以唯一标识数据记录的列作为主键，如果不存在这种列，则MySQL自动为InnoDB表生成一个隐含字段作为主键，这个字段长度为6个字节，类型为长整形。正是因为如此，使用索引查找非常快; 非叶子结点记录着关键字和指向下层数据块的指针，不存储数据记录，这样可以一次加载更多的关键字，减少IO，快速定位叶子节点; 这棵树一般在2~4层，所以IO次数不高 因为索引有序，且子节点相互连接，所以非常适合范围查找 查找过程 例如要查找30，先将根数据块加载到内存，查找判断子数据块的位置，再将子数据块加载进内存，继续查找，直到到达叶子节点，因为叶子节点直接存储着数据记录，所以直接读取即可。 索引注意的点 因为B+的这些性质，这就导致我们在编写sql时要注意以下： 索引字段尽量小：索引字段小每个数据块存储的关键字更多，减少IO,和分页的发生，更快找到叶子节点； 索引的最左匹配特性：当我们使用组合索引时，一定要注意索引的顺序，例如index(A,B,C),查询使用(A,B),(A,C),(A,B,C)均是可以使用到该索引的，但是查询使用(B),(C),(B,C)根据最左匹配规则是无法使用该索引的。 索引尽量保证有序：因为B+数是建立在排序上的，假如主键无序，可能要进行频繁的分页，同时也增加了查找的复杂度(不能使用二分法)。 选择性建立索引：因为索引在插入删除时也需要维护 下节介绍索引的分类","tags":[{"name":"mysql","slug":"mysql","permalink":"https://cnkeep.github.io/tags/mysql/"}]},{"title":"多项目依赖加载同名Class的问题排查","date":"2018-10-02T18:26:00.000Z","path":"2018/10/03/01-多项目依赖加载同名Class的问题排查/","text":"01-多项目依赖加载同名Class的问题排查1.问题概要&nbsp;&nbsp;旧平台有一个用户导入的功能，是基于tomcat比较老了，现在要将这个接口做成独立服务以供外界调用。当我开始接手时，就在想选什么技术，照搬旧系统？ What the fuck! 那么旧的技术架构，那么繁重，还是webapps老式的tomcat war包哎，排查问题太复杂了，最终还是决定选用springboot,采用内置servlet容器去重新发布服务。&nbsp;&nbsp;终于代码写完了，本地eclipse跑通了，打包发布到虚拟机中测试，运行不了？？？？？ 明明有的方法，却报方法不存在的错误…… 问题排查 bug第一步，日志看一看 字面看是Mybatis报的错，SimpleUser类中没有set方法，跳转到代码中查看，该方法是存在的，怎么回事。 猜想：是不是重名了？ 全文检索SimpleUser看看 还真有重名了，再看看项目依赖结构： 发现自己的项目依赖了web-common模块，而web-common模块中存在重名的SimpleUser类, 而web-common模块中的该类确实没有set方法。怀疑：因为重名加载了web-common中的class 验证 通过在jvm启动参数中添加配置，使其打印类加载日志。(-XX:+TraceClassLoading或-verbose:class) 通过日志检索，发现果然是因为加载了web-common下的类导致的。 解决方案 既然已经知道是因为类加载导致的，那我们就强制指定前加载我们正确的类。 12345之前的脚本：java -d64 -XX:MaxPermSize=192M -Xms500M -Xmx1000M -XX:+HeapDumpOnOutOfMemoryError -cp conf/:lib/* Starter 2&gt;&amp;1 &amp;修改后的脚本：java -d64 -XX:MaxPermSize=192M -Xms500M -Xmx1000M -XX:+HeapDumpOnOutOfMemoryError -cp conf/:lib/web-importer.jar:/lib/web-common.jar:lib/* Starter 2&gt;&amp;1 &amp; 启动服务，查看类加载日志问题解决 知识回顾jvm的类加载采用双亲委托模型，一旦类被类加载器加载一次后就不会再加载第二次，一旦我们项目中出现重名的类，就有可能因为类加载器的问题，导致bug出现。 拓展疑问 问什么之前基于war包的tomcat部署方式没有出现过这种问题呢？ tomcat重写了自己的类加载器，加载顺序如下： $java_home/lib 目录下的java核心api $java_home/lib/ext 目录下的java扩展jar包 java -classpath/-Djava.class.path所指的目录下的类与jar包 $CATALINA_HOME/common目录下按照文件夹的顺序从上往下依次加载 $CATALINA_HOME/server目录下按照文件夹的顺序从上往下依次加载 $CATALINA_BASE/shared目录下按照文件夹的顺序从上往下依次加载 我们的项目路径/WEB-INF/classes下的class文件 我们的项目路径/WEB-INF/lib下的jar文件 而旧项目的重写类都在classes目录下，所以优先加载，就没有问题喽~~~","tags":[{"name":"bugs","slug":"bugs","permalink":"https://cnkeep.github.io/tags/bugs/"}]},{"title":"catalog","date":"2018-10-02T16:49:00.000Z","path":"2018/10/03/00-catalog/","text":"bug记录分析集 记录遇到的各种bug, 以及解决方案 01-多项目依赖加载同名Class的问题排查","tags":[{"name":"bugs","slug":"bugs","permalink":"https://cnkeep.github.io/tags/bugs/"}]},{"title":"Mysql的介绍和安装","date":"2018-10-02T16:37:00.000Z","path":"2018/10/03/01-Mysql的介绍和安装/","text":"Mysql的介绍与安装 标签：MySql, Linux 介绍MySQL 是最流行的关系型数据库管理系统，在 WEB 应用方面 MySQL 是最好的 RDBMS(Relational Database Management System：关系数据库管理系统)应用软件之一。 安装安装环境 Centos7 获取源官网地址 选择DOWNLOADS =&gt; Archives =&gt; MySQL Community Server选择相应版本下载，这里我下载5.7.17 开始安装 1.建立mysql用户 1234567891011#后面mysql就使用这个用户来运行（注意这也是mysql启动脚本中默认的用户，因此最好不要改名）。$ groupadd mysql$ useradd -r -g mysql mysql（使用-r参数表示mysql用户是一个系统用户，不能登录） #建立目录/usr/local/mysql，后面mysql就安装在这个目录下面。$ mkdir /usr/local/mysql /usr/local/mysql/data#将mysql及其下所有的目录所有者和组均设为mysql:$ cd /usr/local/mysql$ chown mysql:mysql -R . 2.解压 12345#安装mysql的依赖库$yum install libaio #将前面得到的mysql-5.7.11-linux-glibc2.5-x86_64.tar.gz解压至/usr/local/mysql目录下$ tar zxvf mysql-5.7.11-linux-glibc2.5-x86_64.tar.gz -C /usr/local/mysql 3.初始化 1234567891011121314151617181920$ /usr/local/mysql/bin/mysqld \\ --initialize --user=mysql \\ --datadir=/usr/local/mysql/data \\ --basedir=/usr/local/mysql 注意：1. data目录解压后没有，需要手动建立（见上文）；2. mysql5.7和之前版本不同，很多资料上都是这个命令...../scripts/mysql_install_db --user=mysql而5.7版本根本没有这个。初始化成功后出现如下信息：201x-xx-xxT07:10:13.583130Z 0 [Warning] TIMESTAMP with implicit DEFAULT value is deprecated. Please use --explicit_defaults_for_timestamp server option (see documentation for more details).201x-xx-xx T07:10:13.976219Z 0 [Warning] InnoDB: New log files created, LSN=45790201x-xx-xx T07:10:14.085666Z 0 [Warning] InnoDB: Creating foreign key constraint system tables.201x-xx-xx T07:10:14.161899Z 0 [Warning] No existing UUID has been found, so we assume that this is the first time that this server has been started. Generating a new UUID: 1fa941f9-effd-11e5-b67d-000c2958cdc8.201x-xx-xx T07:10:14.165534Z 0 [Warning] Gtid table is not ready to be used. Table &apos;mysql.gtid_executed&apos; cannot be opened.201x-xx-xx T07:10:14.168555Z 1 [Note] A temporary password is generated for root@localhost: q1SLew5T_6K, 注意最后一行，这也是和之有版本不同的地方，它给了root一个初始密码，后面要登录的时候要用到这个密码。 4.配置[可以跳过] 123456789101112131415161718192021$ cp /usr/local/mysql/support-files/my-default.cnf /etc/my.cnf$ vi /etc/my.cnf [client] socket=/var/lib/mysql/mysql.sock [mysqld] #用于找回密码 #skip-grant-tables #忽略表名大小写，linux区分大小写的，windows不区分 lower_case_table_names=1 sql_mode=NO_ENGINE_SUBSTITUTION,STRICT_TRANS_TABLES basedir=/usr/local/mysql datadir=/usr/local/mysql/data port=3306 socket=/var/lib/mysql/mysql.sock character-set-server=utf8如果不把my.cnf拷到/etc下，运行时会出现：mysqld: Can&apos;t change dir to &apos;/usr/local/mysql/data/&apos; (Errcode: 2 - No such file or directory)这样的出错提示，说明它没找到my.cnf中的配置；而去找了程序编译时的默认安装位置：/usr/local/mysql 5.启动 1234$ mysqld_safe&amp;#停止$ service mysqld stop 5.设置开机启动 123456789101112将&#123;mysql&#125;/ support-files/mysql.server 拷贝为/etc/init.d/mysql并设置运行权限 #cp mysql.server /etc/init.d/mysql#chmod +x /etc/init.d/mysql 把mysql注册为开机启动的服务#chkconfig --add mysql 【mysql开启和关闭】#/etc/init.d/mysql start#/etc/init.d/mysql stop#service mysql start 拓展 修改密码1234567$ mysqld_safe --skip-grant-tables &amp;root$ mysql -uroot -p&gt; use mysql&gt; update user set password=PASSWORD(&quot;123456&quot;) where user=&apos;root&apos;;发现报错：ERROR 1054 (42S22): Unknown column &apos;password&apos; in &apos;field list&apos;&gt; show create user //发现密码字段已经发生改变，变为`authentication_string` &gt; update user set authentication_string=password(&apos;123456&apos;) where user=&apos;root&apos;; 设置远程访问 123mysq&gt; use mysql;mysql&gt;grant all privileges on *.* to &apos;root&apos;@&apos;&apos; identified by &apos;123456&apos; with grant option;msyql&gt;flush privileges;","tags":[{"name":"mysql","slug":"mysql","permalink":"https://cnkeep.github.io/tags/mysql/"}]},{"title":"00_Mysql介绍","date":"2018-10-01T15:30:00.000Z","path":"2018/10/01/00-00_Mysql介绍/","text":"Mysql介绍","tags":[{"name":"mysql","slug":"mysql","permalink":"https://cnkeep.github.io/tags/mysql/"}]},{"title":"tcpdump的使用","date":"2018-07-18T16:13:00.000Z","path":"2018/07/19/19-tcpdump的使用/","text":"tcpdump抓包工具的使用1. 介绍&nbsp;&nbsp;面对一些线上的网络问题排查时，经常需要抓取网络数据包进行分析，windows有大名鼎鼎的winshark, 而在linux就少有图形化的抓包工具，幸好有tcpdump工具的存在。 &nbsp;&nbsp; 它可以使用定义的规则抓取网卡上的数据包，便于我们分析数据。 2. 抓包原理2.1 原理介绍 抓包原理 Linux抓包是通过注册一种虚拟的底层网络协议来完成对网络报文(准确的说是网络设备)消息的处理权。当网卡接收到一个网络报文之后，它会遍历系统中所有已经注册的网络协议，当抓包模块把自己伪装成一个网络协议的时候，系统在收到报文的时候就会给这个伪协议一次机会，让它来对网卡收到的报文进行一次处理，此时该模块就会趁机对报文进行窥探，也就是把这个报文完完整整的复制一份，假装是自己接收到的报文，汇报给抓包模块。 注意事项 必须使用root身份执行 要抓取其他主机的数据包，需要开启混杂模式，即抓取任何经过它的数据包，不管这个数据包是不是发给它或者是它发出的。一般而言，Unix不会让普通用户设置混杂模式，因为这样可以看到别人的信息，比如telnet的用户名和密码，这样会引起一些安全上的问题，所以只有root用户可以开启混杂模式，开启混杂模式的命令是：ifconfig en0 promisc, en0是你要打开混杂模式的网卡。 2.2 命令介绍1. 命令格式12345678tcpdump [ -AdDefIKlLnNOpqRStuUvxX ] [ -B buffer_size ] [ -c count ] [ -C file_size ] [ -G rotate_seconds ] [ -F file ] [ -i interface ] [ -m module ] [ -M secret ] [ -r file ] [ -s snaplen ] [ -T type ] [ -w file ] [ -W filecount ] [ -E spi@ipaddr algo:secret,... ] [ -y datalinktype ] [ -z postrotate-command ] [ -Z user ] [ expression ] 2. 选项介绍-A：以ASCII编码打印每个报文（不包括链路层的头），这对分析网页来说很方便-a：将网络地址和广播地址转变成名字-c&lt;数据包数目&gt;：在收到指定的包的数目后，tcpdump就会停止-C：用于判断用 -w 选项将报文写入的文件的大小是否超过这个值，如果超过了就新建文件（文件名后缀是1、2、3依次增加）-d：将匹配信息包的代码以人们能够理解的汇编格式给出-dd：将匹配信息包的代码以c语言程序段的格式给出-ddd：将匹配信息包的代码以十进制的形式给出-D：列出当前主机的所有网卡编号和名称，可以用于选项 -i-e：在输出行打印出数据链路层的头部信息-f：将外部的Internet地址以数字的形式打印出来-F&lt;表达文件&gt;：从指定的文件中读取表达式,忽略其它的表达式-i&lt;网络界面&gt;：监听主机的该网卡上的数据流，如果没有指定，就会使用最小网卡编号的网卡（在选项-D可知道，但是不包括环路接口），linux 2.2 内核及之后的版本支持 any 网卡，用于指代任意网卡-l：如果没有使用 -w 选项，就可以将报文打印到 标准输出终端（此时这是默认）-n：显示ip，而不是主机名-N：不列出域名-O：不将数据包编码最佳化-p：不让网络界面进入混杂模式-q：快速输出，仅列出少数的传输协议信息-r&lt;数据包文件&gt;：从指定的文件中读取包(这些包一般通过-w选项产生)-s&lt;数据包大小&gt;：指定抓包显示一行的宽度，-s0表示可按包长显示完整的包，经常和-A一起用，默认截取长度为60个字节，但一般ethernet MTU都是1500字节。所以，要抓取大于60字节的包时，使用默认参数就会导致包数据丢失-S：用绝对而非相对数值列出TCP关联数-t：在输出的每一行不打印时间戳-tt：在输出的每一行显示未经格式化的时间戳记-T&lt;数据包类型&gt;：将监听到的包直接解释为指定的类型的报文，常见的类型有rpc （远程过程调用）和snmp（简单网络管理协议）-v：输出一个稍微详细的信息，例如在ip包中可以包括ttl和服务类型的信息-vv：输出详细的报文信息-x:/-xx/-X/-XX：以十六进制显示包内容，几个选项只有细微的差别，详见man手册-w:&lt;数据包文件&gt;：直接将包写入文件中，并不分析和打印出来expression：用于筛选的逻辑表达式； 3. 常用选项 抓取指定数目的包(-c选项) 默认情况下tcpdump将一直抓包，直到按下”ctrl+c”中止，使用-c选项可以指定抓包的数量。 将抓到包写入文件中(-w选项) 使用-w选项，可将抓包记录到一个指定文件中，以供后续分析 读取tcpdump保存文件(-r选项) 对于保存的抓包文件，可以使用-r选项进行读取 抓包时不进行域名解析(-n选项) 默认情况下，tcpdump抓包结果中将进行域名解析，显示的是域名地址而非ip地址，使用-n选项，可指定显示ip地址。 显示完整的包(-s0) 4. 表达式介绍 表达式是一个正则表达式，tcpdump利用它作为过滤报文的条件，如果一个报文满足表达式的条件，则这个报文将会被捕获。如果没有给出任何条件，则网络上所有的信息包将会被截获。在表达式中一般如下几种类型的关键字: 类型关键字主要包括host，net，port，缺省是host 传输方向关键字主要包括src（源地址）, dst(目标地址) ,dst or src, dst and src, 缺省是src or dst 协议关键字主要包括fddi,ip ,arp,rarp,tcp,udp等类型 3. 示例1. 监视指定网络接口的数据包 12# tcpdump -i eth1 如果不指定网卡，默认tcpdump只会监视第一个网络接口上所有流过的数据包，一般是eth0，下面的例子都没有指定网络接口。 2. 监视指定主机的数据包12345678910111213141516171819202122232425截获所有210.27.48.1 的主机收到的和发出的所有的数据包 # tcpdump host 210.27.48.1``` ### 3. TCP连接 这里通过telnet工具连接redis测试tcp连接的报文抓取 ```text [root@localhost redis-5.0.3]# tcpdump -i eth1 -v -s 0 port 6379 tcpdump: listening on eth1, link-type EN10MB (Ethernet), capture size 262144 bytes[1] 21:56:41.602246 IP (tos 0x0, ttl 64, id 54118, offset 0, flags [DF], proto TCP (6), length 52)[2] 172.16.22.235.54586 &gt; localhost.localdomain.6379: Flags [S], cksum 0xbe8b (correct), seq 1436544059, win 64240, options [mss 1460,nop,wscale 8,nop,nop,sackOK], length 0[3] 21:56:41.602294 IP (tos 0x0, ttl 64, id 0, offset 0, flags [DF], proto TCP (6), length 52)[4] localhost.localdomain.6379 &gt; 172.16.22.235.54586: Flags [S.], cksum 0x85b9 (incorrect -&gt; 0xdaf5), seq 3525089865, ack 1436544060, win 29200, options [mss 1460,nop,nop,sackOK,nop,wscale 7], length 0[5] 21:56:41.602446 IP (tos 0x0, ttl 64, id 54119, offset 0, flags [DF], proto TCP (6), length 40)[6] 172.16.22.235.54586 &gt; localhost.localdomain.6379: Flags [.], cksum 0x85d3 (correct), ack 1, win 2053, length 0[7] 21:56:48.665457 IP (tos 0x0, ttl 64, id 54123, offset 0, flags [DF], proto TCP (6), length 40)[8] 172.16.22.235.54586 &gt; localhost.localdomain.6379: Flags [F.], cksum 0x85d2 (correct), seq 1, ack 1, win 2053, length 0[9] 21:56:48.665797 IP (tos 0x0, ttl 64, id 58678, offset 0, flags [DF], proto TCP (6), length 40)[10] localhost.localdomain.6379 &gt; 172.16.22.235.54586: Flags [F.], cksum 0x85ad (incorrect -&gt; 0x8cf1), seq 1, ack 2, win 229, length 0[11] 21:56:48.666220 IP (tos 0x0, ttl 64, id 54124, offset 0, flags [DF], proto TCP (6), length 40)[12] 172.16.22.235.54586 &gt; localhost.localdomain.6379: Flags [.], cksum 0x85d1 (correct), ack 2, win 2053, length 0 ^C 6 packets captured 16 packets received by filter 0 packets dropped by kernel 分析12345678[1]~[6] 3次握手 [7]~[12] 4次挥手拿[1]来看，proto： 指明协议是TCPlength：报文长度srcHost:srcPort &gt; destHost:destPort :指明源地址端口和目标地址端口Flags[*]: 指明包类型，S代表SYN; .代表ACK; F代表FIN","tags":[{"name":"Linux","slug":"Linux","permalink":"https://cnkeep.github.io/tags/Linux/"}]},{"title":"EOF的使用","date":"2018-07-18T13:59:00.000Z","path":"2018/07/18/18-EOF的使用/","text":"Linux下EOF的使用1. 介绍如果我们需要往一个文件里自动输入N行内容。如果是少数的几行内容，还可以用echo追加方式，但如果是很多行，那么单纯用echo追加的方式就显得复杂了，这时候就可以使用EOF结合cat命令进行内容的追加了。 2. 用法123&lt;&lt;EOF ...EOF 通过cat配合重定向能够生成文件并追加操作,在它之前先熟悉几个特殊符号:&lt; :输入重定向&gt; :输出重定向&gt;&gt; :输出重定向,进行追加,不会覆盖之前内容&lt;&lt; :标准输入来自命令行的一对分隔号的中间内容. 3.示例1234567# cat &lt;&lt;EOF&gt;test.sh&gt; ssss&gt; lllll&gt; EOF# more test.sh sssslllll","tags":[{"name":"Linux","slug":"Linux","permalink":"https://cnkeep.github.io/tags/Linux/"}]},{"title":"which_where_find","date":"2018-07-17T12:46:00.000Z","path":"2018/07/17/17-which_where_find/","text":"which, whereis, find的区别","tags":[{"name":"Linux","slug":"Linux","permalink":"https://cnkeep.github.io/tags/Linux/"}]},{"title":"挂载","date":"2018-07-16T09:42:00.000Z","path":"2018/07/16/16-挂载/","text":"https://blog.csdn.net/csh86277516/article/details/78844830https://blog.csdn.net/qq_39521554/article/details/79501714","tags":[{"name":"Linux","slug":"Linux","permalink":"https://cnkeep.github.io/tags/Linux/"}]},{"title":"proc目录探查系统信息","date":"2018-07-16T07:40:00.000Z","path":"2018/07/16/16-proc目录探查系统信息/","text":"/proc目录探查系统信息 标签：/proc 介绍/proc 文件系统下的多种文件提供的系统信息不是针对某个特定进程的, 而是能够在整个系统范围的上下文中使用。可以使用的文件随系统配置的变化而变化。命令procinfo 能够显示基于其中某些文件的多种系统信息。包含：内存，硬盘等 使用 CPU1$ cat /proc/cpuinfo Memory12$ cat /proc/meminfo$ free -m 硬盘12$ fdisk -l$ df -h 其他 123456netstat -lntp # 查看所有监听端口 netstat -antp # 查看所有已经建立的连接 w # 查看活动用户 id # 查看指定用户信息 last # 查看用户登录日志 rpm -qa # 查看所有安装的软件包","tags":[{"name":"Linux","slug":"Linux","permalink":"https://cnkeep.github.io/tags/Linux/"}]},{"title":"yum工具配置","date":"2018-07-15T06:27:00.000Z","path":"2018/07/15/15-yum工具配置/","text":"yum工具配置介绍yum是一款软件包管理器，能够从指定的服务器自动下载RPM包并且安装，还可以自动处理依赖性关系，yum提供了查找、安装、删除某一个，一组甚至全部软件包的命令，而且命令简洁而又好记。 常用命令介绍 1.查找与显示123456# 查找软件yum search &lt;package&gt;#显示安装包信息yum info &lt;package&gt;#显示已经安装和可以安装的程序包yum list 2.安装1yum install &lt;package&gt; 3.更新与升级12345#更新yum update [&lt;package&gt;]#升级程序yum upgrade &lt;package&gt; 4.移除程序1234#移除程序包yum remove &lt;package&gt;##查看依赖yum deplist &lt;package&gt; 5.缓存12345#缓存清除yum clean #生成缓存yum makecache 配置及目录介绍 1.配置文件/etc/yum.conf 1234567891011121314151617181920212223242526[main]cachedir=/var/cache/yum/$basearch/$releaseverkeepcache=0debuglevel=2logfile=/var/log/yum.logexactarch=1obsoletes=1gpgcheck=1plugins=1installonly_limit=5bugtracker_url=http://bugs.centos.org/set_project.php?project_id=23&amp;ref=http://bugs.centos.org/bug_report_page.php?category=yumdistroverpkg=centos-release# This is the default, if you make this bigger yum won&apos;t see if the metadata# is newer on the remote and so you&apos;ll &quot;gain&quot; the bandwidth of not having to# download the new metadata and &quot;pay&quot; for it by yum not having correct# information.# It is esp. important, to have correct metadata, for distributions like# Fedora which don&apos;t keep old packages around. If you don&apos;t like this checking# interupting your command line usage, it&apos;s much better to have something# manually check the metadata once an hour (yum-updatesd will do this).# metadata_expire=90m# PUT YOUR REPOS HERE OR IN separate files named file.repo# in /etc/yum.repos.d 简单介绍一下： cachedir: yum缓存目录，yum在此存储下载的rpm包和数据库 logfile: 日志文件 2.镜像仓库配置目录/etc/yum.repos.d/123$ ls /etc/yum.repos.d/Centos-7.repo CentOS-CR.repo CentOS-fasttrack.repo CentOS-Sources.repo docker-ce.repo_bakCentOS-Base.repo_bak CentOS-Debuginfo.repo CentOS-Media.repo CentOS-Vault.repo 该目录下面的配置文件指明了镜像的源地址配置，我们可以在此配置新的镜像源地址 3.插件等其他的配置文件目录/etc/yum/yum/12$ ls /etc/yum/yum/fssnap.d pluginconf.d protected.d vars version-groups.conf 这里重点介绍pluginconf.d/目录是相关插件的配置目录，后面介绍的fastestmirror插件配置文件就在这里 拓展添加源添加阿里的镜像源 123#配置域名解析# vi /etc/resolve.conf nameserver 8.8.8.8 方式一：12345678910#备份原有配置$ mv /etc/yum.repos.d/Centos-7.repo /etc/yum.repos.d/Centos-7.repo.bak# 下载阿里镜像配置$ sudo yum-config-manager \\ --add-repo \\ http://mirrors.aliyun.com/repo/Centos-7.repo#清理缓存$ sudo yum clean#构建缓存$ sudo yum makecache 方式二:123456789#备份原有配置$ mv /etc/yum.repos.d/Centos-7.repo /etc/yum.repos.d/Centos-7.repo.bak# 下载阿里镜像配置$ wget http://mirrors.aliyun.com/repo/Centos-7.repo$ mv Centos-7.repo /etc/yum.repos.d/Centos-7.repo#清理缓存$ sudo yum clean all#构建缓存$ sudo yum makecache 方式三：12345678910111213141516171819202122232425262728293031323334353637383940414243444546# 查看缓存存储路径$ yum-config-manager|grep base_persistdirpersistdir = /var/lib/yum/repos/x86_64/7base_persistdir = /var/lib/yum/repos/x86_64/7base_persistdir = /var/lib/yum/repos/x86_64/7$ cd /var/lib/yum/repos/x86_64/7$ vi /etc/yum.conf [main] cachedir=/var/cache/yum/$basearch/$releasever keepcache=0 debuglevel=2 logfile=/var/log/yum.log exactarch=1 obsoletes=1 gpgcheck=1 plugins=1 #将plugins的值修改为0 installonly_limit=5$ sudo yum install yum-plugin-fastestmirror$ vi /etc/yum/pluginconf.d/fastestmirror.conf [main] enabled=1 #配置为1 verbose=0 always_print_best_host = true socket_timeout=3 # Relative paths are relative to the cachedir (and so works for users as well # as root). hostfilepath=timedhosts.txt maxhostfileage=10 maxthreads=15 #exclude=.gov, facebook #include_only=.nl,.de,.uk,.ie $ vi /var/cache/yum/x86_64/7/timedhosts.txt #加入下列内容 mirrors.aliyuncs.com 99999999999 mirrors.cloud.aliyuncs.com 99999999999 mirrors.aliyun.com 2.03075098991#清理缓存$ sudo yum clean all#构建缓存$ sudo yum makecache","tags":[{"name":"Linux","slug":"Linux","permalink":"https://cnkeep.github.io/tags/Linux/"}]},{"title":"开机启动项","date":"2018-07-13T02:09:00.000Z","path":"2018/07/13/14-开机启动项/","text":"开机启动项 标签：Redis, systemctl, chkconfig参考：Linux实现开机自动运行普通用户脚本参考：systemctl管理Redis启动、停止、开机启动 前言最近在玩Redis时，因为是放在虚拟机里跑的，但是虚拟机需要经常关机和开机，导致每次都要手动重启redis, 但是我受够了想让它开机自启，于是就有了今天的内容。 设置开机自启方案方案一:使用/etc/rc.d/rc.local自启动脚本文件实现开机自动运行普通用户脚本。 把需要开机启动的脚本程序直接写入/etc/rc.d/rc.local文件中，这样子开机时就会自动执行这些脚本程序，运行对应的服务程序。需要在root环境下编辑。 方案二:使用chkconfig和/etc/init.d 我们都了解/etc/init.d目录下的所有文件都是脚本文件，这个目录下的脚本文件，在设置到开机自启动后，会在开机时自动执行。 1.root账号编写自启动脚本12345678$ vi /etc/init.d/redis#!/bin/bash# redis auto start scripts#chkconfig: 235 80 30 --235指定的启动级别，在哪写启动级别下启动；--80 启动的优先级；--30 关闭的优先级su /usr/local/app/redis-5.0.3/src/redis-server /usr/local/app/redis-5.0.3/redis.conf --daemonize no 等级0表示：表示关机等级1表示：单用户模式等级2表示：无网络连接的多用户命令行模式等级3表示：有网络连接的多用户命令行模式等级4表示：不可用等级5表示：带图形界面的多用户模式等级6表示：重新启动 2.添加执行权限1$ chmod +x /etc/init.d/redis 3.加入启动项配置1234#假如启动项$ chkconfig --add redis #设置开机启动$ chkconfig redis on 4.查看启动项1$ chkconfig --list 方案三(推荐):使用systemctl和/lib/systemd/system/ 1.编写脚本1234567891011121314$ vi /lib/systemd/system/redis.service#写入以下内容[Unit]Description=Redis_5.0.1After=network.target[Service]#redis安装绝对路径ExecStart=/usr/local/app/redis-5.0.3/src/redis-server /usr/local/app/redis-5.0.3/redis.conf --daemonize noExecStop=/usr/local/app/redis-5.0.3/src//redis-cli -h 127.0.0.1 -p 6379 shutdown[Install]WantedBy=multi-user.target [Unit] 表示这是基础信息 Description 是描述 After 是在那个服务后面启动，一般是网络服务启动后启动 [Service] 表示这里是服务信息 ExecStart 是启动服务的命令 ExecStop 是停止服务的指令 [Install] 表示这是是安装相关信息 WantedBy 是以哪种方式启动：multi-user.target表明当系统以多用户方式（默认的运行级别）启动时，这个服务需要被自动运行。 详细请移步至：CoreOS实践指南（八）：Unit文件详解 2.设置开机启动123456789$ ln -s /lib/systemd/system/redis.service /etc/systemd/system/multi-user.target.wants/redis.service#刷新配置$ systemctl daemon-reload#开启开机自启功能$ systemctl enable redis$ systemctl [start|stop|restart|status] redis 3.查看启动项1$ systemctl list-unit-files *","tags":[{"name":"Linux","slug":"Linux","permalink":"https://cnkeep.github.io/tags/Linux/"}]},{"title":"wc命令","date":"2018-07-11T23:02:00.000Z","path":"2018/07/12/13-wc命令/","text":"1234567891011wc -l #统计行数 -c, --bytes print the byte counts -m, --chars print the character counts -l, --lines print the newline counts --files0-from=文件 从指定文件读取以NUL 终止的名称，如果该文件被 指定为&quot;-&quot;则从标准输入读文件名 -L, --max-line-length 显示最长行的长度 -w, --words 显示单词计数 --help 显示此帮助信息并退出 --version 显示版本信息并退出","tags":[{"name":"Linux","slug":"Linux","permalink":"https://cnkeep.github.io/tags/Linux/"}]},{"title":"echo转义&&换行","date":"2018-07-10T20:57:00.000Z","path":"2018/07/11/12-echo转义&&换行/","text":"echo中的转义与换行经常使用echo命令，但是不知道怎么转义可不换行，今天用到了，做一下笔记。 1.原样输出(使用单引号)12345678910111213echo &apos;$name \\n end&apos;#output:# $name \\n end``` &gt; 2.显示转义字符(-e) ```textecho -e &apos;$name \\n end&apos;#output:# $name # end 3.不换行(\\c)12345echo -e &quot;$name \\n end \\c&quot; &amp;&amp; echo &quot;===&quot; #output:# $name # end ===","tags":[{"name":"Linux","slug":"Linux","permalink":"https://cnkeep.github.io/tags/Linux/"}]},{"title":"head&tail命令","date":"2018-07-10T16:44:00.000Z","path":"2018/07/11/11-head&tail命令/","text":"head &amp; tail命令命令功能&nbsp;&nbsp;倒序或者顺序查看文件内容 命令格式12$tail [filename]$head [filename] 常用参数示例 倒序查看100行1$tail -n 100 [filename] 倒序查看文件动态刷新，用于日志观察1$tail -f [filename] 查看文件前10行1$head -n 10 [filename]","tags":[{"name":"Linux","slug":"Linux","permalink":"https://cnkeep.github.io/tags/Linux/"}]},{"title":"vi命令","date":"2018-07-08T13:32:00.000Z","path":"2018/07/08/10-vi命令/","text":"vi命令命令功能&nbsp;&nbsp;vi命令是linux提供的强大的文本编辑工具，接下来我们就常用的功能做一下记录(非全部)。 vi查找 当你用vi打开一个文件后，因为文件太长，如何才能找到你所要查找的关键字呢？ /或者?,在命令模式下敲斜杆(/)这时在状态栏（也就是屏幕左下脚）就出现了 “/”然后输入你要查找的关键字敲回车就可以了。如果你要继续查找此关键字，敲字符n就可以继续查找了。值得注意的是“/”是向下查找，而“?”是向上查找，而在键盘定义上“?”刚好是“/”的上档符。 vi替换： vi/vim 中可以使用 ：s 命令来替换字符串以前只会使用一种格式来全文替换，这里只说部分 123456：s/vivian/sky/ 替换当前行第一个 vivian 为 sky：s/vivian/sky/g 替换当前行所有 vivian 为 sky：n,$s/vivian/sky/ 替换第 n 行开始到最后一行中每一行的第一个 vivian 为 sky：n,$s/vivian/sky/g 替换第 n 行开始到最后一行中每一行所有 vivian 为 sky n 为数字，若 n 为 .，表示从当前行开始到最后一行：%s/vivian/sky/（等同于：g/vivian/s//sky/）替换每一行的第一个 vivian 为 sky：%s/vivian/sky/g（等同于：g/vivian/s//sky/g）替换每一行中所有 vivian 为 sky 鼠标移动操作12345命令模式下``H,J,K,L``字符可以实现鼠标跳转，这几个键都是紧挨着的，操作多方便哦！ H:向前移动J:向下移动K:向上移动L:向后移动 行数操作12345#显示行号:set nu跳转到第10行:10 翻页12Ctrl+B: 上一页Ctrl+F: 下一页 复制，粘贴，撤销，删除1234yy: 复制当前行p: 粘贴u: 撤销更改dd: 删除","tags":[{"name":"Linux","slug":"Linux","permalink":"https://cnkeep.github.io/tags/Linux/"}]},{"title":"03_通过Dockerfile构建自己的镜像文件","date":"2018-07-07T15:24:00.000Z","path":"2018/07/07/01-03_通过Dockerfile构建自己的镜像文件/","text":"通过Dockerfile构建自己的镜像文件目录Dockerfile是什么Dockerfile指令介绍示例-构建jdk镜像Dockerfile是什么&emsp;&emsp;Dockerfile是由一系列命令和参数构成的脚本，这些命令应用于基础镜像并最终创建一个新的镜像。其产出为一个新的可以用于创建容器的镜像。 Dockerfile指令 FROM &emsp;&emsp;FROM指令用于指定当前镜像所使用的的基础镜像，一般写在文件开头，如果想自定义构建docker镜像，那么引用的基础镜像一般是：centos, debian, ubuntu。 指令语法：1234567FROM &lt;image&gt;FROM &lt;image&gt;:&lt;tag&gt;FROM &lt;image&gt;:&lt;digest&gt;示例：FROM centos:v1 MAINTAINER &emsp;&emsp;MAINTAINER用于描述当前Dockerfile的文件信息指令语法：12345MAINTAINER &lt;name&gt;示例：MAINTAINER LeiLi.Zhang &lt;zhangleili924@gmail.com&gt; 3.RUN &emsp;&emsp;运行指定命令，此命令只有在执行docker build构建镜像时才会执行，Dokcerfile中的命令每执行一条即产生一个新的镜像，当前命令总是在最新的镜像上执行，所以为了避免缓存之类的影响，应尽量将多条命令合并为一条执行，每条RUN指令将在当前镜像基础上执行指定命令，并提交为新的镜像。当命令较长时可以使用\\来换行。 指令格式：1234567RUN &lt;command&gt;示例：RUN yum update \\&amp;&amp; yum install openjdk-8-jdk -y \\&amp;&amp; yum clean all 4.CMD &emsp;&emsp;设置容器启动时要执行的命令，只有在执行run 或者 start时才会运行，假如有多条命令只会执行最后一条，执行会覆盖。指令格式：1CMD [&quot;java -version&quot;] 5.EXPOSE 设置容器的暴露端口，注意并不是指暴露到物理机上的端口号！！！指令语法：1EXPOSE port 6.ENV 此指令为设置环境变量指令语法：1234ENV &lt;key&gt; &lt;value&gt;示例：ENV JAVA_HOME /usr/local/jdk 7.ADD 该指令的功能是把宿主机文件复制到镜像中，目录会自动创建。指令格式：12345ADD &lt;src&gt; &lt;dest&gt; src可以是网络资源示例：ADD /usr/local/app/jdk /usr/lib/jdk 绝对路径方式：将宿主机/usr/local/app/jdk目录拷贝到镜像/usr/lib/jdk目录ADD jdk /usr/lib/jdk 相对路径方式：将相对当前Dockerfile文件路径下的jdk目录拷贝至惊醒/usr/lib/jdk目录 8.COPY 此命令与ADD命令功能相似，不同的是，src只能是本地文件，且文件路径是Dockerfile的相对路径指令格式：1234COPY &lt;src&gt; &lt;dest&gt;示例：COPY jdk /usr/lib/jdk 将相对当前Dockerfile文件路径下的jdk目录拷贝至惊醒/usr/lib/jdk目录 9.VLOUME &emsp;&emsp;设置你的卷，在启动容器的时候Docker会在/var/lib/docker/的下一级目录下创建一个卷，以保存你在容器中产生的数据。若没有申明则不会创建。(可以把此指令看成shell中的mkdir）此指令不是独立数据卷，数据会随着容器的停止而消失，如果想数据持久化，请参考docker 简单命令 在run启动容器是加-V 参数！ 指令格式：1VLOUME [&quot;&quot;] 10.WORKDIR 指定容器的工作目录，可以在构建镜像的时候使用，也可以在启动容器的时候使用，构建使用是通过WORKDIR将当前目录切换到指顶目录中，可以理解为shell的cd，启动容器的时候使用的意思为 docker run 启动容器时，默认进入到目录是WORKDIR 指定的。 指令语法：1WORKDIR /usr 11.ENTRYPOINT RNTRYPONT指令与CMD指令的作用类似，都是在容器启动时执行相关的指令，不同的是， CMD中的参数会被启动时指定的动态参数替换掉，而ENTRYPOINT不会被替换掉。 CMD和ENTRYPOINT同时存在时，CMD中的指令会被一般结合这两者进行配置，ENTRYPOINT设置指令，CMD设置参数, 例如： 123#DockerfileENTRYPOINT [&quot;/bin/ping&quot;,&quot;-c&quot;,&quot;3&quot;]CMD [&quot;localhost&quot;] /bin/ping -c 3 localhost 示例-构建jdk镜像这里示例制作一个基于nginx镜像的jdk镜像 1.下载jdk 这里使用的是jdk1.8.0_172，通过工具上传到服务器/usr/local/dock_file目录（请自行事先建立该目录）12$ls /usr/local/docker_fileDockerfile jdk1.8.0_172 2.创建Dockerfile文件1234567891011121314151617181920212223$ cd /usr/local/docker_file$ touch Dockerfile#vi Dockerfile追加以下内容：#这里的基础镜像为nginxFROM nginxMAINTAINER LeiLi.Zhang#切换镜像目录WORKDIR /usrRUN mkdir jdk#将宿主机当前目录下的jdk1.8.0_172拷贝至镜像的/user/jdk目录下ADD jdk1.8.0_172 /usr/jdk$ 设置环境变量ENV JAVA_HOME=/usr/jdkENV JRE_HOME=$JAVA_HOME/jreENV CLASSPATH=.:$CLASSPATH:$JAVA_HOME/bin/dt.jar:$JAVA_HOME/lib/tools.jarENV PATH=$PATH:$JAVA_HOME/binCMD [&quot;java&quot;, &quot;-version&quot;] 3.构建镜像文件 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556$ docker build -f Dockerfile -t nginx_jdk8:1.0 ./Sending build context to Docker daemon 388.7MBStep 1/10 : FROM nginx ---&gt; 06144b287844Step 2/10 : MAINTAINER LeiLi.Zhang ---&gt; Running in 9fd7dfb2fdd4Removing intermediate container 9fd7dfb2fdd4 ---&gt; 8a1b63746989Step 3/10 : WORKDIR /usr ---&gt; Running in 600d738f937fRemoving intermediate container 600d738f937f ---&gt; 337b6327788cStep 4/10 : RUN mkdir jdk ---&gt; Running in e6533a32955eRemoving intermediate container e6533a32955e ---&gt; d18d4dd904d9Step 5/10 : ADD jdk1.8.0_172 /usr/jdk_8 ---&gt; 5a537bbcd84cStep 6/10 : ENV JAVA_HOME=/usr/jdk_8 ---&gt; Running in 3db8397affdbRemoving intermediate container 3db8397affdb ---&gt; 596cf637b2a7Step 7/10 : ENV JRE_HOME=$JAVA_HOME/jre ---&gt; Running in c16754fc69e0Removing intermediate container c16754fc69e0 ---&gt; b4b8531e6846Step 8/10 : ENV CLASSPATH=.:$CLASSPATH:$JAVA_HOME/bin/dt.jar:$JAVA_HOME/lib/tools.jar ---&gt; Running in d0be05ab96cfRemoving intermediate container d0be05ab96cf ---&gt; 606d80d75a70Step 9/10 : ENV PATH=$PATH:$JAVA_HOME/bin ---&gt; Running in eedc61c6889bRemoving intermediate container eedc61c6889b ---&gt; f5f3ca76f4dbStep 10/10 : CMD [&quot;java&quot; ,&quot;-version&quot;] ---&gt; Running in 5101d2bc1ebcRemoving intermediate container 5101d2bc1ebc ---&gt; 0118d7df925aSuccessfully built 0118d7df925aSuccessfully tagged nginx_jdk8:1.0``` &gt; 4.查看镜像是否成功 ```text$ docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEnginx_jdk8 1.0 cb6b8f24a584 24 seconds ago 496MBredis latest c188f257942c 7 weeks ago 94.9MBtime69/docker_study centos_base 7121cccf893c 3 months ago 982MBhello-world latest 4ab4c602aa5e 4 months ago 1.84kBtomcat latest 7671687227db 4 months ago 463MBnginx latest 06144b287844 4 months ago 109MBcentos latest 5182e96772bf 5 months ago 200MB 4.使用创建的镜像 123456$ docker run -it --name jdk8 nginx_jdk8:1.0 /bin/bash$ java -versionjava version &quot;1.8.0_172&quot;Java(TM) SE Runtime Environment (build 1.8.0_172-b11)Java HotSpot(TM) 64-Bit Server VM (build 25.172-b11, mixed mode) 5.导出镜像123$ docker save -o jdk_1.8_nginx.tar nginx_jdk8:1.0当面目录生成xxx.tar 6.导入镜像1$ docker load -i jdk_1.8_nginx.tar","tags":[{"name":"Docker","slug":"Docker","permalink":"https://cnkeep.github.io/tags/Docker/"}]},{"title":"user_group_chmod命令","date":"2018-07-07T10:27:00.000Z","path":"2018/07/07/09-user_group_chmod命令/","text":"user相关 &amp; group相关 &amp; chmod命令 本文中的命令参数选项并非全部选项，仅包含常用选项 用户系统&emsp;&emsp;linux是一个多用户多任务的系统，它有用户，用户组的概念，一个用户必须属于一个组。 user相关 与用户相关的配置文件1234567891011121314151617181920212223242526272829303132333435/etc/passwd 用户的配置文件/etc/shadow 用户的影子口令``` &gt; 与用户相关的命令 ```text1. useradd 功能：新建用户 用法：useradd [选项] 登录 useradd -D useradd -D [选项] 选项： -b, --base-dir BASE_DIR 新账户的主目录的基目录 -d, --home-dir HOME_DIR 新账户的主目录 -e, --expiredate EXPIRE_DATE 新账户的过期日期 -f, --inactive INACTIVE 新账户的密码不活动期 -g, --gid GROUP 新账户主组的名称或 ID -G, --groups GROUPS 新账户的附加组列表 -m, --create-home 创建用户的主目录 -M, --no-create-home 不创建用户的主目录 -p, --password PASSWORD 加密后的新账户密码 -r, --system 创建一个系统账户 示例： $ useradd -g zll_group -G root zll #新建用户zll, 用户主组zll_group, 附组root $ passwd zll #设置密码 $ id zll #查看用户zll的组 uid=1122(zll_group) gid=1125(zll_group) groups=1125(zll_group),0(root) $ useradd -s /sbin/nologin test #创建一个不能登录的用户2. usermod 功能：修改用户3. userdel 功能：删除用户 group相关 group相关配置文件 1/etc/group group相关命令 123456789101112131415161. groupadd 功能：新建用户组 用法：groupadd [选项] 用户组 选项： -g GID 指定新用户组的组标识号（GID）。 -o 一般与-g选项同时使用，表示新用户组的GID可以与系统已有用户组的GID相同。 示例： $ groupadd zll_group2. groupmod 功能：修改用户组3. groupdel 功能：删除用户组 文件权限&emsp;&emsp;linux的文件系统拥有严格访问权限，文件的可读可写，所属人，所属组都有严格的限制，我们也可以人为控制修改，这时候就要用到chmod命令了。我们先看看文件都有什么权限：123456789101112131415161718192021222324252627282930311：代表文件类型，为目录是为d2: 代表文件的宿主权限，由三位表示，4：可读，2：可写，1：可执行 3：代表文件的所属组权限4：代表其他用户的权限5：代表文件的所属用户6：代表文件的所属组``` &gt; chmod命令可以修改文件的相关权限 ```text语法： chmod [-cfvR] [--help] [--version] mode file...参数说明： [ugoa...][[+-=][rwxX]...][,...] 其中： u 表示该文件的拥有者，g 表示与该文件的拥有者属于同一个群体(group)者，o 表示其他以外的人，a 表示这三者皆是。 + 表示增加权限、- 表示取消权限、= 表示唯一设定权限。 r 表示可读取，w 表示可写入，x 表示可执行，X 表示只有当该文件是个子目录或者该文件已经被设定过为可执行。 其他参数说明： -c : 若该文件权限确实已经更改，才显示其更改动作 -f : 若该文件权限无法被更改也不要显示错误讯息 -v : 显示权限变更的详细资料 -R : 对目前目录下的所有文件与子目录进行相同的权限变更(即以递回的方式逐个变更) --help : 显示辅助说明 --version : 显示版本 示例： $ chmod +x start.sh #增加执行权限 chown可以修改文件的饿所属者 12345678910111213141516171819语法 chown [-cfhvR] [--help] [--version] user[:group] file...参数 : user : 新的文件拥有者的使用者 ID group : 新的文件拥有者的使用者组(group) -c : 显示更改的部分的信息 -f : 忽略错误信息 -h :修复符号链接 -v : 显示详细的处理信息 -R : 处理指定目录以及其子目录下的所有文件 --help : 显示辅助说明 --version : 显示版本实例 #将文件 file1.txt 的拥有者设为 users 群体的使用者 runoob : $ chown runoob:users file1.txt #将目前目录下的所有文件与子目录的拥有者皆设为 users 群体的使用者 lamport : $ chown -R lamport:users *","tags":[{"name":"Linux","slug":"Linux","permalink":"https://cnkeep.github.io/tags/Linux/"}]},{"title":"sed命令","date":"2018-07-07T09:23:00.000Z","path":"2018/07/07/08-sed命令/","text":"sed命令[注：文中的命令参数列表至列出了常用的几个，并不是全部] sed命令 功能 &emsp;&emsp;sed是一种流编辑器，它是文本处理中非常中的工具，能够完美的配合正则表达式使用, 可以完成文件内容的批量替换，删除等。 使用方式及参数列表 1234567891011121314151617181920212223242526sed [options] &apos;command&apos; file(s)参数列表： g 表示行内全面替换。 p 表示打印行。 i 修改源文件 w 表示把行写入一个文件。 x 表示互换模板块中的文本和缓冲区中的文本。 y 表示把一个字符翻译为另外的字符（但是不用于正则表达式） \\1 子串匹配标记 &amp; 已匹配字符串标记 匹配模式： ^ 匹配行开始，如：/^sed/匹配所有以sed开头的行。 $ 匹配行结束，如：/sed$/匹配所有以sed结尾的行。 . 匹配一个非换行符的任意字符，如：/s.d/匹配s后接一个任意字符，最后是d。 * 匹配0个或多个字符，如：/*sed/匹配所有模板是一个或多个空格后紧跟sed的行。 [] 匹配一个指定范围内的字符，如/[ss]ed/匹配sed和Sed。 [^] 匹配一个不在指定范围内的字符，如：/[^A-RT-Z]ed/匹配不包含A-R和T-Z的一个字母开头，紧跟ed的行。 \\(..\\) 匹配子串，保存匹配的字符，如s/\\(love\\)able/\\1rs，loveable被替换成lovers。 &amp; 保存搜索字符用来替换其他字符，如s/love/**&amp;**/，love这成**love**。 \\&lt; 匹配单词的开始，如:/\\&lt;love/匹配包含以love开头的单词的行。 \\&gt; 匹配单词的结束，如/love\\&gt;/匹配包含以love结尾的单词的行。 x\\&#123;m\\&#125; 重复字符x，m次，如：/0\\&#123;5\\&#125;/匹配包含5个0的行。 x\\&#123;m,\\&#125; 重复字符x，至少m次，如：/0\\&#123;5,\\&#125;/匹配至少有5个0的行。 x\\&#123;m,n\\&#125; 重复字符x，至少m次，不多于n次，如：/0\\&#123;5,10\\&#125;/匹配5~10个0的行。 示例 12345678910111.选项-i,匹配file文件中每一行的第一个book替换为books： $ sed -i &apos;s/book/books/g&apos; file2.-n选项和p命令一起使用表示只打印那些发生替换的行： $ sed -n &apos;s/test/TEST/p&apos; file 3.使用后缀 /g 标记会替换每一行中的所有匹配： $ sed &apos;s/book/books/g&apos; file #不会修改源文件 4.替换文件中所有的6379为6380并生成新文件 $ sed &apos;s/6379/6380/g&apos; redis.conf &gt; redis-6380.conf","tags":[{"name":"Linux","slug":"Linux","permalink":"https://cnkeep.github.io/tags/Linux/"}]},{"title":"时间同步","date":"2018-07-06T08:22:00.000Z","path":"2018/07/06/07-时间同步/","text":"Centos7时间不同步总是早8小时转载-CentOS 7系统时间与实际时间差8个小时 问题场景&emsp;&emsp;自己安装在虚拟机中的Centos时间总是早8个小时，每次都需要我手动改，太麻烦，今天终于决定彻底结局它。 结局方案123456789101112131415161718192021222324252627282930313233$ timedatectl #查看时间 Local time: 三 2018-10-24 23:30:48 CST Universal time: 三 2018-10-24 15:30:48 UTC RTC time: 三 2018-10-24 15:30:48 Time zone: Asia/Shanghai (CST, +0800) NTP enabled: n/aNTP synchronized: no RTC in local TZ: no DST active: n/a $ ls /usr/share/zoneinfo/ #查看时区列表Africa Australia Cuba Etc GMT-0 Indian Kwajalein Navajo posix ROK UTCAmerica Brazil EET Europe GMT+0 Iran Libya NZ posixrules Singapore WETAntarctica Canada Egypt GB Greenwich iso3166.tab MET NZ-CHAT PRC Turkey W-SUArctic CET Eire GB-Eire Hongkong Israel Mexico Pacific PST8PDT UCT zone.tabAsia Chile EST GMT HST Jamaica MST Poland right Universal ZuluAtlantic CST6CDT EST5EDT GMT0 Iceland Japan MST7MDT Portugal ROC US$ rm /etc/localtime #删除原有的时区rm：是否删除符号链接 &quot;/etc/localtime&quot;？y$ sudo ln -s /usr/share/zoneinfo/Universal /etc/localtime #设置新的时区$ timedatectl Local time: 三 2018-10-24 15:46:55 UTC Universal time: 三 2018-10-24 15:46:55 UTC RTC time: 三 2018-10-24 15:46:54 Time zone: Universal (UTC, +0000) NTP enabled: n/aNTP synchronized: no RTC in local TZ: no DST active: n/a[root@localhost default]# date2018年 10月 24日 星期三 15:46:58 UTC","tags":[{"name":"Linux","slug":"Linux","permalink":"https://cnkeep.github.io/tags/Linux/"}]},{"title":"centos7防火墙","date":"2018-07-06T05:08:00.000Z","path":"2018/07/06/06-centos7防火墙/","text":"centos防火墙1234567891011121314151617181920212223242526272829303132333435363738391、firewalld的基本使用启动： systemctl start firewalld查看状态： systemctl status firewalld 停止： systemctl disable firewalld禁用： systemctl stop firewalld 2.systemctl是CentOS7的服务管理工具中主要的工具，它融合之前service和chkconfig的功能于一体。启动一个服务：systemctl start firewalld.service关闭一个服务：systemctl stop firewalld.service重启一个服务：systemctl restart firewalld.service显示一个服务的状态：systemctl status firewalld.service在开机时启用一个服务：systemctl enable firewalld.service在开机时禁用一个服务：systemctl disable firewalld.service查看服务是否开机启动：systemctl is-enabled firewalld.service查看已启动的服务列表：systemctl list-unit-files|grep enabled查看启动失败的服务列表：systemctl --failed3.配置firewalld-cmd查看版本： firewall-cmd --version查看帮助： firewall-cmd --help显示状态： firewall-cmd --state查看所有打开的端口： firewall-cmd --zone=public --list-ports更新防火墙规则： firewall-cmd --reload查看区域信息: firewall-cmd --get-active-zones查看指定接口所属区域： firewall-cmd --get-zone-of-interface=eth0拒绝所有包：firewall-cmd --panic-on取消拒绝状态： firewall-cmd --panic-off查看是否拒绝： firewall-cmd --query-panic 那怎么开启一个端口呢添加firewall-cmd --zone=public --add-port=80/tcp --permanent （--permanent永久生效，没有此参数重启后失效）重新载入firewall-cmd --reload查看firewall-cmd --zone= public --query-port=80/tcp删除firewall-cmd --zone= public --remove-port=80/tcp --permanent","tags":[{"name":"Linux","slug":"Linux","permalink":"https://cnkeep.github.io/tags/Linux/"}]},{"title":"启动脚本中1,2&是什么","date":"2018-07-06T00:57:00.000Z","path":"2018/07/06/05-启动脚本中1,2&是什么/","text":"启动脚本末尾的2&gt;&amp;1 &amp;的含义脚本中2&gt;&amp;1 &amp;的含义 &emsp;&emsp;我们总是在启动脚本中会发现这样的 “2&gt;&amp;1 &amp;”的奇怪部分，那这部分到底是什么含义呢？ linux标准输入输出流的标示&emsp;&emsp;我们知道设备都有输入输出流，那么linux中是如何去标示它们的呢？1230: 标准stdin输入流1：标准stdout输出流2：标准stderr错误流 举例分析 来看这样一个例子 command >2>&1 & ```12345678910我们对命令进行拆分： ![&amp;](images/how_about_&amp;.png) ```text1：我们执行的命令2：命令执行的打印信息流重定向的设备，**/dev/null**表示一个空设备，即重定向到它后不显示任何信息3：代表标准stderr流4：代表标准stdout输出流，这里的**&amp;**表示不把1当做文件看待，不添加&amp;则会把1当做文件看待5：表示以后台job的形式执行命令6：把stderr流重定向到stdout流","tags":[{"name":"Linux","slug":"Linux","permalink":"https://cnkeep.github.io/tags/Linux/"}]},{"title":"重定向符号区别","date":"2018-07-05T22:56:00.000Z","path":"2018/07/06/04-重定向符号区别/","text":"&gt; 和 &gt;&gt; 区别 功能 &emsp;&emsp;都表示重定向到新的设备。 区别 两者的区别主要发生在重定向的设备已经存在时： &gt;会覆盖源文件 &gt;&gt;不会覆盖源文件，而是追加到源文件末尾","tags":[{"name":"Linux","slug":"Linux","permalink":"https://cnkeep.github.io/tags/Linux/"}]},{"title":"lrzsz上传下载工具","date":"2018-07-05T20:44:00.000Z","path":"2018/07/06/03-lrzsz上传下载工具/","text":"lrzsz上传下载工具应用场景&emsp;&emsp;我们使用linux时，经常需要与本机完成文件的传输，我们可以使用xftp, 但是还有更简单的命令。 lrzsz的使用 123$ yum install lrzsz$ rz #弹框选择文件上传到linux当前目录 $ sz [file] #从Linux下载文件","tags":[{"name":"Linux","slug":"Linux","permalink":"https://cnkeep.github.io/tags/Linux/"}]},{"title":"02_Docker命令","date":"2018-07-05T15:21:00.000Z","path":"2018/07/05/01-02_Docker命令/","text":"命令列表 删除Docker12$ sudo yum remove docker-ce$ sudo rm -rf /var/lib/docker 搜索可用镜像1$ docker search [name] 拉取镜像123$ docker pull [name]:[version]其中version可以省略 查看镜像列表1$ docker images 运行容器123456789101112$ docket start [container] 运行一个已经存在的容器$ docker run -p 8080:80 --name test -d [image] 利用镜像image新建一个容器并启动$ docker run -d -i -t [image] /bin/bash 后台运行防止直接退出docker run 等价于 docker create + docker startrun: 利用镜像image创建一个容器并启动-p: 端口映射，容器内80端口映射到外部的8080端口--name: 容器命名-d: 后台运行-v: 文件映射 查看容器列表1$ docker ps -a 进入容器1$ docker exec -it [container] /bin/bash 停止容器1$ docker stop [container] 重启docker守护进程12$ systemctl daemon-reload$ systemctl restart docker 查看容器运行日志1$ docker logs [container] 查看容器信息1$ docker inspect [container] 移除容器1234$ docker rm [container]移除所有容器$ docker rm $(docker ps -a -q) 通过Dockerfile构建镜像1$ docker build -it [image] [path] 通过容器构建镜像1234567$ docker commit -a [author] [container] [repository[:tag]]示例：$ docker commit -a &quot;LeiLi.Zhang&quot; -m &quot;jdk8,nginx1.15,tomcat8.5.23&quot; centos_7 my:centos_base通过容器centos_7构建一个镜像-a 作者-m 提交信息 镜像的导出导入12345导出$ docker save -o xxxx.tar [image]导入$ docker load -i xxx.tar 容器的导入导出12$ docker export [container] &gt; xxx.tar$ cat xxx.tar | docker import - [res]:[tag]： DockerHub1234567拉取镜像$ docker login$ docker pull [name:tag]上传镜像$ docker tag [image:tag] [username/repository:tag] 将本地镜像重命名为标准的名称$ docker push [username/repository:tag] 查看容器/镜像的分层1$ docker history [image] 网络操作123456$ docker network [option]|[contain]# ls 查看网卡# inspect 查看网络信息# create 创建新的网卡# connect 连接到新的网卡","tags":[{"name":"Docker","slug":"Docker","permalink":"https://cnkeep.github.io/tags/Docker/"}]},{"title":"grep命令","date":"2018-07-03T16:30:00.000Z","path":"2018/07/04/02-grep命令/","text":"grep命令[注：文中的命令参数列表至列出了常用的几个，并不是全部] grep命令 功能 &emsp;&emsp;Linux系统中grep命令是一种强大的文本搜索工具，它能使用正则表达式搜索文本，并把匹配的行打印出来. 使用方式及参数列表 1234567891011121314151617181920212223242526272829grep [option] pattern file参数列表： -a --text #不要忽略二进制的数据。 -A&lt;显示行数&gt; --after-context=&lt;显示行数&gt; #除了显示符合范本样式的那一列之外，并显示该行之后的内容。 -b --byte-offset #在显示符合样式的那一行之前，标示出该行第一个字符的编号。 -B&lt;显示行数&gt; --before-context=&lt;显示行数&gt; #除了显示符合样式的那一行之外，并显示该行之前的内容。 -c --count #计算符合样式的列数。 -C&lt;显示行数&gt; --context=&lt;显示行数&gt;或-&lt;显示行数&gt; #除了显示符合样式的那一行之外，并显示该行之前后的内容。 -d &lt;动作&gt; --directories=&lt;动作&gt; #当指定要查找的是目录而非文件时，必须使用这项参数，否则grep指令将回报信息并停止动作。 -e&lt;范本样式&gt; --regexp=&lt;范本样式&gt; #指定字符串做为查找文件内容的样式。 -E --extended-regexp #将样式为延伸的普通表示法来使用。 -f&lt;规则文件&gt; --file=&lt;规则文件&gt; #指定规则文件，其内容含有一个或多个规则样式，让grep查找符合规则条件的文件内容，格式为每行一个规则样式。 -F --fixed-regexp #将样式视为固定字符串的列表。 -G --basic-regexp #将样式视为普通的表示法来使用。 -h --no-filename #在显示符合样式的那一行之前，不标示该行所属的文件名称。 -H --with-filename #在显示符合样式的那一行之前，表示该行所属的文件名称。 -i --ignore-case #忽略字符大小写的差别。 -l --file-with-matches #列出文件内容符合指定的样式的文件名称。 -L --files-without-match #列出文件内容不符合指定的样式的文件名称。 -n --line-number #在显示符合样式的那一行之前，标示出该行的列数编号。 -q --quiet或--silent #不显示任何信息。 -r --recursive #此参数的效果和指定“-d recurse”参数相同。 -s --no-messages #不显示错误信息。 -v --revert-match #显示不包含匹配文本的所有行。 -V --version #显示版本信息。 -w --word-regexp #只显示全字符合的列。 -x --line-regexp #只显示全列符合的列。 -y #此参数的效果和指定“-i”参数相同 示例 12345678910111213141516171. 进程查找 $ ps -ef|grep redis 2. 查找指定进程个数 $ ps -ef|grep -c svn 3. 从文件中读取关键词进行搜索 $ cat test.txt | grep -f test2.txt 4. 找出已u开头的行内容 $ cat test.txt |grep ^u 5. 去除注释和空格显示 $ cat redis.conf |grep -v &quot;#&quot;|grep -v &quot;^$&quot; &gt; redis-template.conf 6. 显示匹配行后面几行 $ grep -A &apos;options&apos; my.cnf","tags":[{"name":"Linux","slug":"Linux","permalink":"https://cnkeep.github.io/tags/Linux/"}]},{"title":"01_Docker介绍与安装","date":"2018-07-03T15:10:00.000Z","path":"2018/07/03/01-01_Docker介绍与安装/","text":"Docker目录什么是DockerDocker用来干什么Docker相关的点安装Docker什么是Docker&emsp;&emsp;不同的应用程序可能会有不同的应用环境，如果把他们依赖的软件都安装在一个服务器上就要调试很久，而且很麻烦，还会造成一些冲突。这个时候你就要隔离，我们可以在服务器上创建不同的虚拟机在不同的虚拟机上放置不同的应用，但是虚拟机开销比较高。docker可以实现虚拟机隔离应用环境的功能，并且开销比虚拟机小。 除此之外，它还可以像jvm屏蔽操作系统的底层细节，让我们的应用在不同系统部署变得更加方便。 总结就是：Docker是一个便携的应用容器, 我们的程序可以运行在容器之中。 Docker用来干什么 更快的运行服务，更高效的利用机器资源，更多的服务发布 屏蔽不同系统之间的差异，可以做到处处可部署运行，避免多次搭建环境(再也不会出现开发环境好好地，线上跑不起来了) 容器化带来了安全隔离，不在因为一个服务挂，导致所有服务挂 Docker相关的点Kubernetes, jenkins 安装Docker 本文采用Centos7作为服务器安装Docker, 详情参考官方：https://docs.docker.com/install/linux/docker-ce/centos/. 卸载docker旧版本12345678910$ sudo yum remove docker \\ docker-client \\ docker-client-latest \\ docker-common \\ docker-latest \\ docker-latest-logrotate \\ docker-logrotate \\ docker-selinux \\ docker-engine-selinux \\ docker-engine 安装工具类123$ sudo yum install -y yum-utils \\ device-mapper-persistent-data \\ lvm2 配置docker仓库1234567891011$ sudo yum-config-manager \\ --add-repo \\ https://download.docker.com/linux/centos/docker-ce.repo 会报以下错误： Loaded plugins: fastestmirror adding repo from: https://download.docker.com/linux/centos/docker-ce.repo grabbing file https://download.docker.com/linux/centos/docker-ce.repo to /etc/yum.repos.d/docker-ce.repo Could not fetch/save url https://download.docker.com/linux/centos/docker-ce.repo to file /etc/yum.repos.d/docker-ce.repo : [Errno 14] curl$ 35 - &quot;TCP connection reset by peer 这是由于国内访问不到docker官方镜像的缘故可以通过aliyun的源来完成：1234567$ sudo yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo 出现以下内容则表示docker仓库配置成功： Loaded plugins: fastestmirror adding repo from: http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo grabbing file http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo to /etc/yum.repos.d/docker-ce.repo repo saved to /etc/yum.repos.d/docker-ce.repo 参考：Docker CE 镜像源站 安装社区版docker 1234567891011$ sudo yum install docker-ce 内容如下： Installed: docker-ce.x86_64 0:18.03.0.ce-1.el7.centos Dependency Installed: audit-libs-python.x86_64 0:2.7.6-3.el7 checkpolicy.x86_64 0:2.5-4.el7 container-selinux.noarch 2:2.42-1.gitad8f0f7.el7 libcgroup.x86_64 0 libtool-ltdl.x86_64 0:2.4.2-22.el7_3 pigz.x86_64 0:2.3.3-1.el7.centos policycoreutils-python.x86_64 0:2.5-17.1.el7 python-IPy.noarch Complete! 验证是否安装成功123456789101112启动docker：$ sudo systemctl start docker验证docker:$ sudo docker run hello-world则会出现以下异常：Unable to find image &apos;hello-world:latest&apos; locallylatest: Pulling from library/hello-world9bb5a5d4561a: Pulling fs layerdocker: error pulling image configuration: Get https://dseasb33srnrn.cloudfront.net/registry-v2/docker/registry/v2/blobs/sha256/e3/e38bc07ac18eSee &apos;docker run --help&apos;. 镜像加速 上面提到的错误也是网络问题：国内无法访问dockerhub, 我们配置一下加速地址，登录https://www.daocloud.io 注册账号,使用Docker 加速器或者注册阿里云开发者获取加速地址 1$ curl -sSL https://get.daocloud.io/daotools/set_mirror.sh | sh -s http://b481a1dd.m.daocloud.io 该脚本可以将 –registry-mirror 加入到你的 Docker 配置文件 /etc/docker/daemon.json 中（该文件不存在可手动建立）。 12345678910111213141516171819202122232425262728测试是否成功$ docker run hello-worldUnable to find image &apos;hello-world:latest&apos; locallylatest: Pulling from library/hello-worldd1725b59e92d: Pull complete Digest: sha256:523e382ab1801f2a616239b1052bb7ee5a7cce6a06cfed27ccb93680eacad6efStatus: Downloaded newer image for hello-world:latestHello from Docker!This message shows that your installation appears to be working correctly.To generate this message, Docker took the following steps: 1. The Docker client contacted the Docker daemon. 2. The Docker daemon pulled the &quot;hello-world&quot; image from the Docker Hub. (amd64) 3. The Docker daemon created a new container from that image which runs the executable that produces the output you are currently reading. 4. The Docker daemon streamed that output to the Docker client, which sent it to your terminal.To try something more ambitious, you can run an Ubuntu container with: $ docker run -it ubuntu bashShare images, automate workflows, and more with a free Docker ID: https://hub.docker.com/For more examples and ideas, visit: https://docs.docker.com/get-started/ 目录结构默认情况下Docker的存放位置为：/var/lib/docker可以通过docker info命令查看 创建容器时通过-v 命令可以指定容器与宿主机之间的目录映射关系 启动Docker服务 pull命令从远程仓库拉取指定镜像(Image)，或者通过Dockerfile构建(build)新的镜像 使用run或者create+start命令，利用镜像生成容器并启动，一个镜像可以生成多个容器 镜像可以通过save,load命令导入导出，完成镜像的共享转移","tags":[{"name":"Docker","slug":"Docker","permalink":"https://cnkeep.github.io/tags/Docker/"}]},{"title":"maven私服搭建","date":"2018-07-02T16:22:00.000Z","path":"2018/07/03/01-maven私服搭建/","text":"使用Docker搭建maven私服 标签：maven, nexus3 参考： Nexus安装、使用说明、问题总结 从Maven私服获取依赖 Maven私有仓库: 发布release版本报错： Maven私服:Docker安装nexus3 前言作为一个Java程序员不可避免的要使用到maven仓库，但是我们经常遇见这样的情形： 网络受限，无法下载远程仓库的jar 公司内部的jar，无法获取，只能手动安装 远程现在速度太慢 针对以上的问题，我们就需要自己搭建一个Maven私服仓库。 介绍私服是架设在局域网的一种特殊的远程仓库，目的是代理远程仓库及部署第三方构件。有了私服之后，当 Maven 需要下载构件时，直接请求私服，私服上存在则下载到本地仓库；否则，私服请求外部的远程仓库，将构件下载到私服，再提供给本地仓库下载。 安装配置Nexus为了方便操作，我们直接采用docker安装 安装12345678910111213141516$ mkdir /home/nexus3# 搜索镜像$ docker search nexus3#下载镜像$ docker pull sonatype/nexus3# 启动$ docker run -id \\ --name=nexus3 \\ --privileged=true \\ --restart=always \\ -p 8081:8081 \\ -v /home/nexus3:/var/nexus-data \\ sonatype/nexus3 等待几分钟后，访问http://{ip}:8081/ 默认用户名admin，密码admin123 配置 1.配置镜像地址12345678910111213141516171819202122232425262728293031323334353637383940 &lt;mirrors&gt; &lt;!-- 私服镜像 --&gt;&lt;mirror&gt; &lt;id&gt;nexus&lt;/id&gt; &lt;mirrorOf&gt;*&lt;/mirrorOf&gt; &lt;name&gt;Human Readable Name for this Mirror.&lt;/name&gt; &lt;url&gt;http://172.16.22.136:8081/nexus/content/groups/public/&lt;/url&gt; &lt;/mirror&gt; &lt;!-- 阿里云镜像 --&gt; &lt;mirror&gt; &lt;id&gt;alimaven&lt;/id&gt; &lt;name&gt;aliyun maven&lt;/name&gt; &lt;url&gt;http://maven.aliyun.com/nexus/content/groups/public/&lt;/url&gt; &lt;mirrorOf&gt;central&lt;/mirrorOf&gt; &lt;/mirror&gt; &lt;/mirrors&gt; &lt;profiles&gt; &lt;profile&gt; &lt;id&gt;dev&lt;/id&gt; &lt;repositories&gt; &lt;repository&gt; &lt;id&gt;nexus&lt;/id&gt; &lt;url&gt;http://172.16.22.136:8081/repository/maven-public/&lt;/url&gt; &lt;releases&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/releases&gt; &lt;snapshots&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/snapshots&gt; &lt;/repository&gt; &lt;/repositories&gt; &lt;/profile&gt; &lt;/profiles&gt; &lt;activeProfiles&gt; &lt;activeProfile&gt;dev&lt;/activeProfile&gt; &lt;/activeProfiles&gt; 2.配置用户名密码123456789101112&lt;servers&gt; &lt;server&gt; &lt;id&gt;maven-releases&lt;/id&gt; &lt;username&gt;admin&lt;/username&gt; &lt;password&gt;admin123&lt;/password&gt; &lt;/server&gt; &lt;server&gt; &lt;id&gt;maven-snapshots&lt;/id&gt; &lt;username&gt;admin&lt;/username&gt; &lt;password&gt;admin123&lt;/password&gt; &lt;/server&gt;&lt;/servers&gt; 3.配置项目pom.xml12345678910111213&lt;!--私服仓库--&gt;&lt;distributionManagement&gt; &lt;repository&gt; &lt;id&gt;maven-releases&lt;/id&gt; &lt;name&gt;Nexus Release Repository&lt;/name&gt; &lt;url&gt;http://172.16.22.136:8081/repository/maven-releases/&lt;/url&gt; &lt;/repository&gt; &lt;snapshotRepository&gt; &lt;id&gt;maven-snapshots&lt;/id&gt; &lt;name&gt;Nexus Snapshot Repository&lt;/name&gt; &lt;url&gt;http://172.16.22.136:8081/repository/maven-snapshots/&lt;/url&gt; &lt;/snapshotRepository&gt;&lt;/distributionManagement&gt; 4.发布1$ mvn deploy 登录到nexus上就可以看到刚才打包的jar, 如果遇到：Return code is: 400, ReasonPhrase: Repository does not allow upd ating assets: maven-releases. 这是因为重复发布导致的，需要设置：","tags":[{"name":"Docker","slug":"Docker","permalink":"https://cnkeep.github.io/tags/Docker/"}]},{"title":"cd命令","date":"2018-07-02T12:26:00.000Z","path":"2018/07/02/01-cd命令/","text":"cd命令1.命令功能 切换当前目录至指定目录下 2.命令格式1cd [dirpath] 3.示例 显示当前目录12$pwd/home/zll 切换目录到当前用户home目录下12345678$cd ~或者$cd ``` &gt; 切换目录到上一次目录下 ```text$cd - 切换到父目录下1$cd ..","tags":[{"name":"Linux","slug":"Linux","permalink":"https://cnkeep.github.io/tags/Linux/"}]},{"title":"pom引入lib目录下jar","date":"2018-06-12T08:29:00.000Z","path":"2018/06/12/04-pom引入lib目录下jar/","text":"pom文件引入lib目录下jar介绍当我们在使用springboot带给我们的便利的同时，也偶尔会遇到一些问题。比如无需安装而引入项目lib目录下的jar, 这可以通过pom配置做到 1234567&lt;dependency&gt; &lt;groupId&gt;htmlunit&lt;/groupId&gt; &lt;artifactId&gt;htmlunit&lt;/artifactId&gt; &lt;version&gt;2.21-OSGi&lt;/version&gt; &lt;scope&gt;system&lt;/scope&gt; &lt;systemPath&gt;$&#123;project.basedir&#125;/libs/htmlunit-2.21-OSGi.jar&lt;/systemPath&gt; &lt;/dependency&gt;","tags":[{"name":"others","slug":"others","permalink":"https://cnkeep.github.io/tags/others/"}]},{"title":"bower使用","date":"2018-06-11T07:58:00.000Z","path":"2018/06/11/03-bower使用/","text":"bower使用1.介绍在做前端相关的系统时，少不了各种js, css的库文件引用，各种版本的额依赖关系管理也是让人头疼，Linux有yum管理工具，那bower也是同样的包管理工具，用于管理前端各种的版本依赖和下载。 2.使用2.1bower安装 需要系统已安装nodejs1npm install bower -g 2.2基础命令 基础命令可以通过bower help查看 命令简介： cache: bower 缓存管理 help: 显示 Bower 命令的帮助信息 home: 通过浏览器打开一个包的 github 发布页 info: 查看包的信息 init: 创建 bower.json 文件 install: 安装包到项目 link: 在本地 bower 库建立一个项目链接 list: 列出项目已安装的包 lookup: 根据包名查询包的 URL prune: 删除项目无关的包 register: 注册一个包 search: 搜索包 update: 更新项目的包 uninstall: 删除项目的包 2.3使用bower下载库作为各种库的管理工具，bower 主要就是对它们下载和管理。举例，我们新建了一个项目，我们的项目需要如下前端库：jquery 1.11.1、bootstrap。针对项目我们可以依次执行如下命令：12bower install jquery#1.11.1 -savebower install bootstrap -save 执行完命令后，bower都会自动将对应的库文件下载到bower_components/目录下(包含依赖库) 2.4bower.json`文件我们在项目中使用时，因为库文件众多，都会采取配置文件指定的方式。可以通过bower init生成改配置文件：12345678910111213141516171819202122232425262728293031323334353637383940414243&#123; \"name\":\"\", //必须，如果需要注册包，则该包名唯一。 \"description\":\"\", //可选，包描述 \"main\":[], //可选，入口文件，bower本身不使用，供第三方构建工具会使用 //每种文件类型只能有一个。 \"ignore\":[], //可选，文件或目录列表。bower安装的时候将忽略该列表中的文件。 //bower是从git仓库或压缩包下载一个包，里面的文件并不一定全部需要。 \"dependencies\":[], //依赖包，name:value，value可以是包的semver //range(版本号范围)，也可以直接是一个包的git地址或压缩包地址。 \"devDependencies\":[], //开发依赖包，仅仅在开发过程中测试或者编译文档用，部署生产环境是不需要。 //格式和dependencies 相同 \"resolutions\":[], //包引用冲突自动使用该模块指定的包版本 //格式和dependencies 相同 \"overrides\" :[ //这个也很关键，可以覆盖一个包中的默认设置，比如main里面设定的入口文件 \"package-name\":&#123; //这样可以根据需要，让第三方工具只打包需要的文件。 \"main\":[] &#125; ], \"moduleType\":\"\", //可选，指定包采用那种模块化方式(globals,amd,node,es6,yui) \"private\":Boolean, //是否公开发布当前包,如果只是使用bower来管理项目的包，设置为true. \"license\":\"\", //授权方式(GPL-3.0,CC-BY-4.0.....) \"keywords\":[], //可选，方便注册后容易被其他人搜索到。 \"authors\":[], //作者列表 \"homepage\":[], //主页，包介绍页 \"repository\":&#123; //包所在仓库。 \"type\": \"git\", \"url\": \"git://github.com/foo/bar.git\" &#125;,｝ 版本的指定 ~1.2.3 ~1.2 ~1 ~代表最小的补丁更新范围 ~1.2.3 := &gt;=1.2.3 &lt;1.(2+1).0 := &gt;=1.2.3 &lt;1.3.0 ~1.2 := &gt;=1.2.0 &lt;1.(2+1).0 := &gt;=1.2.0 &lt;1.3.0 (Same as 1.2.x) ~1 := &gt;=1.0.0 &lt;(1+1).0.0 := &gt;=1.0.0 &lt;2.0.0 (Same as 1.x) ~0.2.3 := &gt;=0.2.3 &lt;0.(2+1).0 := &gt;=0.2.3 &lt;0.3.0 ~0.2 := &gt;=0.2.0 &lt;0.(2+1).0 := &gt;=0.2.0 &lt;0.3.0 (Same as 0.2.x) ~0 := &gt;=0.0.0 &lt;(0+1).0.0 := &gt;=0.0.0 &lt;1.0.0 (Same as 0.x) #: 代表具体的版本号 ^1.2.3 ^0.2.5 ^0.0.4§ ^ 代表从左开始第一个非零数为基础版本 ^1.2.3 := &gt;=1.2.3 &lt;2.0.0 ^0.2.3 := &gt;=0.2.3 &lt;0.3.0 ^0.0.3 := &gt;=0.0.3 &lt;0.0.4","tags":[{"name":"others","slug":"others","permalink":"https://cnkeep.github.io/tags/others/"}]},{"title":"安装jar到本地","date":"2018-06-11T06:21:00.000Z","path":"2018/06/11/02-安装jar到本地/","text":"手动安装jar到本地仓库12345678910111213141516171819&lt;dependency&gt; &lt;groupId&gt;com.oracle&lt;/groupId&gt; &lt;artifactId&gt;ojdbc14&lt;/artifactId&gt; &lt;version&gt;10.2.0.1.0&lt;/version&gt;&lt;/dependency&gt;#执行命令#-Dfile=jar包存放位置#-DgroupId=&lt;groupId&gt;#-DartifactId=&lt;artifactId&gt;#-Dversion=&lt;version&gt;#-Dpackaging=&lt;jar&gt; mvn install:install-file \\-Dfile=C:\\Users\\zll\\Desktop\\ojdbc14-10.2.0.1.0.jar \\-DgroupId=com.oracle \\-DartifactId=ojdbc14 \\-Dversion=10.2.0.1.0 \\-Dpackaging=jar","tags":[{"name":"others","slug":"others","permalink":"https://cnkeep.github.io/tags/others/"}]},{"title":"正则表达式","date":"2018-06-10T05:19:00.000Z","path":"2018/06/10/01-正则表达式/","text":"正则表达式介绍&nbsp;&nbsp;正则表达式描述了一种字符串匹配的模式，类似于一种通用模型，可以用来检查一个串是否含有某种子串、将匹配的子串替换或者从某个子串中取出符合某个条件的子串等。最近在研究git钩子时有用到，在此做笔记记录一下，如有错误，欢迎指正。 特殊字符所谓特殊字符，就是一些有特殊含义的字符，若要匹配这些特殊字符，必须要先使用反斜杠转义，下面列出正则表达式中的特殊字符： 例如要匹配字符串中是否包含()就需要这样写： 1\\(\\w+\\) 限定符限定符用来指定正则表达式的一个给定组件必须要出现多少次才能满足匹配。有 * 或 + 或 ? 或 {n} 或 {n,} 或 {n,m} 共6种。 正则表达式的限定符有： 定位符定位符使您能够将正则表达式固定到行首或行尾。它们还使您能够创建这样的正则表达式，这些正则表达式出现在一个单词内、在一个单词的开头或者一个单词的结尾。 定位符用来描述字符串或单词的边界，^ 和 $ 分别指字符串的开始与结束，\\b 描述单词的前或后边界，\\B 表示非单词边界。 正则表达式的定位符有： 元符号 示例 字符组1[a-zA-Z0-9]:表示大小写或者数字，区间采用横杠分开 枚举1(fix|feature|doc): 表示枚举fix,feature,doc其中任意一个满足即可 首尾匹配1^a\\w+z$ : ``^``表示匹配头部，``$``表示匹配尾部 git 提交规范 1(fix|feature|doc)+:(\\(\\w+\\))\\s+(\\w+): 满足&lt;type&gt;:(&lt;module&gt;) &lt;digest&gt;","tags":[{"name":"others","slug":"others","permalink":"https://cnkeep.github.io/tags/others/"}]},{"title":"XShell快捷键","date":"2018-04-15T15:52:00.000Z","path":"2018/04/15/006-XShell快捷键/","text":"XShell快捷键 清屏： ctrl+L 切换tab: ctrl+Tab 新建连接： ctrl+O 复制： ctrl+Insert 粘贴： shift+Insert","tags":[{"name":"tools","slug":"tools","permalink":"https://cnkeep.github.io/tags/tools/"}]},{"title":"idea安装lombok插件","date":"2018-04-15T15:29:00.000Z","path":"2018/04/15/005-idea安装lombok插件/","text":"idea安装lombok插件介绍 Lombok是一个可以通过简单的注解形式来帮助我们简化消除一些必须有但显得很臃肿的Java代码的工具，通过使用对应的注解，可以在编译源码的时候生成对应的方法。 官方地址:https://projectlombok.org/ github地址:https://github.com/rzwitserloot/lombok idea安装lombok插件尝试在线安装(本人操作失败) 尝试离线安装 获取资源包 https://github.com/mplushnikov/lombok-intellij-plugin/releases 或者 http://plugins.jetbrains.com/plugin/6317-lombok-plugin 下载对应的版本 安装 依次进入IDEA–&gt;Settings/Preferences–&gt;Plugins 选择install plugin from disk选中下载的插件，等待安装完成即可。 使用引入依赖&lt;dependency&gt; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt; &lt;artifactId&gt;lombok&lt;/artifactId&gt; &lt;version&gt;1.16.18&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt; &lt;/dependency&gt; 使用在对应的累活方法上使用对应的注解即可 Lombok有哪些注解 @Setter @Getter @Data @Log(这是一个泛型注解，具体有很多种形式) @AllArgsConstructor @NoArgsConstructor @EqualsAndHashCode @NonNull @Cleanup @ToString @RequiredArgsConstructor @Value @SneakyThrows @Synchronized 官网介绍","tags":[{"name":"tools","slug":"tools","permalink":"https://cnkeep.github.io/tags/tools/"}]},{"title":"eclipse中maven项目jar包不会自动下载解决办法","date":"2018-04-14T12:04:00.000Z","path":"2018/04/14/004-eclipse中maven项目jar包不会自动下载解决办法/","text":"eclipse中maven项目jar包不会自动下载解决办法 Eclipse中maven从远程仓库中下载jar包有时会很慢，有些甚至进度停止不动，这个时候我们可能会终止当前下载，但是终止jar包下载后会出现一个问题，再次打开Eclipse时，你会发现提示你项目中依赖的jar包找不到. 如下图所示，项目右键打—》Build Path—》Configure Build Path 打开项目的 Java Build Path 在 Libraries 页签下Maven Dependenicies 你会发现报错提示 依赖的jar包 missing如下图所示 此时我们可以通过如下方案解决: 找到我们的本地maven仓库目录 我的是 H:\\Java\\maven\\Repository 搜索出该目录下的*lastUpdated.properties文件并删除，如下图所示，可以通过模糊搜索匹配出这样的文件 Maven 更新当前项目，maven就会继续下载缺失的依赖jar包，直至缺失jar包下载完成，上述问题就解决了。 自动删除脚本 1234567891011# 1. 新建clean.bat# 2. 输入以下内容，修改REPOSITORY_PATH为自己的本地仓库路径# 3. 执行脚本set REPOSITORY_PATH=E:\\maven\\reporem 正在搜索...for /f &quot;delims=&quot; %%i in (&apos;dir /b /s &quot;%REPOSITORY_PATH%\\*lastUpdated&quot;&apos;) do ( del /s /q %%i)rem 搜索完毕pause","tags":[{"name":"tools","slug":"tools","permalink":"https://cnkeep.github.io/tags/tools/"}]},{"title":"windows激活","date":"2018-04-14T08:40:00.000Z","path":"2018/04/14/003-windows激活/","text":"windows激活系统安装完毕后，首先以管理员身份打开CMD命令行窗口，按下Win+X，选择命令提示符(管理员)。说明：kms.xspace.in是kms服务器地址，可能会失效，如果激活失败，可以自行搜索kms服务器地址，将kms.xspace.in替换成新的地址即可，比如换成kms.03k.org123456789win10专业版用户请依次输入：slmgr /ipk W269N-WFGWX-YVC9B-4J6C9-T83GXslmgr /skms kms.xspace.inslmgr /atowin10企业版用户请依次输入：slmgr /ipk NPPR9-FWDCX-D2C8J-H872K-2YT43slmgr /skms kms.xspace.inslmgr /ato 网上搜了一大堆。一点作用没有。结果看到这句话 换了一下还真成功了~将kms.xspace.in替换成新的地址即可，比如换成kms.03k.org 查看到期时间：slmgr.vbs -xpr 来自 https://tieba.baidu.com/p/5592942485?red_tag=0307692371&amp;traceid=","tags":[{"name":"tools","slug":"tools","permalink":"https://cnkeep.github.io/tags/tools/"}]},{"title":"idea快捷键","date":"2018-04-12T08:21:00.000Z","path":"2018/04/12/002-idea快捷键/","text":"Idea快捷键 激活 License Server: http://idea.iteblog.com/key.php 360打开 https://licensez.com/ 快捷键 切换工作空间： ctrl+alt+]/] 搜索类中的方法： ctrl+F12 搜索类： ctrl+N or 双击shift 替换： ctrl+R 全局搜索： ctrl+shift+R 格式化代码： ctrl+alt+L 移除不用的import: ctrl+alt+O 查看注释： ctrl+Q 提示： ctrl+shift+space,alt+/ 建议： alt+enter 调用层次： ctrl+H 重写、覆盖方法： ctrl+o 删除一行： ctrl+x 复制一行： ctrl+V/D 移动一行： alt+shift+up/down 快速surround by: ctrl+alt+T 大小写切换: ctrl+shift+U get/set: alt+insert 去除多余import: ctrl+alt+O 查看最近打开的文件: ctrl+E 自动补全分号，大括号: ctrl+shift+enter 查看方法的实现: ctrl+alt+B","tags":[{"name":"tools","slug":"tools","permalink":"https://cnkeep.github.io/tags/tools/"}]},{"title":"idea配置全局代码注释模板","date":"2018-04-11T07:57:00.000Z","path":"2018/04/11/001-idea配置全局代码注释模板/","text":"idea设置全局注释模板 作为程序员代码要与代码规范，规范中必不可少的就是代码注释了，但是每次都手敲太麻烦，今天我们就来看看如何利用idea来配置自动生成代码注释。 配置新建class时自动生成注释 1.进入setting设置，搜索template关键词，出现下面俩个选项，就是我们要操作的功能： 2.选中File and Code Template, 在右侧Files页面菜单下，找到Class, Interface, Enum, AnnotationType(idea支持直接检索) 3.在最右侧的编辑区，填写需要的注释格式，也可以添加相关变量。 4.新建Class文件，就会发现自动生成了代码注释。 配置快捷键自动生成注释&nbsp;&nbsp;上面的方式只会在新建的Class文件中自动添加注释，但是对已经存在的文件无能为了，需要问么手动添加，我们可以采用全局替换去实现，但是，这里我提供一种快捷键的方式，在引申到自动生成Logger代码段上。 1.进入setting设置，搜索template关键词 2.选中Live Templates 3.点击右侧加号，添加我们自定义的快捷操作： 4.选中Template Group,新建custom分组，再新建Live Template 5.配置相关规则 Abbreviation:填写我们的匹配触发规则，这里填写了/**,即但我们键盘输入/**时会触发生成注释； Description:填写说明，便于查看具体功能； Teamplate Text:填写具体的生成规则 6.配置应用的对象，我么选择最下面的No applicable contexts yet. Define, 选择java 7.Class文件中输入/**然后按Tab键就会生成注释。 配置自动生成Logger对象&nbsp;&nbsp;我们编写业务代码经常要记录日志，每个类中都要写Logger属性，能不能手动生成呢？ 可以的，利用快捷操作，就行输入sout自动生成打印语句一样。 1.进入setting设置，搜索template关键词 2.选中Live Templates 3.点击右侧加号，添加我们自定义的快捷操作： 4.选中Template Group,新建custom分组，再新建Live Template 5.配置相关规则 Abbreviation: 我们填写logger, 只要页面键入logger就会出现提示，回车后就可自动生成Logger对象。 Description:填写说明，便于查看具体功能； Teamplate Text: private static final Logger LOGGER = LoggerFactory.getLogger($CLASS$.class); 6.配置应用的对象，我么选择最下面的No applicable contexts yet. Define, 选择java 7.Class文件中输入logger然后出现提示，回车即可生成Logger对象。","tags":[{"name":"tools","slug":"tools","permalink":"https://cnkeep.github.io/tags/tools/"}]},{"title":"hook钩子","date":"2018-03-16T19:44:00.000Z","path":"2018/03/17/02-hook钩子/","text":"Git中的hook钩子程序原文: Git中文官网-8.3 自定义 Git - Git 钩子 Git钩子&nbsp;&nbsp;Git能在特定的重要动作发生时触发自定义的脚本，俗称钩子(hooks)，类似于事件回调机制。Git中存在两组这样的钩子： 客户端钩子：由诸如提交和合并这样的操作所调用。 服务端钩子：作用域诸如接收被推送的提交这样的联网操作。 安装钩子&nbsp;&nbsp;当我们clone或者init一个仓库后，会在目录下生成.git目录，该目录即我们的git管理目录，其内部存在一个hooks的子目录，该目录下默认防止了一些以.sample结尾的示例脚本，当我们要激活一个钩子时，只需要去掉后缀，改写脚本即可。脚本支持多种语言,可以在首行指定#!/usr/bin/env XXX 这些脚本只对当前项目发挥作用，而且每次都需要从其他仓库复制一份进来，我们可以修改全局的配置（以windows为例），在git安装目录下的mingw64\\share\\git-core\\templates\\hooks目录下存在着全局脚本，我们可以修改这里的脚本，这样每次init后当前仓库中的脚本就会从全局中复制过来。 客户端钩子 pre-commit: 执行git commit命令时触发，常用于检查代码风格 prepare-commit-msg: commit message编辑器呼起前default commit message创建后触发，常用于生成默认的标准化的提交说明 commit-msg: 开发者编写完并确认commit message后触发，常用于校验提交说明是否标准 post-commit: 整个git commit完成后触发，常用于邮件通知、提醒 applypatch-msg: 执行git am命令时触发，常用于检查命令提取出来的提交信息是否符合特定格式 pre-applypatch: git am提取出补丁并应用于当前分支后，准备提交前触发，常用于执行测试用例或检查缓冲区代码 post-applypatch: git am提交后触发，常用于通知、或补丁邮件回复（此钩子不能停止git am过程） pre-rebase: 执行git rebase命令时触发 post-rewrite: 执行会替换commit的命令时触发，比如git rebase或git commit –amend post-checkout: 执行git checkout命令成功后触发，可用于生成特定文档，处理大二进制文件等 post-merge: 成功完成一次 merge行为后触发 pre-push: 执行git push命令时触发，可用于执行测试用例 pre-auto-gc: 执行垃圾回收前触发 服务端钩子 pre-receive: 当服务端收到一个push操作请求时触发，可用于检测push的内容 update: 与pre-receive相似，但当一次push想更新多个分支时，pre-receive只执行一次，而此钩子会为每一分支都执行一次 post-receive: 当整个push操作完成时触发，常用于服务侧同步、通知 这些钩子有如下的关系(颜色深的为常用钩子)： 示例：提交后自动maven打包 如果目录下存在pom.xml则打包(需已安装maven)，不考虑打包失败的情况, 修改post-commit文件`text #!/bin/sh# An example hook script to prepare a packed repository for use overdumb transports.# To enable this hook, rename this file to “post-update”.if [ -f “pom.xml” ];then mvn clean -Dmaven.test.skip=true package installelse echo “[!Error]not found pom.xml” echo “==========================”fi`","tags":[{"name":"Git","slug":"Git","permalink":"https://cnkeep.github.io/tags/Git/"}]},{"title":"gitlab搭建","date":"2018-03-15T16:38:00.000Z","path":"2018/03/16/01-gitlab搭建/","text":"Docker方式安装gitlab 标签： Gitlab，Docker 开发中如果我们不希望代码托管在第三方平台，我们就可以自己搭建一套git服务端，这里采用流行的Gitlab和Docker搭建 1.安装gitlab1.1 环境介绍 Linux Centos, Linux localhost.localdomain 3.10.0-514.10.2.el7.x86_64 #1 SMP Fri Mar 3 00:04:05 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux docker Docker version 18.09.0, build 4d60db4 内存&amp;硬盘 内存&gt;4G, 硬盘&gt;20G 1.2 下载安装 1.镜像下载&amp;安装 拉取镜像1$ docker pull gitlab/gitlab-ce 建立容器映射文件夹 1234# 提前建立映射文件夹$ mkdir /home/gitlab/config$ mkdir /home/gitlab/data$ mkdir /home/gitlab/logs 启动容器 123456789101112131415$ docker run \\ --d \\ --privileged=true \\ --p 8443:443 \\ --p 8090:8090 \\ #web访问端口 --p 4422:4422 \\ #ssh访问端口 --name gitlab \\ --restart unless-stopped \\ -v /home/gitlab/config:/etc/gitlab \\ -v /home/gitlab/logs:/var/log/gitlab \\ -v /home/gitlab/data:/var/opt/gitlab \\ v /etc/localtime:/etc/localtime \\ gitlab/gitlab-ce:latest \\# 启动较为缓慢，需要等大约2分钟 2.配置端口123456789101112131415$ vi /home/gitlab/config/gitlab.rb## 修改http方式的端口密码 external_url &apos;http://172.16.22.135:8090&apos;## 修改ssh端口gitlab_rails[&apos;gitlab_shell_ssh_port&apos;] = 4422# 修改ssh端口$ docker exec -it gitlab /bin/bash$ vi /assets/sshd_config#修改ssh端口Port 4422#重启配置$gitlab-ctl reconfigure 参见：Gitlab SSH端口不生效 配置邮件服务 邮件服务配置 阿里云邮箱配置12345678910$ vi /home/gitlab/config/gitlab.rbgitlab_rails[&apos;smtp_enable&apos;] = truegitlab_rails[&apos;smtp_address&apos;] = &quot;smtp.mxhichina.com&quot;gitlab_rails[&apos;smtp_port&apos;] = 465gitlab_rails[&apos;smtp_user_name&apos;] = &quot;&lt;your user_name&gt;&quot;gitlab_rails[&apos;smtp_password&apos;] = &quot;&lt;your passwd&gt;&quot;gitlab_rails[&apos;smtp_domain&apos;] = &quot;&lt;your domain&gt;&quot;gitlab_rails[&apos;smtp_authentication&apos;] = &quot;login&quot;gitlab_rails[&apos;smtp_enable_starttls_auto&apos;] = truegitlab_rails[&apos;smtp_tls&apos;] = true 邮件服务测试12345678910111213141516171819202122232425262728293031$ docker exec -it gitlab /bin/bash#重启配置$gitlab-ctl reconfigureroot@8176a338c3ef:/# gitlab-rails console------------------------------------------------------------------------------------- GitLab: 11.6.5 (237bddc) GitLab Shell: 8.4.3 postgresql: 9.6.11-------------------------------------------------------------------------------------Loading production environment (Rails 5.0.7)irb(main):001:0&gt; Notify.test_email('1348555156@qq.com','subject','content').deliver_nowNotify#test_email: processed outbound mail in 173.5msSent mail to 1348555156@qq.com (2970.6ms)Date: Tue, 22 Jan 2019 14:40:57 +0000From: Gitlab &lt;zhangleili@cnkeep.cn&gt;Reply-To: Gitlab &lt;noreply@172.16.22.135&gt;To: 1348555156@qq.comMessage-ID: &lt;5c472b798ae1b_ae3fba61cca5f08927@8176a338c3ef.mail&gt;Subject: subjectMime-Version: 1.0Content-Type: text/html; charset=UTF-8Content-Transfer-Encoding: 7bitAuto-Submitted: auto-generatedX-Auto-Response-Suppress: All&lt;!DOCTYPE html PUBLIC \"-//W3C//DTD HTML 4.0 Transitional//EN\" \"http://www.w3.org/TR/REC-html40/loose.dtd\"&gt;&lt;html&gt;&lt;body&gt;&lt;p&gt;content&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;=&gt; #&lt;Mail::Message:70069490324460, Multipart: false, Headers: &lt;Date: Tue, 22 Jan 2019 14:40:57 +0000&gt;, &lt;From: Gitlab &lt;zhangleili@cnkeep.cn&gt;&gt;, &lt;Reply-To: Gitlab &lt;noreply@172.16.22.135&gt;&gt;, &lt;To: 1348555156@qq.com&gt;, &lt;Message-ID: &lt;5c472b798ae1b_ae3fba61cca5f08927@8176a338c3ef.mail&gt;&gt;, &lt;Subject: subject&gt;, &lt;Mime-Version: 1.0&gt;, &lt;Content-Type: text/html; charset=UTF-8&gt;, &lt;Content-Transfer-Encoding: 7bit&gt;, &lt;Auto-Submitted: auto-generated&gt;, &lt;X-Auto-Response-Suppress: All&gt;&gt;irb(main):002:0&gt;","tags":[{"name":"Git","slug":"Git","permalink":"https://cnkeep.github.io/tags/Git/"}]},{"title":"git的原理","date":"2018-03-15T15:27:00.000Z","path":"2018/03/15/01-git的原理/","text":"git的内部原理Git的内部原理&emsp;&emsp;Git本质上是一个内容寻址文件系统，从内部来看它是一个key-value数据库，可以插入任意内容，并返回一个键值，可以通过该键值在任何时候再取出该内容。接下来我们初始化一个本地仓库看看细节原理。 原理解析目录结构首先我们初始化一个本地仓库后，发现生成一个.git文件夹，git的存储和操作都是在操作这些文件(还记得它是一个文件系统吗)我们来看看目录结构以及每个目录的作用：12345678910111213141516├─HEAD 保存当前指向的分支├─config 保存当前项目的git配置选项├─index 保存暂存区信息├─COMMIT_EDITMSG 保存提交记录├─hooks├─logs 提交记录│ ├─HEAD│ └─refs├─info│ └─exclude├─objects git的核心数据存储│ ├─info│ └─pack└─refs 存储指向数据 (分支) 的提交对象的指针 ├─heads └─tags 上面这些目录中最重要的就是index和HEAD, objects和refs, logs这几个部分 原理分析接下来我们从add和commit命令来看git的运行原理。 为什么是内容寻址文件系统 &emsp;&emsp;我们前面提到Git是一个内容寻址文件系统，那么为什么这么说呢？其实Git是采用Key-Value数据存储原理，为每个数据通过SHA-1计算出一个键值key，采用key的前2位分目录创建目录，key的剩余部分为文件名，将原来的内容压缩作为文本内容存储。这样只要我们知道一个key就能轻易找到文件并还原出原文件内容，这就是.git/objects目录的内容结构。我们可以使用git cat-file -p key查看文件内容。 git的三个区域 介绍之前我们先要了解git相关的三个目录区域： 工作区(working directory): 对应我们真是操作的文件目录 暂存区(staged或者叫index): 对应.git/index 本地仓库(repository): 对应.git/objects目录它们之间的关系如图所示： git中的对象 Git中的对象有Commit, Tree, Blob, Tag四种对象，前三种最重要，我们先介绍着几个对象的作用，后面再讲Git是如何通过这些对象完成版本控制的。 Commit: 记录提交记录的信息，包含author, message, parent(前一个commit对象指针)，Tree对象指针 Tree: 用于记录目录结构的一组指针，包含文件类型，文件指针(Tree或者Blob),文件名 Blob: 存储压缩后的内容它们之间的关系如图所示： 正向操作 接下来就最常见的正常操作add，commit命令来看Git的工作原理12345678910111213141516171819202122232425262728293031323334353637383940414243$ git init #初始化一个本地仓库 $ vi readme.txt #创建一个文件$ This is readme. $ find .git/objects/ -type f #查看对象库，为空$ git ls-files --stage #查看暂存区信息，即.git/index内容$ git add readme.txt #文件放入暂存区，这一步会将readme.txt放入对象库并生成key存储在.git/index中$ find .git/objects/ -type f #查看对象库，生成新的文件.git/objects/52/cb6cdb81a64344370c918a301eb153035f915a$ git cat-file -t 52cb6cdb81a64344370c918a301eb153035f915a #查看对象类型blob #blob对象，存储文件内容$ git cat-file -p 52cb6cdb81a64344370c918a301eb153035f915a #查看文件内容This is readme. #文件内容与我们写入的一致$ git ls-file --stage #查看缓存区100644 52cb6cdb81a64344370c918a301eb153035f915a 0 readme.txt #记录着blob对象$ git commit -m &quot;add readme.txt&quot; #提交暂存区到本地仓库中$ find .git/objects/ -type f #查看对象库.git/objects/52/cb6cdb81a64344370c918a301eb153035f915a #Blob对象.git/objects/9b/6f349ad11c58d0e930dd12134a3e536bf5b057 #Tree对象.git/objects/d6/1479babc75f198416b5fd3caece3495b976391 #Commit对象$ vi readme.txtThis is readme.add line.$ git status #查看工作区和暂存区的文件状态，可以知道那些已经被暂存，那些没有On branch masterChanges not staged for commit: (use &quot;git add &lt;file&gt;...&quot; to update what will be committed) (use &quot;git checkout -- &lt;file&gt;...&quot; to discard changes in working directory) modified: readme.txtno changes added to commit (use &quot;git add&quot; and/or &quot;git commit -a&quot;)$ git add readme.txt $ git commit -m &quot;change readme.txt&quot;$ find .git/objects/ -type f #查看对象库.git/objects/3a/3cf98cad23cb084fa1753bee60e9f97d08e317 #Tree对象.git/objects/52/cb6cdb81a64344370c918a301eb153035f915a #Blob对象，readme.txt:v1.git/objects/62/c1331ab1c19f93dd776e894404aebc6de14b85 #Blob对象，readme.txt:v2.git/objects/9b/6f349ad11c58d0e930dd12134a3e536bf5b057 #Tree对象，.git/objects/a6/08354310f5aec4436a99feab92c96c35fa895c #Commit对象，commit:2.git/objects/d6/1479babc75f198416b5fd3caece3495b976391 #Commit对象, commit:1 通过上面的操作我们可以分析出原理： git add: 通过将文件生成新的Blob对象，并提交到暂存区，写.git/index文件 git commit:通过暂存区生成Tree对象，Commit对象，提交到版本库，写.git/logs目录 git会为每一个的更改都保存一个副本经过一系列操作，最终形成如下结构: 逆向操作 所谓逆向操作，就是指撤销之前的操作，来分析一下Git的处理原理, 即checkout和reset命令。我们以撤销readme.txt的修改为例。12345678910111213141516171819202122232425262728293031323334353637383940414243444546$ git log #查看当前的commit结点commit a608354310f5aec4436a99feab92c96c35fa895c (HEAD -&gt; master)Author: TIME69 &lt;zhangleili924@gmail.com&gt;Date: Wed Sep 19 01:48:36 2018 +0800 change readme.txtcommit d61479babc75f198416b5fd3caece3495b976391Author: TIME69 &lt;zhangleili924@gmail.com&gt;Date: Wed Sep 19 01:38:42 2018 +0800 add readme.txt$ git rest HEAD^ #将版本库回退到上一个版本，此操作只是将HEAD指针指向前一个Commit对象而已,并将暂存区会退到上一个版本Unstaged changes after reset: #可以看到撤销了暂存区的更改M readme.txt$ git log #查看当前Head, 看到上次更改撤销了commit d61479babc75f198416b5fd3caece3495b976391 (HEAD -&gt; master)Author: TIME69 &lt;zhangleili924@gmail.com&gt;Date: Wed Sep 19 01:38:42 2018 +0800 add readme.txt $ git ls-files --stage #查看暂存区100644 52cb6cdb81a64344370c918a301eb153035f915a 0 readme.txt$ git cat-file -p 52cb6cdb81a64344370c918a301eb153035f915a #暂存区内容已经回退，但是工作区呢？This is readme.$ git diff --stage #查看暂存区和版本库的差异，发现没有差异，是相同的$ cat readme.txt #发现工作区没有回退This is readme.add line.$ git status #On branch masterChanges not staged for commit: (use &quot;git add &lt;file&gt;...&quot; to update what will be committed) (use &quot;git checkout -- &lt;file&gt;...&quot; to discard changes in working directory) modified: readme.txtno changes added to commit (use &quot;git add&quot; and/or &quot;git commit -a&quot;)$ git checkout readme.txt #将工作区回退到上一个版本$ cat readme.txt #发现工作区内容变回来了This is readme. 看似很完美，但是这时候有人会说了那我不想撤销了咋办，我找不到记录了吗？怎么会，Git帮你存着呢。所有的提交记录Git都记录下来了(保存在.git/logs目录下的相应文件里)，不用怕丢，我们可以通过git reflog命令查看立即记录。1234$ git reflogd61479b (master) HEAD@&#123;6&#125;: reset: moving to HEAD^a608354 HEAD@&#123;7&#125;: commit: change readme.txtd61479b (master) HEAD@&#123;8&#125;: commit (initial): add readme.txt 我们知道Git操作的都是指针而已，那我们只要把HEAD指针指向下一个Commit结点不就完了，完美！1234$ git reset a608354 #暂存区Unstaged changes after reset:M readme.txt$ git checkout readme.txt #工作区 最终我们解决的问题，只要搞懂了底层机理是不是一下简单了许多，我们把工作区，暂存区，版本库之间的转换总结为下面这一张图： 内存压缩 我们上面说到git会对每一次的提交都保存一个副本Blob对象，那我们想一下那不是有很多冗余，很占存储空间，那能不能像svn那样存储变化的部分呢？其实Git已经考虑到了这一点，所以其内部有一个碎片处理的gc操作，类似于svn，但却有所区别。12345678910111213141516171819202122232425$ git gc #手动执行碎片处理Counting objects: 5, done.Delta compression using up to 4 threads.Compressing objects: 100% (3/3), done.Writing objects: 100% (5/5), done.Total 5 (delta 1), reused 0 (delta 0)$ find .git/objects/ -type f #查看对象库，发现pack目录多了文件，之前的object文件不见了.git/objects/info/packs.git/objects/pack/pack-57cbdc16b7fd157d77451245ef44ba68ea567e14.idx.git/objects/pack/pack-57cbdc16b7fd157d77451245ef44ba68ea567e14.pack$ git verify-pack -v .git/objects/pack/pack-57cbdc16b7fd157d77451245ef44ba68ea567e14.idx #查看pack文件a2d7df5381d7c3086ba998c6e7528725082e7e92 commit 218 157 12a608354310f5aec4436a99feab92c96c35fa895c commit 73 79 169 1 a2d7df5381d7c3086ba998c6e7528725082e7e92d61479babc75f198416b5fd3caece3495b976391 commit 167 125 24862c1331ab1c19f93dd776e894404aebc6de14b85 blob 26 33 373a06a3e45fb33f83522f2459baf4b2d6ebfc196bf tree 38 49 406ffd655096935b0b00e7eac2190ac0e61ea978e2e blob 33 40 4553a3cf98cad23cb084fa1753bee60e9f97d08e317 tree 38 49 4959b6f349ad11c58d0e930dd12134a3e536bf5b057 tree 38 48 54452cb6cdb81a64344370c918a301eb153035f915a blob 16 24 592non delta: 8 objectschain length = 1: 1 object.git/objects/pack/pack-57cbdc16b7fd157d77451245ef44ba68ea567e14.pack: ok 我们看看这两个文件是什么作用： .pack 是包文件，这个文件包含了从文件系统中移除的所有对象的内容 .idx是索引文件，这个文件包含了包文件的偏移信息值得注意的是，git不同于svn的是最后一个版本保存的是完成的文件内容，之前的版本保存的是差异部分，因为git认为这样更高效。 总结我们分析了Git的内部机理，我们来总结一下： Git是一个内容寻址文件系统, 会对每一份内容生成校验和，以便通过校验和再次获取原有数据 Git本质是一个Key-Value数据存储方式 Git存在三个区：工作区，暂存区，版本库，我们操作git就是操作这三个区域 Git理论上会针对每一次修改创建一个副本，但是为了减少存储空间会压缩采用存储差异 Git内部包含这些对象: Tag, Commit(记录提交信息，上一次提交的指针,Tree指针), Tree(记录文件列表，Tree+Blob), Blob(内容对象)","tags":[{"name":"Git","slug":"Git","permalink":"https://cnkeep.github.io/tags/Git/"}]},{"title":"ssh协议配置免密码提交","date":"2018-03-07T00:34:00.000Z","path":"2018/03/07/06-ssh协议配置免密码提交/","text":"ssh协议配置免密码提交 标签：Git, ssh, windows 介绍我们平时在使用git进行版本控制时，为了避免每次都输入密码，可以为账号添加ssh key,这样就可以避免输入账号密码的繁琐步骤。 配置ssh key 笔者这里介绍windows环境(linux环境配置类似)，笔者本地配置了多个远程仓库服务，所以这里是一种通用的配置方式 1.生成公钥123456789101112131415161718192021222324252627282930#任意目录打开git bash##为账号生成公钥$ ssh-keygen -t rsa -C &apos;1348555156@qq.com&apos;Generating public/private rsa key pair.##指定公钥文件的生成位置，这里配置为用户的主目录（前面括号中提示的位置,.ssh为ssh相关的配置目录），文件名随意指定Enter file in which to save the key (/c/Users/zll/.ssh/id_rsa): /c/Users/zll/.ssh/personal_rsa## 键入密码，可选Enter passphrase (empty for no passphrase):Enter same passphrase again:Your identification has been saved in /c/Users/zll/.ssh/personal_rsa.Your public key has been saved in /c/Users/zll/.ssh/personal_rsa.pub.The key fingerprint is:SHA256:e2fhKJwQ6vlWGjeHHXIUmoDefqnpGWBGt2XcieqiINs zhangleili@wxchina.comThe key&apos;s randomart image is:+---[RSA 2048]----+| .. .. || . o =.. || ...o B.o || ..o.*. o || =.+ S* .. || + +ooBooo . ||o + oO=oo + ||.+ . o=o o o ||. E o+ |+----[SHA256]-----+## 查看公钥$ cat /c/Users/zll/.ssh/personal_rsa.pubssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDRxhEozNNvtwL+CauYrs5fXznTvHpjzneNUG4RtEVrCkIxn8ld4W9VuFJ9UbC3cKIxB43svDiQqdnDPM08A8RfGDX0Jt686orkk7DAJerEEewoYollxcz79CHy17DhJ58yyz7zhVWW9sht/H6hEZuulliTtZIvtieH3xnL0zZVt/VWKk42pC+L/gEPWNDI0nkgmkVpKHaTxX503+Vh6tGJFf92Xg+t7UnqE+zz2WvZ4XTtX1Fli1y3ES7xS7ZwU5LwreDSrgT0u8EIk5HLCwcS5//9ZHJgkBGY3cdrgN0BUviNdWnYd4fI+4/Qb0C+acuMb/1ow1VxEKlE+6YMr6B7 XXXXXX@wxchina.com 2.服务端配置ssh key 登录到服务端，将上一步中的cat **.pub的输出内容粘贴复制到服务端 3.配置不同的服务端账号和地址 1.配置用户 1234567891011121314151617181920# 配置用户$ git config --global --add user.name &quot;&lt;username&gt;&quot;$ git config --global --add user.email &quot;&lt;email&gt;&quot;或者vi ~/.gitconfig##增加name和email[user] name = ******* email = ********* name = ******** email = *********[http] sslVerify = false[url &quot;https://&quot;] insteadOf = git://[credential] helper = store 2.配置服务端地址123456789101112131415161718192021222324252627#进入用户主目录，第一步中有提示$ cd /c/Users/zll/.ssh/#新建config配置文件$ touch config$ vi config# 配置github.comHost github.com User cnkeep HostName github.com IdentityFile C:\\Users\\zll\\.ssh\\id_rsa PreferredAuthentications publickey # 配置私有gitlabHost 172.16.22.135 User zhangleili #指定用户，在~/.gitconfig中配置的用户名 HostName 172.16.22.135 #指定服务器地址 Port 4422 #指定服务器端口(默认22) IdentityFile C:\\Users\\zll\\.ssh\\personal_rsa #指定公钥的文件 PreferredAuthentications publickey #指定验证策略# 码云Host gitee.com HostName gitee.com IdentityFile C:\\Users\\zll\\.ssh\\mayun_rsa PreferredAuthentications publickey 3.测试配置是否可用123456$ ssh -T git@github.comWarning: Permanently added the RSA host key for IP address &apos;52.74.223.119&apos; to the list of known hosts.Hi cnkeep! You&apos;ve successfully authenticated, but GitHub does not provide shell access.$ ssh -T git@172.16.22.135 Welcome to GitLab, @cnkeep! 4.ssh方式拉取项目12345678910$ git clone ssh://git@172.16.22.135:4422/developer/test.gitCloning into &apos;test&apos;...The authenticity of host &apos;[172.16.22.135]:4422 ([172.16.22.135]:4422)&apos; can&apos;t be established.ECDSA key fingerprint is SHA256:yvJ524f2Cxd60O4onSsE4K8TAtgWQISzWM+g6wi+H7Y.Are you sure you want to continue connecting (yes/no)? yesWarning: Permanently added &apos;[172.16.22.135]:4422&apos; (ECDSA) to the list of known hosts.remote: Enumerating objects: 3, done.remote: Counting objects: 100% (3/3), done.remote: Total 3 (delta 0), reused 0 (delta 0)Receiving objects: 100% (3/3), done. 结束语 这些都是笔者在搭建私有gitlab是亲自测试过的操作，可能因为不同环境有所差异。如有错误欢迎指正！","tags":[{"name":"Git","slug":"Git","permalink":"https://cnkeep.github.io/tags/Git/"}]},{"title":"Https协议配置免输密码","date":"2018-03-05T21:29:00.000Z","path":"2018/03/06/05-Https协议配置免输密码/","text":"Https方式clone的项目每次都要输入用户名密码使用git从远程仓库clone下来的项目时，连接如果是https://, 而不是git@git (ssh)的形式时，我们每次git pull/push到远程仓库时，总提示需要输入账号和密码，太麻烦了，有没有办法呢？ 有！ 解决方案： 12git bash进入项目目录，输入：git config --global credential.helper store 完成配置后，再操作一次git pull, 然后提示输入账号密码，这次输入后就不需要再次输入密码了！","tags":[{"name":"Git","slug":"Git","permalink":"https://cnkeep.github.io/tags/Git/"}]},{"title":"提交规范","date":"2018-03-04T17:03:00.000Z","path":"2018/03/05/04-提交规范/","text":"Git的提交规范参考： 阮一峰-Commit message 和 Change log 编写指南 并非严格按照此规范执行，可以实际情况实际定制规则，主要为了规范化。 提交格式12345&lt;type&gt;(&lt;scope&gt;) : &lt;subject&gt; &lt;空行&gt; &lt;body&gt; &lt;空行&gt; &lt;footer&gt; 其中 type 的值可以有很多，下面有几个我们常用到的 feature :新功能 fixbug:修复bug doc : 文档改变 style : 代码格式改变 refactor :某个已有功能重构 performance:性能优化 test :增加测试 chore: 修改了改变构建流程、或者增加依赖库、工具等 build :改变了build工具 如 grunt换成了 npm revert: 撤销上一次的 commit scope :用来说明此次修改的影响范围 可以随便填写任何东西,我推荐使用下列 all ：表示影响面大 ，如修改了网络框架 会对真个程序产生影响 loation： 表示影响小，某个小小的功能 module：表示会影响某个模块 如登录模块、首页模块 、用户管理模块等等 subject: 用来简要描述本次改动，概述就好了body:具体的修改信息 应该尽量详细footer: 附加信息，例如fix #*;issue #;ref #*** 1234567891011示例： [feature](user): 新增用户管理 description: 针对用户…… 示例： [fixbug](sqlmapper): issue #1252 sql error desctiption: 修复sql问题 使用插件校验提交信息(validate-commit-msg)依赖：安装npm, 安装node, ghooks, validate-commit-msg 1.跳转到项目根目录下 2.安装ghooks123456npm install ghooks --save-dev``` &gt; 3.安装[validate-commit-msg](https://github.com/conventional-changelog-archived-repos/validate-commit-msg) ```textnpm install --save-dev validate-commit-msg 4.生成package.json1npm init --yes 5.添加hooks配置123456在package.json中增加：&quot;config&quot;: &#123; &quot;ghooks&quot;: &#123; &quot;commit-msg&quot;: &quot;validate-commit-msg&quot; &#125; &#125; 6.测试 规范化辅助插件(commitizen) 1.全局安装commitizennode模块1npm install -g commitizen 2.在项目目录下运行命令1commitizen init cz-conventional-changelog --save --save-exact 3.此时可能会报找不到package.json的错误,使用下面命令来自动生成一个项目的package,然后在运行2中的命令.1npm init --yes 4.运行完以上一律使用git cz 代替git commit来提交代码,同时会显示一下选项来自动生成符合格式的commit message.123456789101112131415$ git czcz-cli@2.10.1, cz-conventional-changelog@2.1.0Line 1 will be cropped at 100 characters. All other lines will be wrapped after 100 characters.? Select the type of change that you&apos;re committing: (Use arrow keys)&gt; feat: A new feature fix: A bug fix docs: Documentation only changes style: Changes that do not affect the meaning of the code (white-space, formatting, missing semi-colons, etc) refactor: A code change that neither fixes a bug nor adds a feature perf: A code change that improves performance test: Adding missing tests or correcting existing tests(Move up and down to reveal more choices)","tags":[{"name":"Git","slug":"Git","permalink":"https://cnkeep.github.io/tags/Git/"}]},{"title":"git的常见命令","date":"2018-03-03T16:42:00.000Z","path":"2018/03/04/03-git的常见命令/","text":"Git的简单使用前言&emsp;&emsp;开发过程当中难以避免的要使用到版本控制工具，以前使用的SVN, 但随着Git的出现，它以及其优良的特性迅速火热，本节我们就来简单了解一下。 介绍&emsp;&emsp;Git是一个开源的分布式版本控制系统(本质是一个内容寻址文件系统，后面章节会做介绍)，用于敏捷高效地处理任何或小或大的项目。Git 与常用的版本控制工具 CVS, Subversion 等不同，它采用了分布式版本库的方式，不必服务器端软件支持（这点很重要，他可以在服务端无法访问的时候提交到本地仓库，等网络恢复后推送到远程仓库）。 Git 与 SVN的区别 简单使用初始化命令 init 初始化仓库1$ git init clone 克隆仓库12$ git clone https://github.com/repname$ git clone https://github.com/repname myrep #myrep作为本地仓库名 config 配置1234$ git config --list #查看配置$ git config --system user.name #系统级配置$ git config --global user.name #全局配置，系统用户级$ git config user.name #仓库级配置 remote 远程仓库12345$ git remote -v #查看远程仓库详细信息$ git remote show origin #查看远程仓库$ git remote add pb https://github.com/pb #添加远程仓库映射$ git remote rename pb paul #重命名远程仓库映射$ git remote rm paul #移除远程仓库映射关系 ### 基础命令 status 查看文件状态123$ git status$ git status -s #状态简览$ git ls-files --stage #查看缓存区索引文件内容，即.git/index add 暂存已修改文件12$ git add filename$ git add -A #暂存所有已修改文件 commit 提交更新12$ git commit -m &quot;commit message&quot;$ git commit -a -m &quot;commit message&quot; #跳过暂存区，直接提交 push 推送到远程仓库12$ git push$ git push origin master #将本地master分支推动到远程origin仓库 fetch 从远程仓库拉取数据1$ git fetch [remote-name] merge 合并分支123$ git merge --no-ff #推荐的合并方式，会做作一个新的提交，便于历史查询 $ git merge hotfix #把 hotfix 分支，合并到当前分支$ git mergetool #图形化解决冲突的工具 pull ( fetch + merge ) 常用命令 diff 查看修改12$ git diff #比较 暂存区－工作区$ git diff --staged #比较 仓库－暂存区 log 查看提交历史1234567$ git log$ git log --stat #展示提交的简略统计信息$ git log --oneline #简要显示$ git log --grep #类似grep命令，支持正则查找$ git log --since/after/before/until #只是时间点查找$ git log $ git reflog #回退版本后看不到之后的历史记录，此命令可以完成该功能 撤销操作12345$ git commit --amend #重新提交$ git reset HEAD filename #取消暂存的文件$ git reset -soft #暂存区-&gt;工作区, 类似于checkout$ git reset --mixed #版本库-&gt;暂存区 $ git reset --hard #版本库-&gt;暂存区-&gt;工作区 checkout1234567$ git checkout &lt;filename&gt;# 会覆盖工作区文件# 如果暂存区有改动的文件，则从暂存区到工作区# 如果暂存区无改动的文件，则从仓库到工作区#新建分子并切换至新分支$ git checkout -b &lt;branch_name&gt; tag 打标签1234567$ git tag #查看标签$ git tag -a v1.4 -m &quot;my version 1.4&quot; #创建附注标签$ git tag v1.4 #创建轻量标签$ git tag -a v1.2 9fceb02 #对某次提交后期打标签$ git push origin v1.5 #上传某个标签，GIT 默认不会 push 标签到远程仓库$ git push origin --tags #上传所有不在远程仓库的标签$ git checkout -b version2 v2.0.0 #检出标签 rm 移除文件12$ git rm filename #个人感觉效果同 rm$ git rm --cached filename #移除暂存区中的文件 分支命令 branch 创建分支123456789$ git branch #查看分支，前面带星号*的，是当前分支$ git branch testing #创建 testing 分支$ git branch -d testing #删除 testing 分支$ git branch -v #查看每个分支最后一次提交$ git branch --merged #查看已合并到当前分支的分支$ git branch --no-merged #查看未合并到当前分支的分支$ git branch -r #查看远程分支$ git push origin &lt;local_branch&gt;:&lt;remote_branch&gt; #推送本地分支到远程分支，不存在时新建远程分支$ git push origin :&lt;remote_branch&gt; #与上一条不同的时本地分支留空了，这将会删除远程分支 checkout 切换分支12$ git checkout testing$ git checkout -b iss53 #创建分支，并切换到新创建的分支 底层命令 cat-file 读取 GIT 仓库对象1234$ git cat-file -p f8a67de1d4bf0d6dbaaaf8990ffe8394e5fa88ee #查看对象内容$ git cat-file -p master^&#123;tree&#125; #master 分支上最新的提交所指向的 tree 对象$ git cat-file -t f8a67de1d4bf0d6dbaaaf8990ffe8394e5fa88ee #查看对象类型$ git cat-file -s f8a67de1d4bf0d6dbaaaf8990ffe8394e5fa88ee #查看对象大小 gc 生成包文件1234$ git gc#作用：完整保存最新版文件，历史版本文件保存差异#GIT 会根据情况自己执行，一般不需要手动之行$ git verify-pack -v .git/objects/pack/pack-57cbdc16b7fd157d77451245ef44ba68ea567e14.idx 查看对象 $ find .git/object/ -type f #所有对象列表 $ git rev-list --objects --all #blob列表","tags":[{"name":"Git","slug":"Git","permalink":"https://cnkeep.github.io/tags/Git/"}]},{"title":"git和svn有什么区别","date":"2018-03-01T16:40:00.000Z","path":"2018/03/02/02-git和svn有什么区别/","text":"Git和Svn的区别以前有Svn这种工具来进行版本控制，为什么还要用Git呢，两者有什么区别? 其实Git和Svn都是版本控制工具，但是Git更倾向于分布式，而且效率高，功能更强大。 Git和Svn的主要差别： 在Git 中的绝大多数操作都只需要访问本地文件和资源，不必联网就可以看到所有的历史版本记录，而SVN 却需要联网。 &nbsp;&nbsp;因为 Git 在本地磁盘上就保存着所有当前项目的历史更新，所以处理起来速度飞快，但我们需要浏览项目的历史更新摘要，Git 不用跑到外面的服务器上去取数据回来，而直接从本地数据库读取后展示给你看。如果想要看当前版本的文件和一个月前的版本之间有何差异，Git 会取出一个月前的快照和当前文件作一次差异运算。 SVN 断开网络或者断开VPN就无法commit代码，但是Git 可以先commit到本地仓库, svn断网就傻了 Git 克隆一个完整项目的速度非常快，SVN 非常慢。 Git 只关心文件数据的整体是否发生变化，而SVN这类版本控制系统则只关心文件内容的具体差异。 &nbsp;&nbsp;这类系统（如SVN）每次记录有哪些文件作了更新，以及都更新了哪些行的什么内容，然而Git 并不保存这些前后变化的差异数据。实际上，Git更像是把变化的文件作快照后，记录在一个微型的文件系统中。每次提交更新时，它会纵览一遍所有文件的指纹信息并对文件作一快照，然后保存一个指向这次快照的索引。为提高性能，若文件没有变化，Git 不会再次保存，而只对上次保存的快照作一链接。 Git分支管理比svn强大太多 在 SVN 这类的版本控制系统上，分支（branch）是一个完整的目录，且这个目录拥有完整的实际文件。如果工作成员想要开启新的分支，那将会影响“全世界”！每个人都会拥有和你一样的分支。如果你的分支是用来对系统模块进行安全检查测试的，那将会像传染病一样，你改一个分支，还得让其他人重新切分支重新下载，而且这些代码很可能对稳定版本还是具有破坏性的。 在 Git上，每个工作成员可以任意在自己的本地版本库开启无限个分支。举例：当我想尝试破坏自己的程序（安检测试），并且想保留这些被修改的文件供日后使用，我可以开一个分支，做我喜欢的事。完全不需担心妨碍其他工作成员。只要我不合并及提交到主要版本库，没有一个工作成员会被影响。等到我不需要这个分支时， 我只要把它从我的本地版本库删除即可，无痛无痒。","tags":[{"name":"Git","slug":"Git","permalink":"https://cnkeep.github.io/tags/Git/"}]},{"title":"git是什么","date":"2018-03-01T13:23:00.000Z","path":"2018/03/01/01-git是什么/","text":"Git是什么？ 作为一个Coder, 在编写代码过程中不可避免的要提交和修改文件，如何没有版本控制工具，我们想象一下，不小心删除了代码，恢复不了，昨天改了哪些，总会遇到这样对我问题，哪有没有一种工具能保存我们的历史修改记录，方便我们自由的切换到任意一个版本呢？ &nbsp;&nbsp;Git就是这样一个分布式版本控制工具, 能帮助我们记录文件的历史修改记录，可以自由的切换到文件的历史版本，再也不怕文件丢了！当然git远不止做了这些，它还提供了各种对比，合并，统计的功能。","tags":[{"name":"Git","slug":"Git","permalink":"https://cnkeep.github.io/tags/Git/"}]}]