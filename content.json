[{"title":"JVM参数配置打印gc日志","date":"2019-02-04T18:32:00.000Z","path":"2019/02/05/02-JVM参数配置打印gc日志/","text":"JVM参数配置设置打印gc日志 我们在排查内存问题时，经常需要知道系统的gc情况，如果需要输入gc信息到日志中，需要我们配置相关的jvm参数： 12-XX:+PrintGCDetails-Xloggc:gc1.log","tags":[{"name":"JVM","slug":"JVM","permalink":"https://cnkeep.github.io/tags/JVM/"}]},{"title":"jstack命令","date":"2019-02-04T16:09:00.000Z","path":"2019/02/05/02-jstack命令/","text":"JDK命令-jstack1. 介绍jstack用于生成当前java虚拟机当前时刻的线程快照。线程快照是当前虚拟机中每一个线程的执行情况，通过观察这些信息可以为我们分析程序异常提供帮助，例如：线程死锁等。 线程出现停顿的时候通过jstack来查看各个线程的调用堆栈，就可以知道没有响应的线程到底在后台做什么事情，或者等待什么资源。 2. 参数介绍1234567Usage: jstack [-options] &lt;pid&gt;Options: -F 当’jstack [-l] pid’没有响应的时候强制打印栈信息 -m 打印java和native c/c++框架的所有栈信息. -h | -help打印帮助信息 -l 长列表. 打印关于锁的附加信息,例如属于java.util.concurrent的ownable synchronizers列表 4. 案例使用4.1 案例1-cpu飙升问题排查 情景模拟12345678910111213141516171819202122232425import java.util.concurrent.Executor;import java.util.concurrent.Executors;public class CpuTask &#123; public static final Executor executor = Executors.newFixedThreadPool(2); public static final Object lock = new Object(); public static void main(String[] args) &#123; executor.execute(new Task()); executor.execute(new Task()); &#125; public static class Task implements Runnable &#123; @Override public void run() &#123; synchronized (lock) &#123; int i = 0; while (true) &#123; i++; &#125; &#125; &#125; &#125;&#125; 上面的程序会开启2个子线程，其中一个子线程中存在一个死循环(占用CPU), 另外一个线程会阻塞。12#javac CpuTask.java#java CpuTask &amp; 查看CPU占用 可以看到有一个pid为11775的Java进程高占CPU, 我们来看看进程内的线程情况： 我们发现是pid：11787的线程一直在占用CPU, 接着我们通过jstack看看线程的相关情况。 发现其中存在2个用户线程，其中一个阻塞，一个在运行，pid=0X2e0b(通过printf %d 0x2e0b转换为十进制), 与我们上一步发现高占CPU的线程是同一个，至此定位到了该线程，接下来就是反查代码了。","tags":[{"name":"JVM","slug":"JVM","permalink":"https://cnkeep.github.io/tags/JVM/"}]},{"title":"Jconsole无法连接程序","date":"2019-02-02T16:54:00.000Z","path":"2019/02/03/01-Jconsole无法连接程序/","text":"JConsole无法连接程序介绍我们在检测程序内存和线程使用情况时会使用到提供的jconsole工具，但是会出现连接不上的问题，需要我们设置启动参数：1234-Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.port=8011-Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.authenticate=false -XX:MaxPermSize=192M-Xms1000M-Xmx2000M-XX:+HeapDumpOnOutOfMemoryError","tags":[{"name":"JVM","slug":"JVM","permalink":"https://cnkeep.github.io/tags/JVM/"}]},{"title":"jps命令","date":"2019-02-02T14:58:00.000Z","path":"2019/02/02/01-jps命令/","text":"JDK命令-jps命令1. 介绍jps(Java Virtual Machine Process Status Tool)是JDK 1.5提供的一个显示当前所有java进程pid的命令，简单实用，非常适合在linux/unix平台上简单察看当前java进程的一些简单情况。 2. 参数介绍1234567# jps [options] [&lt;pid&gt;]Options: -q: 只显示pid，不显示class名称,jar文件名和传递给main 方法的参数 -m: 输出传递给main 方法的参数，在嵌入式jvm上可能是null -l: 输出应用程序main class的完整package名 或者 应用程序的jar文件完整路径名 -v: 输出传递给JVM的参数 3. 使用案例 查看当前运行的程序123[root@localhost conf]# jps11539 Jps11132 WebApplication","tags":[{"name":"JVM","slug":"JVM","permalink":"https://cnkeep.github.io/tags/JVM/"}]},{"title":"看懂UML类图","date":"2019-01-03T15:51:00.000Z","path":"2019/01/03/看懂UML类图/","text":"看懂UML类图前言&nbsp;&nbsp;作为一个程序员，除了基础的编码工作外，经常还需要输出各种各样的文档，例如：接口说明，系统设计等。其中作为系统设计的一部分，UML类图能直观的表达出各个类之间的关系，尤为重要，那么UML类图中中的图形，线条都是什么意思呢，今天我们就来学习一下，会看然后再用。 从实例开始举个例子，一家公司，有好几个开发部门，每个开发部门有多个程序员，每个程序员都有自己的身份证，通过PC工作。其UMl类图如下： 在这其中包含了如下关系： 实现关系，例如Person继承了Entity接口，即接口实现，采用空心箭头表示 泛化关系，例如Programmer是Person中的一类，即子类继承，采用实心箭头表示 关联关系，例如Programmer与IdCard的关联关系，即作为成员变量，采用实线箭头表示 依赖关系，例如Programmer工作依赖于Computer,但是可以随便换电脑， 即作为参数依赖，采用虚线箭头表示 聚合关系，例如Programmer聚合在一起成了部门，但是部门散了，程序员还在，部分不完全依赖于全局，可单独存在，用空心菱形表示 组合关系，例如部门组合成了公司，当公司倒闭，部门也就不在了，部分依赖于全局的存在, 不能单独存在，用实心菱形表示","tags":[{"name":"UML","slug":"UML","permalink":"https://cnkeep.github.io/tags/UML/"}]},{"title":"Redis设置开机自启","date":"2018-11-14T00:29:00.000Z","path":"2018/11/14/03-Redis设置开机自启/","text":"Redis设置开机自启 我们将Redis作为DB服务时，有时候会碰上关机的情况，及时这样我们也希望可以开机自启,本次我们就来学习如何是实现。 从docker引发的思考 1.设置docker开机自启12$ systemctl enable dockerCreated symlink from /etc/systemd/system/multi-user.target.wants/docker.service to /usr/lib/systemd/system/docker.service. 发现这里有两个目录：/etc/systemd/system/multi-user.target.wants/, /usr/lib/systemd/system/ 2./etc/systemd/system/multi-user.target.wants/ 该目录下存在许多软连接，指向/usr/lib/systemd/system/，开机时会扫描执行该目录下的脚本 3./usr/lib/systemd/system/ 真正的service单元配置文件都在这个目录下面，其中存在docker.service的文件：123456789101112131415161718192021222324252627282930313233[Unit]Description=Docker Application Container EngineDocumentation=https://docs.docker.comAfter=network-online.target firewalld.serviceWants=network-online.target[Service]Type=notify# the default is not to use systemd for cgroups because the delegate issues still# exists and systemd currently does not support the cgroup feature set required# for containers run by dockerExecStart=/usr/bin/dockerdExecReload=/bin/kill -s HUP $MAINPID# Having non-zero Limit*s causes performance problems due to accounting overhead# in the kernel. We recommend using cgroups to do container-local accounting.LimitNOFILE=infinityLimitNPROC=infinityLimitCORE=infinity# Uncomment TasksMax if your systemd version supports it.# Only systemd 226 and above support this version.#TasksMax=infinityTimeoutStartSec=0# set delegate yes so that systemd does not reset the cgroups of docker containersDelegate=yes# kill only the docker process, not all processes in the cgroupKillMode=process# restart the docker process if it exits prematurelyRestart=on-failureStartLimitBurst=3StartLimitInterval=60s[Install]WantedBy=multi-user.target 4.猜想 既然已经摸清了docker开机自启的猫腻，那么完全可以模仿实现redis的开机自启，按照以下步骤即可： 编写/user/lib/systemd/system/redis.service脚本 建立软连接至/etc/systemd/system/multi-user.target.wants/ 刷新配置，开启开启自启即可 设置开机自启 1.编写脚本1234567891011121314$ vi /lib/systemd/system/redis.service#写入以下内容[Unit]Description=Redis_5.0.1After=network.target[Service]#redis安装绝对路径ExecStart=/usr/local/app/redis-5.0.3/src/redis-server /usr/local/app/redis-5.0.3/redis.conf --daemonize noExecStop=/usr/local/app/redis-5.0.3/src/redis-cli -h 127.0.0.1 -p 6379 shutdown[Install]WantedBy=multi-user.target [Unit] 表示这是基础信息 Description 是描述 After 是在那个服务后面启动，一般是网络服务启动后启动 [Service] 表示这里是服务信息 ExecStart 是启动服务的命令 ExecStop 是停止服务的指令 [Install] 表示这是是安装相关信息 WantedBy 是以哪种方式启动：multi-user.target表明当系统以多用户方式（默认的运行级别）启动时，这个服务需要被自动运行。 详细请移步至：CoreOS实践指南（八）：Unit文件详解 2.设置开机启动12345678910#配置软连接$ ln -s /lib/systemd/system/redis.service /etc/systemd/system/multi-user.target.wants/redis.service#刷新配置$ systemctl daemon-reload#开启开机自启功能$ systemctl enable redis$ systemctl [start|stop|restart|status] redis `","tags":[{"name":"Redis","slug":"Redis","permalink":"https://cnkeep.github.io/tags/Redis/"}]},{"title":"Redis设置访问ip限制","date":"2018-11-12T19:55:00.000Z","path":"2018/11/13/02-Redis设置访问ip限制/","text":"Redis设置访问ip限制介绍鉴于Redis没有用户的概念，我们在实际生产中不能直接将其暴露在外网的环境下，普遍都是通过反向代理限制在内网访问，那么如何设置限制ip访问呢？ 1.仅限部署Redis服务器的客户端连接：12345#永久生效配置redis.conf, 设置protected-mode=true#当次生效，重启失效config set protected-mode=true 2.指定客户端信任ip列表：1配置redis.conf, 设置bind &lt;ipList 多个采用空格分隔&gt; 3.开启密码保护：1配置redis.conf, 设置requirepass &lt;pwd&gt; 4.效果：123456789101112131415161718192021222324#重启客户端访问后无法操作： 172.17.0.2:6379&gt; config get mode*(error) DENIED Redis is running in protected mode because protected mode is enabled, no bind address was specified, no authentication password is requested to clients. In this mode connections are only accepted from the loopback interface. If you want to connect from external computers to Redis you may adopt one of the following solutions: 1) Just disable protected mode sending the command &apos;CONFIG SET protected-mode no&apos; from the loopback interface by connecting to Redis from the same host the server is running, however MAKE SURE Redis is not publicly accessible from internet if you do so. Use CONFIG REWRITE to make this change permanent. 2) Alternatively you can just disable the protected mode by editing the Redis configuration file, and setting the protected mode option to &apos;no&apos;, and then restarting the server. 3) If you started the server manually just for testing, restart it with the &apos;--protected-mode no&apos; option. 4) Setup a bind address or an authentication password.NOTE: You only need to do one of the above things in order for the server to start accepting connections from the outside.#需要密码登录[root@localhost conf]# redis-cli 127.0.0.1:6379&gt; keys *(error) NOAUTH Authentication required.[root@localhost conf]# redis-cli127.0.0.1:6379&gt; auth java1024 #auth命令验证密码OK","tags":[{"name":"Redis","slug":"Redis","permalink":"https://cnkeep.github.io/tags/Redis/"}]},{"title":"Redis介绍与安装","date":"2018-11-12T16:22:00.000Z","path":"2018/11/13/01-Redis介绍与安装/","text":"Redis介绍与安装【目录】Redis介绍Redis安装&nbsp;&nbsp;压缩包安装&nbsp;&nbsp;Docker安装Redis Redis介绍&nbsp;&nbsp;Redis是当前最火的Nosql系统之一，它是一个key-value的存储系统，与其他Nosql不同的是它的优良性能以及多种数据结构的支持(只会在理论篇做对比介绍)。 Redis安装 相信大家已经对Redis做过相应了解了，那现在就来动手安装实践一下，本次安装分为:压缩包安装和Docker安装（推荐）。 压缩包安装 1.环境介绍12Centos 7Redis压缩包：http://download.redis.io/releases/redis-5.0.3.tar.gz 2.资源获取 参见Redis官网 3.配置分组和用户，默认目录(非必须)123456789# 这里我采用redis独立用户去操作，可以直接跳过$groupadd redis$useradd redis -g redis -d /home/redis -s /bin/bash$passwd redis更改用户 redis 的密码 。新的 密码：无效的密码： 密码少于 8 个字符重新输入新的 密码：passwd：所有的身份验证令牌已经成功更新。 4.安装编译12345678910111213141516171819$cd /home/redis$tar -zxvf redis-5.0.3.tar.gz $cd /home/redis/redis-5.0.3$make #编译$cd src$make install CC Makefile.depHint: It&apos;s a good idea to run &apos;make test&apos; ;) INSTALL install INSTALL install INSTALL install INSTALL install INSTALL install#建立软连接方便执行启动命令$ln -s /home/redis/redis-5.0.3/src/redis-cli /usr/sbin/redis-cli$ln -s /home/redis/redis-5.0.3/src/redis-server /usr/sbin/redis-server 5.修改配置12345678910111213141516$mkdir /home/redis/redis-5.0.3/data #用于存储数据$mkdir /home/redis/redis-5.0.3/conf #用于存储配置文件$mkdir /home/redis/redis-5.0.3/log #用于存储日志#copy一份配置文件$cp /home/redis/redis-5.0.3/redis.conf /home/redis/redis-5.0.3/conf/redis_template.conf #修改端口，可以跳过$sed s/6379/6389/g /home/redis/redis-5.0.3/conf/redis_template.conf&gt;/home/redis/redis-5.0.3/conf/redis_6389.conf$vi /home/redis/redis-5.0.3/conf/redis_6389.conf#配置日志项：logfile &quot;/home/redis/redis-5.0.3/logs/log&quot;#配置数据项：dir /home/redis/redis-5.0.3/data/#关闭保护：protected-mode no#配置信任ip：bind 172.16.*.*#配置密码：require redis@Pwd11 6.启动1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950#设置加载的配置文件，设置后台启动$redis-server /home/redis/redis-5.0.3/conf/redis_6389.conf &amp;#查看进程信息$ps -ef|grep redisroot 6550 2150 0 17:15 pts/0 00:00:00 redis-server 127.0.0.1:6389root 6555 2150 0 17:15 pts/0 00:00:00 grep --color=auto redis#查看启动日志$tail -n 100 -f /home/redis/redis-5.0.3/logs/log6483:C 28 Dec 2018 16:53:55.715 # oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo6483:C 28 Dec 2018 16:53:55.715 # Redis version=5.0.3, bits=64, commit=00000000, modified=0, pid=6483, just started6483:C 28 Dec 2018 16:53:55.715 # Configuration loaded6483:M 28 Dec 2018 16:53:55.716 * Increased maximum number of open files to 10032 (it was originally set to 1024). _._ _.-``__ &apos;&apos;-._ _.-`` `. `_. &apos;&apos;-._ Redis 5.0.3 (00000000/0) 64 bit .-`` .-```. ```\\/ _.,_ &apos;&apos;-._ ( &apos; , .-` | `, ) Running in standalone mode |`-._`-...-` __...-.``-._|&apos;` _.-&apos;| Port: 6389 | `-._ `._ / _.-&apos; | PID: 6550 `-._ `-._ `-./ _.-&apos; _.-&apos; |`-._`-._ `-.__.-&apos; _.-&apos;_.-&apos;| | `-._`-._ _.-&apos;_.-&apos; | http://redis.io `-._ `-._`-.__.-&apos;_.-&apos; _.-&apos; |`-._`-._ `-.__.-&apos; _.-&apos;_.-&apos;| | `-._`-._ _.-&apos;_.-&apos; | `-._ `-._`-.__.-&apos;_.-&apos; _.-&apos; `-._ `-.__.-&apos; _.-&apos; `-._ _.-&apos; `-.__.-&apos; 6483:M 28 Dec 2018 16:53:55.716 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.6483:M 28 Dec 2018 16:53:55.716 # Server initialized6483:M 28 Dec 2018 16:53:55.716 # WARNING overcommit_memory is set to 0! Background save may fail under low memory condition. To fix this issue add &apos;vm.overcommit_memory = 1&apos; to /etc/sysctl.conf and then reboot or run the command &apos;sysctl vm.overcommit_memory=1&apos; for this to take effect.6483:M 28 Dec 2018 16:53:55.717 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command &apos;echo never &gt; /sys/kernel/mm/transparent_hugepage/enabled&apos; as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.6483:M 28 Dec 2018 16:53:55.717 * DB loaded from disk: 0.000 seconds6483:M 28 Dec 2018 16:53:55.717 * Ready to accept connections6483:M 28 Dec 2018 16:56:31.287 # User requested shutdown...6483:M 28 Dec 2018 16:56:31.287 * Saving the final RDB snapshot before exiting.6483:M 28 Dec 2018 16:56:31.288 * DB saved on disk6483:M 28 Dec 2018 16:56:31.288 * Removing the pid file.6483:M 28 Dec 2018 16:56:31.288 # Redis is now ready to exit, bye bye...``` &gt; 7.客户端连接测试 ```text$redis-cli -h &lt;ip&gt; -p &lt;port&gt;127.0.0.1:6389&gt;keys *(empty list or set) 8.关闭redis1$redis-cli shutdown 以上redis就安装完毕了，快去使用吧！ Docker安装Redis 1.环境介绍1234567Centos 7Docker``` &gt; 2.获取资源 ```text$docker search redis$docker pull redis:latest 3.配置基础环境1234567891011121314151617181920212223242526272829303132$mkdir /home/redis/redis-5.0.3/data #用于存储数据$mkdir /home/redis/redis-5.0.3/conf #用于存储配置文件$mkdir /home/redis/redis-5.0.3/log #用于存储日志#copy一份配置文件, 配置文件可以从官方获取$cp /home/redis/redis-5.0.3/redis.conf /home/redis/redis-5.0.3/conf/redis_template.conf #修改端口，可以跳过$sed s/6379/6389/g /home/redis/redis-5.0.3/conf/redis_template.conf&gt;/home/redis/redis-5.0.3/conf/redis_6389.conf$vi /home/redis/redis-5.0.3/conf/redis_6389.conf#配置日志项：logfile &quot;/home/redis/redis-5.0.3/logs/log&quot;#配置数据项：dir /home/redis/redis-5.0.3/data/#关闭保护：protected-mode no#配置信任ip：bind 172.16.*.*#配置密码：require redis@Pwd11``` &gt; 4.启动 ```text$docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEredis latest c188f257942c 6 weeks ago 94.9MB#配置启动参数$docker run \\ #创建并启动容器--name redis_6379 \\#指定容器名称-p 6379:6379 \\#指定端口映射，宿主机：容器-privileged=true \\#给与权限-v /home/redis/redis-5.0.3/conf/redis_6389.conf:/etc/redis/redis.conf \\#配置文件映射-v /home/redis/redis-5.0.3/data:/etc/data \\#数据目录映射-d redis redis-server /etc/redis/redis.conf --appendonly yes#-d后台启动，并指定配置文件，AOF开启 5.验证启动1234567891011121314151617181920212223242526272829303132333435363738#查看日志$docker logs redis_63791:C 28 Dec 2018 17:40:44.775 # oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo1:C 28 Dec 2018 17:40:44.776 # Redis version=5.0.1, bits=64, commit=00000000, modified=0, pid=1, just started1:C 28 Dec 2018 17:40:44.776 # Configuration loaded _._ _.-``__ &apos;&apos;-._ _.-`` `. `_. &apos;&apos;-._ Redis 5.0.1 (00000000/0) 64 bit .-`` .-```. ```\\/ _.,_ &apos;&apos;-._ ( &apos; , .-` | `, ) Running in standalone mode |`-._`-...-` __...-.``-._|&apos;` _.-&apos;| Port: 6379 | `-._ `._ / _.-&apos; | PID: 1 `-._ `-._ `-./ _.-&apos; _.-&apos; |`-._`-._ `-.__.-&apos; _.-&apos;_.-&apos;| | `-._`-._ _.-&apos;_.-&apos; | http://redis.io `-._ `-._`-.__.-&apos;_.-&apos; _.-&apos; |`-._`-._ `-.__.-&apos; _.-&apos;_.-&apos;| | `-._`-._ _.-&apos;_.-&apos; | `-._ `-._`-.__.-&apos;_.-&apos; _.-&apos; `-._ `-.__.-&apos; _.-&apos; `-._ _.-&apos; `-.__.-&apos; 1:M 28 Dec 2018 17:40:44.780 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.1:M 28 Dec 2018 17:40:44.780 # Server initialized1:M 28 Dec 2018 17:40:44.780 # WARNING overcommit_memory is set to 0! Background save may fail under low memory condition. To fix this issue add &apos;vm.overcommit_memory = 1&apos; to /etc/sysctl.conf and then reboot or run the command &apos;sysctl vm.overcommit_memory=1&apos; for this to take effect.1:M 28 Dec 2018 17:40:44.780 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command &apos;echo never &gt; /sys/kernel/mm/transparent_hugepage/enabled&apos; as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.1:M 28 Dec 2018 17:40:44.792 * DB loaded from disk: 0.012 seconds1:M 28 Dec 2018 17:40:44.792 * Ready to accept connections#进入容器查看 [root@localhost conf]# docker exec -it redis_6379 /bin/bashroot@c6c3bf9cb27c:/data# redis-cli127.0.0.1:6379&gt; keys *(empty list or set) 127.0.0.1:6379&gt; info memory......127.0.0.1:6379&gt; info replication 结束语自此redis的安装就完成了，其中关于Docker的安装，以及防火墙的配置不做介绍，读者可以自行解决。Over!","tags":[{"name":"Redis","slug":"Redis","permalink":"https://cnkeep.github.io/tags/Redis/"}]},{"title":"explain查看执行计划","date":"2018-10-10T11:59:00.000Z","path":"2018/10/10/04-explain查看执行计划/","text":"Explain关键字查看执行计划1.是什么explain命令是mysql提供的一个查看sql执行计划的命令。 2.使用场景&nbsp;&nbsp;在工作中我们免不了会遇到各种SQL调优，SQL调优的第一步就是定位执行效率差的SQL，并判断为何执行效率差，这时候explain命令就派上用场了，可以帮我们查看sql的执行计划，例如是否使用了索引，是全表扫描还是索引扫描，这些都可以通过执行计划判断。 3.如何使用&nbsp;&nbsp;使用方式在sql语句前加上explain关键词即可。 4.参数详解&nbsp;&nbsp;知道用法当然没什么作用，我们得知道执行计划中各个参数表达的含义，才能对症下药，有针对性的优化SQL，接下来我们来看重要参数的含义：1234567891011121314151617181920212223242526272829303132mysql&gt; select version();+-----------+| version() |+-----------+| 5.6.26 |+-----------+drop database if exists 'test';create database test;use test;-- 用户表drop table if exisits `user`;create table `user`( id int primary key auto_increment, `name` varchar(20) not null comment '用户名')engine=innodb charset=utf8 comment '用户表';mysql&gt; explain select * from user where id=1\\G;*************************** 1. row *************************** id: 1 select_type: SIMPLE table: user partitions: NULL type: constpossible_keys: PRIMARY key: PRIMARY key_len: 4 ref: const rows: 1 filtered: 100.00 Extra: NULL explain命令输出的记过包含：id, select_type, table, type, possible_keys, key, key_len, ref, rows, filtered, extra 4.1 id 为一组数字，表示查询中执行select字句或者操作表的顺序。 123456789101112131415161718192021222324252627mysql&gt; explain select * from user a where a.id in (select id from user b where b.name like 'user%')\\G;*************************** 1. row *************************** id: 1 select_type: SIMPLE table: b partitions: NULL type: ALLpossible_keys: PRIMARY key: NULL key_len: NULL ref: NULL rows: 5 filtered: 20.00 Extra: Using where*************************** 2. row *************************** id: 1 select_type: SIMPLE table: a partitions: NULL type: eq_refpossible_keys: PRIMARY key: PRIMARY key_len: 4 ref: test_1.b.id rows: 1 filtered: 100.00 Extra: NULL 4.2select_type 表示select查询的类型 select_type属性有一下几种类型： SIMPLE：简单查询，该查询不包含 UNION 或子查询 PRIMARY：如果查询包含UNION 或子查询，则最外层的查询被标识为PRIMARY UNION：表示此查询是 UNION 中的第二个或者随后的查询 DEPENDENT：UNION 满足 UNION 中的第二个或者随后的查询，其次取决于外面的查询 UNION RESULT：UNION 的结果 SUBQUERY：子查询中的第一个select语句(该子查询不在from子句中) DEPENDENT SUBQUERY：子查询中的 第一个 select，同时取决于外面的查询 DERIVED：包含在from子句中子查询(也称为派生表) UNCACHEABLE SUBQUERY：满足是子查询中的第一个 select 语句，同时意味着 select 中的某些特性阻止结果被缓存于一个 Item_cache 中 UNCACHEABLE UNION：满足此查询是 UNION 中的第二个或者随后的查询，同时意味着 select 中的某些特性阻止结果被缓存于一个 Item_cache 中 4.3table 该列表示对应行正在访问的数据库表，存在别名时显示别名。 4.4type 这个参数表示关联类型或者访问类型，即MySQL决定采用何种策略查找表中的行，这也是我们调优时重点关注的列。 执行效率： const &gt; eq_ref &gt; ref &gt; range &gt; index &gt; ALL以下为常见取值： ALL: 全表扫描，这个类型是性能最差的查询之一，通常来讲我们的查询不应该出现ALL类型，因为这样的查询在数据量大的情况下对数据库的性能损耗是巨大的。 index: 全索引扫描，和ALl类型类似，只不过ALL类型是全表扫描。而 index 类型是扫描全部的索引，主要优点是避免了排序，但是开销仍然非常大。如果在 Extra 列看到 Using index，说明正在使用覆盖索引，只扫描索引的数据，它比按索引次序全表扫描的开销要少很多。 range: 范围查找，就是一个有限制的索引扫描，它开始于索引里的某一点，返回匹配这个值域的行。这个类型通常出现在 =、&lt;&gt;、&gt;、&gt;=、&lt;、&lt;=、ISNULL、&lt;=&gt;、BETWEEN、IN() 的操作中，key 列显示使用了哪个索引，当 type 为该值时，则输出的 ref 列为 NULL，并且 key_len列是此次查询中使用到的索引最长的那个。 ref：一种索引访问，也称索引查找，它返回所有匹配某个单个值的行。此类型通常出现在多表的 join 查询, 针对于非唯一或非主键索引, 或者是使用了最左前缀规则索引的查询。 eq_ref：使用这种索引查找，最多只返回一条符合条件的记录。在使用唯一性索引或主键查找时会出现该值，非常高效。 const、system：该表至多有一个匹配行，在查询开始时读取，或者该表是系统表，只有一行匹配。其中 const 用于在和 primary key 或 unique索引中有固定值比较的情形。 NULL: 在执行阶段不需要访问表。 4.5possible_keys 表示查询可能使用哪些索引来查找 4.6key 表示MySQL实际决定使用的索引。如果没有选择索引，键是NULL。 4.7key_len 表示在所引力使用的字节数，当key列为NULL时，此值为NULL 4.8ref 表示哪些字段或者常量被用来和key配合从表中查询记录 4.9rows 表示估计要找到所需的行而要读取的行数，这个值是个估计值，原则上值越小越好。 4.10extra 附加信息 常见的取值如下： Using index：使用覆盖索引，表示查询索引就可查到所需数据，不用扫描表数据文件，往往说明性能不错。 Using Where：在存储引擎检索行后再进行过滤，使用了where从句来限制哪些行将与下一张表匹配或者是返回给用户。 Using temporary：在查询结果排序时会使用一个临时表，一般出现于排序、分组和多表 join 的情况，查询效率不高，建议优化。 Using filesort：对结果使用一个外部索引排序，而不是按索引次序从表里读取行，一般有出现该值，都建议优化去掉，因为这样的查询 CPU 资源消耗大。","tags":[{"name":"mysql","slug":"mysql","permalink":"https://cnkeep.github.io/tags/mysql/"}]},{"title":"02_Mysql执行顺序","date":"2018-10-08T11:22:00.000Z","path":"2018/10/08/03-02_Mysql执行顺序/","text":"Mysql的执行顺序 Mysql的查询解析器，会对我们的sql进行解析生成一颗语法树, 进而判断语法错误，优化调整执行顺序，我们来看看Mysql是按照什么样的顺序来执行各个关键字的： (8) SELECT (9) DISTINCT &lt;select_list&gt; (1) FROM &lt;left_table&gt; (3) &lt;join_type&gt; JOIN &lt;right_table&gt; (2) ON &lt;join_condition&gt; (4) WHERE &lt;where_condition&gt; (5) GROUP BY &lt;group_by_list&gt; (6) WITH {CUBE|ROLLUP} (7) HAVING &lt;having_condition&gt; (10) ORDER BY &lt;order_by_list&gt; (11) LIMIT &lt;limit_number&gt; 可以看到执行的顺序如上面的编号，from，join, on, where, group by, having, select,distinct, order by, limit.每一个操作都会产生一张虚拟表，该虚拟表作为一个处理的输入。这些虚拟表对用户是透明的，只有最后一步生成的虚拟表才会返回给用户。","tags":[{"name":"mysql","slug":"mysql","permalink":"https://cnkeep.github.io/tags/mysql/"}]},{"title":"01_Mysql联接查询算法","date":"2018-10-07T06:47:00.000Z","path":"2018/10/07/03-01_Mysql联接查询算法/","text":"Mysql联接查询算法 转载：运维那点事-MySQL联接查询算法（NLJ、BNL、BKA、HashJoin） 前言&nbsp;&nbsp;这几天偶然发现了关于MySQL联接查询算法的文章，受益匪浅，在此转载记录，原文已置顶说明。 级联查询算法联接算法是MySQL数据库用于处理联接的物理策略。目前MySQL数据库仅支持Nested-Loops Join算法。而MySQL的分支版本MariaDB除了支持Nested-Loops Join算法外，还支持Classic Hash Join算法。当联接的表上有索引时，Nested-Loops Join是非常高效的算法。根据B+树的特性，其联接的时间复杂度为O(N)，若没有索引，则可视为最坏的情况，时间复杂度为O(N²)。 MySQL数据库根据不同的使用场合，支持两种Nested-Loops Join算法，一种是Simple Nested-Loops Join（NLJ）算法，另一种是Block Nested-Loops Join（BNL）算法。 上图的Fetch阶段是指当内表关联的列是辅助索引时，但是需要访问表中的数据，这是就需要回表(辅助索引叶子节点存储的是主键索引，需要回表查询)，无论表的存储引擎是Innodb还是MyISAM，这都是无法避免的，只是MyISAM的回表速度要快点，因为其辅助索引存放的就是指向记录的指针，而InnoDB存储引擎是索引组织表，需要再次通过索引查找才能定位数据。Fetch阶段也不是必须存在的，如果是聚集索引，那么子节点就存储着数据，无需回表。另外上述给出了两张表之间的join成本，多张表的join就是继续上述这个过程。 接着计算两张表的Join成本，这里有下列几种概念： 外表的扫描次数，记为O。通常外表（驱动表，数据量小）的扫描次数都是1，即Join时扫描一次驱动表的数据即可内表的扫描次数，记为I。根据不同的Join算法，内表的扫描次数不同读取表的记录数，记为R。根据不同Join算法，读取记录的数量可能不同Join的比较次数，记为M。根据不同Join算法，比较次数不同回表的读取记录的数，记为F。若Join的是辅助索引，可能需要回表取得最终的数据 评判一个Join算法是否优劣，就是查看上述这些操作的开销是否比较小。当让，这还要考虑I/O的访问方式，顺序 还是随机。总之Join的调优是门艺术。 Simple Nested-Loops Join (SNLJ)算法Simple Nested-Loops Join算法相当简单、直接。即外表（驱动表）中的每一条记录与内表中的记录进行比较判断。算法如下：12345678910111213141516171819202122232425For each row r in R do -- 扫描R表 For each row s in S do -- 扫描S表 If r and s satisfy the join condition -- 如果r和s满足join条件 Then output the tuple &lt;r, s&gt; -- 返回结果集``` 下图能更好地显示整个SNLJ的过程： ![](images/join_002.jpg) 其中R表为外部表（Outer Table），S表为内部表（Inner Table）。这是一个最简单的算法，这个算法的开销其实非常大。假设在两张表R和S上进行联接的列都不含有索引，外表的记录数为RN，内表的记录数位SN。根据上一节对于Join算法的评判标准来看，SNLJ的开销如下表所示： ![](images/join_003.png) 可以看到读取记录数的成本和比较次数的成本都是SN*RN，也就是笛卡儿积。假设外表内表都是1万条记录，那么其读取的记录数量和Join的比较次数都需要上亿。实际上数据库并不会使用到SNLJ算法。 ### Index Nested-Loops Join（INLJ）算法 SNLJ算法虽然简单明了，但是也是相当的粗暴。因此，在Join的优化时候，通常都会建议在内表建立索引，以此降低Nested-Loop Join算法的开销，MySQL数据库中使用较多的就是这种算法，以下称为INLJ。来看这种算法的伪代码： ```textFor each row r in R do -- 扫描R表 lookup s in S index -- 查询S表的索引（固定3~4次IO，B+树高度） If find s == r -- 如果r匹配了索引s Then output the tuple &lt;r, s&gt; -- 返回结果集 由于内表上有索引，所以比较的时候不再需要一条条记录进行比较，而可以通过索引来减少比较，从而加速查询。整个过程如下图所示： 可以可看到外表中的每一条记录通过内表的索引进行访问，即读取外行表的一行数据，然后去内部表索引进行二分法匹配；而一般的B+数的高度为3~4层，也就是或匹配一次的IO也就是3~4次，因此索引查询的成本是比较固定的，故优化器都倾向于使用记录数少的表作为外表。故INLJ的算法成本如下表所示： 上表Smatch表示通过索引找到匹配的记录数量。同时可以发现，通过索引可以大幅降低内表的Join的比较次数，每次比较1条外表的记录，其实就是一次indexlookup（索引查找），而每次index lookup的成本就是树的高度，即IndexHeight。 INLJ的算法并不复杂，也算简单易懂。但是效率是否能达到用户的预期呢？其实如果是通过表的主键索引进行Join，即使是大数据量的情况下，INLJ的效率亦是相当不错的。因为索引查找的开销非常小，并且访问模式也是顺序的（假设大多数聚集索引的访问都是比较顺序的）。 大部分人诟病MySQL的INLJ慢，主要是因为在进行Join的时候可能用到的索引并不是主键的聚集索引，而是辅助索引，这时INLJ的过程又需要多一步Fetch的过程，而且这个过程开销会相当的大： 由于访问的是辅助索引，如果查询需要访问聚集索引上的列，那么必要需要进行回表取数据，看似每条记录只是多了一次回表操作，但这才是INLJ算法 最大的弊端。首先，辅助索引的index lookup是比较随机I/O访问操作。其次，根据index lookup再进行回表又是一个随机的I/O操作。所以说， INLJ最大的弊端是其可能需要大量的离散操作，这在SSD出现之前是最大的瓶颈。而即使SSD的出现大幅提升了随机的访问性能， 但是对比顺序I/O，其还是慢了很多，依然不在一个数量级上。 另外，在INNER JOIN中，两张联接表的顺序是可以变换的，即R INNER JOIN S ON Condition P等效于S INNER JOIN R ON Condition P。 根据前面描述的Simple Nested-Loops Join算法，优化器在一般情况下总是选择将联接列含有索引的表作为内部表。如果两张表R和S在联接列上都有索引， 并且索引的高度相同，那么优化器会选择记录数少的表作为外部表，这是因为内部表的扫描次数总是索引的高度，与记录的数量无关。所以，联接列只要 有一个字段有索引即可，但最好是数据集多的表有索引；但是，但有WHERE条件的时候又另当别论了。 Block Nested-Loops Join（BNL）算法 在有索引的情况下，MySQL会尝试去使用Index Nested-Loop Join算法，在有些情况下，可能Join的列就是没有索引，那么这时MySQL的选择绝对不会 是最先介绍的Simple Nested-Loop Join算法，因为那个算法太粗暴，不忍直视。数据量大些的复杂SQL估计几年都可能跑不出结果。 而Block Nested-Loop Join算法较Simple Nested-Loop Join的改进就在于可以减少内表的扫描次数，甚至可以和Hash Join算法一样， 仅需扫描内表一次。其使用Join Buffer（联接缓冲）来减少内部循环读取表的次数。 12345For each tuple r in R do -- 扫描外表R store used columns as p from R in Join Buffer -- 将部分或者全部R的记录保存到Join Buffer中，记为p For each tuple s in S do -- 扫描内标S If p and s satisfy the join condition -- p与s满足join条件 Then output the tuple -- 返回为结果集 可以看到相比Simple Nested-Loop Join算法，Block Nested-LoopJoin算法仅多了一个所谓的Join Buffer，为什么这样就能减少内表的扫描次数呢？下图相比更好地解释了Block Nested-Loop Join算法的运行过程： 可以看到Join Buffer用以缓存联接需要的列，然后以Join Buffer批量的形式和内表中的数据进行联接比较。就上图来看，记录r1，r2 … rT的联接仅需扫内表一次，如果join buffer可以缓存所有的外表列，那么联接仅需扫描内外表各一次，从而大幅提升Join的性能。 Mysql数据库使用Join Buffer的原则如下： 系统变量Join_buffer_size决定了Join Buffer的大小。 Join Buffer可被用于联接是ALL、index、和range的类型。 每次联接使用一个Join Buffer，因此多表的联接可以使用多个Join Buffer。 Join Buffer在联接发生之前进行分配，在SQL语句执行完后进行释放。 Join Buffer只存储要进行查询操作的相关列数据，而不是整行的记录。 Batched Key Access Join（BKA）算法Index Nested-Loop Join虽好，但是通过辅助索引进行联接后需要回表，这里需要大量的随机I/O操作。若能优化随机I/O，那么就能极大的提升Join的性能。为此，MySQL 5.6（MariaDB 5.3）开始支持Batched Key Access Join算法（简称BKA），该算法通过常见的空间换时间，随机I/O转顺序I/O，以此来极大的提升Join的性能。 在说明Batched Key Access Join前，首先介绍下MySQL 5.6的新特性mrr——multi range read。因为这个特性也是BKA的重要支柱。MRR优化的目的就是为了减少磁盘的随机访问，InnoDB由于索引组织表的特性，如果你的查询是使用辅助索引，并且有用到表中非索引列（投影非索引字段，及条件有非索引字段），因此需要回表读取数据做后续处理，过于随机的回表会伴随着大量的随机I/O。这个过程如下图所示： 而mrr的优化在于，并不是每次通过辅助索引读取到数据就回表去取记录，范围扫描（range access）中MySQL将扫描到的数据存入read_rnd_buffer_size，默认256K。然后对其按照Primary Key（RowID）排序，然后使用排序好的数据进行顺序回表，因为我们知道InnoDB中叶子节点数据是按照PRIMARY KEY（ROWID）进行排列的，那么这样就转换随机读取为顺序读取了。这对于IO-bound类型的SQL查询语句带来性能极大的提升。 MRR优化可用于range，ref，eq_ref类型的查询，工作方式如下图： 即增加一次排序，将回表的随机IO改为顺序IO,从而提升性能，其算法伪代码如下：123456For each tuple r in R do -- 扫描外部表R store used columns as p from R in join buffer -- 将部分或者全部R的记录保存到Join Buffer中，记为p For each tuple s in S do -- If p and s satisfy the join condition use mrr inter face to sort row Id Then output the tuple &lt;p,s&gt; 总结 Join时，扫描外表(驱动表)一次，然后和内表进行匹配 当关联键为主键索引时，与内表匹配走索引(非全表扫描)，减少IO,提升性能 当关联表为辅助索引时，引入缓存区，将关联查询的列存储在缓存区，批量与内表匹配，减少IO,提升性能（但是依旧无法避免因为回表而导致的随机IO问题） 当关联表为辅助索引时，引入缓存区，在上面的基础上在回表之前将主键索引排序后，随机IO转为顺序IO提升性能 一般选取记录数量小的的表做外表，关联键为索引的表做内表(最好是主键索引，比较次数就会稳定在3~4次)","tags":[{"name":"mysql","slug":"mysql","permalink":"https://cnkeep.github.io/tags/mysql/"}]},{"title":"03_Innodb引擎RR下如何避免幻读","date":"2018-10-07T03:13:00.000Z","path":"2018/10/07/02-03_Innodb引擎RR下如何避免幻读/","text":"Innodb引擎RR下如何避免幻读前言&nbsp;&nbsp;我们知道Innodb默认的事务隔离级别是Repeatable-Read, 这种隔离级别下解决了不可重复读的问题，同时部分解决了幻读的问题，这次我们就来谈论它是如何避免幻读的。关键词：锁，MVCC, Next-key Lock 基础概念排它锁(X锁)和共享锁(S锁) 所谓X锁,是事务T对数据A加上X锁时,只允许事务T读取和修改数据A,类似于写锁； 1select * from tableName where ... for update; 所谓S锁,是事务T对数据A加上S锁时,其他事务只能再对数据A加S锁,而不能加X锁,直到T释放A上的S锁，类似于读锁；1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253select * from tableName where ... + lock in share mode;``` ### 行锁和表锁 &gt; 行锁，小粒度锁，只锁部分数据行 &gt; 表锁，大粒度锁，锁定整张数据表 **对比**： | 对比 | 行锁 | 表锁 | | :--: | :--: | :--: | | 优势 | 粒度小，并发能力强，锁冲突概率低|操作简单，开销小 | | 劣势 | 开销大，加锁满，会出现死锁|并发能力弱 | ### 行锁(Record Lock)和间隙锁(Gap Lock)，Next-Key Lock * **行锁**：锁定当前行，行锁锁定的是``索引``，而不是行数据，也就是针对索引加的锁，不是针对记录加的锁。所以当做更新删除操作时，检索条件不是索引，则都会从行锁升级为表锁。**特例，及时使用了索引，但是索引值不存在时会升级为间隙锁。** * **间隙锁**：锁定索引记录间隙，确保索引记录的间隙不变。间隙锁是针对事务隔离级别为可重复读或以上级别而已的，（无论是S还是X）``只会阻塞insert操作。``另一个事务通过范围条件加锁时，如果使用相等条件请求给一个不存在的记录加锁都会使用间隙锁。* **Next-Key Lock**: 行锁和间隙锁和组合，首先对选中的索引记录加上行锁（Record Lock），再对索引记录两边的间隙（向左扫描扫到第一个比给定参数小的值， 向右扫描扫描到第一个比给定参数大的值， 然后以此为界，构建一个区间）加上间隙锁（Gap Lock）。如果一个间隙被事务T1加了锁，其它事务是不能在这个间隙内插入记录的。Innodb引擎采用这种方式来避免当前出现的幻读。**当查询的索引含有唯一属性时，将next-key lock降级为record key** ### 快照读和当前读 * 快照读：即普通的select操作,读取的是历史快照数据,利用MVCC模式实现。 * 当前读：select时指定了锁，for update，lock in share mode。读取的是最新的数据。 ### MVCC(多版本控制) &amp;nbsp;&amp;nbsp;Mysql的Innodb引擎在Repeatable-Read隔离级别下采用MVCC不加锁的方式来代替加锁操作。 #### 实现方式 &amp;nbsp;&amp;nbsp;Innodb的MVCC, 是通过在每行记录后面保存两个隐藏的列来实现的，这两个列分别保存了这行记录的创建时间和删除时间。但在实际的实现里存储的当前的系统版本号(可以理解为事务的ID)。一旦开始一个新的事务，系统的版本号就会递增，事务开始时刻的系统版本号会作为事务的ID,为了避免幻读查询时只会查询创建版本号小于当前事务版本号，删除版本号大于当前事务版本号(保证事务开始前该记录未删除)的记录。 ### 实例分析 ```sqlmysql&gt; select version();+-----------+| version() |+-----------+| 5.6.26 |+-----------+drop database if exists 'test';create database test;use test;-- 用户表drop table if exisits `user`;create table `user`( id int primary key auto_increment, `name` varchar(20) not null comment '用户名')engine=innodb charset=utf8 comment '用户表'; insert&nbsp;&nbsp;Innodb为插入的每一行记录保存当前系统的版本号作为创建版本号，查询只会查询出创建版本号小于当前事务id的记录。12345678910111213141516171819begin;insert into user(name) values('user_01');insert into user(name) values('user_02');insert into user(name) values('user_03');commit;``` 我们看看实际的存储情况： | id | name | 创建版本号 | 删除版本号 | |:--:|:--:|:--:|:--:| | 1 | user_01 | 1 | undefined | | 2 | user_02 | 1 | undefined | | 3 | user_03 | 1 | undefined | #### delete &amp;nbsp;&amp;nbsp;InnoDB会为删除的每一行保存当前系统的版本号(事务的ID)作为删除标识，只会查询出删除版本号大于当前事务版本号(保证事务开始前该记录未删除)的记录。 ![delete](理论类/images/tx_mvcc_delete.png) 步骤2：事务1查询结果为三条记录；步骤3：事务2删除id=2的记录；步骤4：事务1查询结果为三条记录；这是事务2未提交步骤5：事务2提交；步骤6：事务1查询结果依旧为三条记录；事务1提交后查询结果为最新数据1234567891011121314151617181920212223我们看看事务2提交后实际的存储情况： | id | name | 创建版本号 | 删除版本号 | |:--:|:--:|:--:|:--:| | 1 | user_01 | 1 | undefined | | 2 | user_02 | 1 | 3 | | 3 | user_03 | 1 | undefined | 因为查询时会查询出来删除版本号大于当前事务号的记录，所以事务2提交了删除，事务1还是保证了重复读。#### select InnoDB会根据以下两个条件检查每行记录: a.InnoDB只会查找版本早于当前事务版本的数据行(也就是,行的系统版本号小于或等于事务的系统版本号)，这样可以确保事务读取的行，要么是在事务开始前已经存在的，要么是事务自身插入或者修改过的. b.行的删除版本要么未定义,要么大于当前事务版本号,这可以确保事务读取到的行，在事务开始之前未被删除. 只有a,b同时满足的记录，才能返回作为查询结果. #### update InnoDB执行UPDATE，实际上是新插入了一行记录，并保存其创建时间为当前事务的ID，同时保存当前事务ID到要UPDATE的行的删除时间. ```sqlbegin; //当前事务id=4update user set name='user_3' where id=3;commit; 实际的存储情况如下所示： id name 创建版本号 删除版本号 1 user_01 1 undefined 2 user_02 1 3 3 user_03 1 4 3 user_3 5 undefined 结论有了MVCC就可以在不加锁的情况下读取数据，同时采用乐观更新的策略提高并发度。 Innodb在Repeatable-Read隔离级别下如何避免幻读 在快照读情况下，mysql通过mvcc来避免幻读。 在当前读情况下，mysql通过next-key lock来避免幻读。","tags":[{"name":"mysql","slug":"mysql","permalink":"https://cnkeep.github.io/tags/mysql/"}]},{"title":"02_Innodb存储引擎事务的隔离级别","date":"2018-10-06T00:58:00.000Z","path":"2018/10/06/02-02_Innodb存储引擎事务的隔离级别/","text":"Innodb存储引擎事务的隔离级别","tags":[{"name":"mysql","slug":"mysql","permalink":"https://cnkeep.github.io/tags/mysql/"}]},{"title":"Mysql主从同步","date":"2018-10-05T23:37:00.000Z","path":"2018/10/06/04-Mysql主从同步/","text":"Mysql主从同步 标签：Mysql, 主从同步 1. 介绍1.1 主从复制概念Mysql主从复制是指数据可以从一个Mysql数据库主节点复制到所有的从节点中，从而保证所有节点的数据一致性。 1.2 主从复制主要的用途 读写分离 业务中大部分的场景是读多写少，然而写操作需要锁表，导致读操作阻塞，从而降低了系统的吞吐量，假如可以读写分离，写操作在主节点，读操作在从节点，在利用同步机制保证数据的同步，这样便可以提高系统的处理能力。 高可用 2. 原理介绍当master服务器的数据发生改变时，会将其记录进二进制文件binlog日志中，salve服务器会在一定时间间隔内对master二进制日志进行探测其是否发生改变，如果发生改变，则开始一个I/OThread请求master二进制事件，同时主节点为每个I/O线程启动一个dump线程，用于向其发送二进制事件，并保存至从节点本地的中继日志中，从节点将启动SQL线程从中继日志中读取二进制日志，在本地重放，使得其数据和主节点的保持一致，最后I/OThread和SQLThread将进入睡眠状态，等待下一次被唤醒。流程图如下： 2.1 复制方式mysql复制主要有三种方式： 基于SQL语句的复制(statement-based replication) 基于行的复制(row-based replication) 混合模式复制(mixed-based replication)。 对应的，binlog的格式也有三种：STATEMENT，ROW，MIXED。 STATEMENT模式 每一条会修改数据的sql语句会记录到binlog中。优点是并不需要记录每一条sql语句和每一行的数据变化，减少了binlog日志量，节约IO，提高性能。缺点是在某些情况下会导致master-slave中的数据不一致(如sleep()函数， last_insert_id()，以及user-defined functions(udf)等会出现问题) ROW模式（RBR） 不记录每条sql语句的上下文信息，仅需记录哪条数据被修改了，修改成什么样了。而且不会出现某些特定情况下的存储过程、或function、或trigger的调用和触发无法被正确复制的问题。缺点是会产生大量的日志，尤其是alter table的时候会让日志暴涨。 MIXED模式（MBR） 以上两种模式的混合使用，一般的复制使用STATEMENT模式保存binlog，对于STATEMENT模式无法复制的操作使用ROW模式保存binlog，MySQL会根据执行的SQL语句选择日志保存方式。 3. 环境搭建3.1 服务器配置 Ip 配置 172.16.22.135 Centos7 2CPU 2G内存 40G硬盘, Mysql5.7.17 172.16.22.136 Centos7 2CPU 2G内存 40G硬盘, Mysql5.7.17 本次模拟搭建一主一从结构 3.2 Master配置3.2.1 配置文件修改12345678910111213141516171819202122232425262728293031323334353637# vi /etc/my.cnf #修改配置文件[mysqld]#数据存储目录datadir=/var/lib/mysqlsocket=/var/lib/mysql/mysql.sock#忽略大小写lower_case_table_names=1port = 3306#error日志log-error=/var/log/mysqld.logpid-file=/var/run/mysqld/mysqld.pid#生成binlog日志log-bin=/var/lib/mysql/mysql-bin#服务ID，用于区分服务，范围1~2^32-1,需要与从服务器不同server_id=135#MySQL 磁盘写入策略以及数据安全性#每次事务提交时MySQL都会把log buffer的数据写入log file，并且flush(刷到磁盘)中去innodb_flush_log_at_trx_commit=1#当sync_binlog =N (N&gt;0) ，MySQL 在每写 N次 二进制日志binary log时，会使用fdatasync()函数将它的写二进制日志binary log同步到磁盘中去。#sync_binlog 的默认值是0，像操作系统刷其他文件的机制一样，MySQL不会同步到磁盘中去而是依赖操作系统来刷新binary log。sync_binlog=1#同步数据库，如果多库，就以此格式另写几行即可binlog-do-db=test_0001#无需同步的数据库binlog-ignore-db=mysqlbinlog-ignore-db=performance_schemabinlog-ignore-db=information_schema#mysql复制模式binlog_format=ROW#binlog过期清理时间expire_logs_days=7#binlog每个日志文件大小max_binlog_size=20M 3.2.2 重启服务1234567# systemctl restart mysqld# mysql -uroot -p123456 -e &apos;show master status&apos;+------------------+----------+--------------+------------------+-------------------+| File | Position | Binlog_Do_DB | Binlog_Ignore_DB | Executed_Gtid_Set |+------------------+----------+--------------+------------------+-------------------+| mysql-bin.000001 | 573 | | | |+------------------+----------+--------------+------------------+-------------------+ 记录下上面查询出来的File和Position 3.2.3 创建并授权用户123456# mysql -uroot -ppassword:mysql&gt; GRANT REPLICATION SLAVE ON *.* TO &apos;root&apos;@&apos;172.16.22.%&apos; IDENTIFIED BY &apos;123456&apos; WITH GRANT OPTION;mysql&gt; FLUSH PRIVILEGES;mysql&gt; FLUSH tables with read lock; #锁定数据库为只读，确保备份数据一致性 3.2.4 备份数据库123# mysqldump -u root -p --master-data test_0001&gt; dbdump.sql #备份test_0001数据库# 备份所有数据库采用mysqldump -u root -p --all-databases --master-data &gt; dbdump.sql 将dbdump.sql在从服务器执行，然后在主服务器执行mysql -uroot -p123456 -e &#39;unlock tables&#39; 3.3 Slave配置3.3.1 配置文件修改12345678910111213141516171819202122232425262728293031#从库日志记录文件位置或名称前缀log-bin=/var/lib/mysql/mysql-binrelay_log=/var/lib/mysql/mysql-relay-bin#同步日志记录的频率，1为每条都记录，安全但效率低sync_binlog=1#server的id，不能与相同id的mysql主从连接server-id=136#从库日志忽略的数据库名称，不记录#这里记录从库的binlog是为了安全，如果觉得没必要，可以去掉从库binlog的配置binlog-ignore-db = mysqlbinlog-ignore-db = performance_schemabinlog-ignore-db = information_schema#此处添加需要同步的数据库名称，那么它会只接收这个数据库的信息，多个数据库需同步按照此格式另写几行即可#这里同步数据有两种思路，一种是主服务器只发从库需要的，在主库指定；一种是主服务器把所有数据同步过来，从库按需过滤接收#为了让配置更详细些，此处配置了从库过滤接收的配置replicate-do-db=test_0001#忽略接收的库名replicate-ignore-db=mysqlreplicate-ignore-db=performance_schemareplicate-ignore-db=information_schema#跳过所有错误继续slave-skip-errors=all#设置延时时间slave-net-timeout=60#mysql复制模式binlog_format=ROW#binlog过期清理时间expire_logs_days=7#binlog每个日志文件大小max_binlog_size=20M 3.3.2 重启1# systemctl restart mysqld 3.3.3 配置Master位置12345678910# mysql -uroot -ppassword:mysql&gt; CHANGE MASTER TO MASTER_HOST=&apos;172.16.22.135&apos;, MASTER_USER=&apos;repl&apos;, MASTER_PASSWORD=&apos;repl&apos;, MASTER_PORT=3306, MASTER_LOG_FILE=&apos;mysql-bin.000001&apos;, MASTER_LOG_POS= 573, MASTER_CONNECT_RETRY=30;# 注意上面的master二进制文件名和pos要和master信息一致 3.3.4 启动slave123#启动slave进程mysql&gt; slave start;mysql&gt; show slave statue\\G; 可以看到启动成功，如果此步失败，请排查网络问题，或者查看出错日志/var/log/mysqld.log, /var/log/message 3.4 测试在master上新建表，插入数据，观察slave数据库中是否同步了数据。123456#master上执行mysql&gt; show slave hosts;mysql&gt; show mater status;#slave上执行，对比master上的binlog信息mysql&gt; show slave status\\G; 4. 彩蛋4.1 Error-UUID重复发现原来是Mysql的一个配置文件auto.cnf里面记录了mysql服务器的uuid。 server_uuid:服务器身份ID。在第一次启动Mysql时，会自动生成一个server_uuid并写入到数据目录下auto.cnf文件里。原来是这个uuid和主服务器的uuid重复了。经过修改auto.cnf文件中的server-uuid，重启mysql服务器，再查看mysql从节点的状态，终于成功了。 4.2 命令拓展123mysql&gt; stop slave; #停止mysql&gt; reset slave; #重置mysql&gt; delete from user Where user=&apos;repl&apos; and host=&apos;172.16.22.%&apos;; #删除主服务器配置的连接slave用户","tags":[{"name":"mysql","slug":"mysql","permalink":"https://cnkeep.github.io/tags/mysql/"}]},{"title":"Mysql解决断电启动失败问题","date":"2018-10-05T23:24:00.000Z","path":"2018/10/06/03-Mysql解决断电启动失败问题/","text":"Mysql解决断电启动失败的问题现象还原 Windows上运行着Mysql, 而且是开机自启 有一天正在写代码，突然断电了…………… 电来了，开机，MySQL启动失败，mmp………123C:\\WINDOWS\\system32&gt;net start mysqlMySQL 服务正在启动 ........MySQL 服务无法启动。 问题排查1. 查看启动日志启动日志在数据存放目录下(配置文件中指定的datadir目录)，一个后缀为.err的文件, 查看发现了如下的错误: 发现原来是我之前手动删除过data目录下的数据(不要学我), 2. 解决方案 1.停止mysql12345C:\\WINDOWS\\system32&gt;tasklist|findstr &quot;mysql&quot;mysqld.exe 6744 Services 0 378,652 KC:\\WINDOWS\\system32&gt;taskkill /f -pid 6744成功: 已终止 PID 为 6744 的进程。 3.设置recoveryhttps://blog.csdn.net/lyhdream/article/details/787464392.删除文件 删除data目录下的ib_logfile0, ib_logfile1, ibtmp1三个文件，重启数据库123C:\\WINDOWS\\system32&gt;net start mysqlMySQL 服务正在启动 ...MySQL 服务已经启动成功。 3.刷新数据1mysqladmin -u root -p flush-tables","tags":[{"name":"mysql","slug":"mysql","permalink":"https://cnkeep.github.io/tags/mysql/"}]},{"title":"隐藏版本号","date":"2018-10-05T01:23:00.000Z","path":"2018/10/05/03-隐藏版本号/","text":"隐藏Nginx的版本号介绍处于安全考虑，在系统中的Nginx需要隐藏其自身的版本号，防止被不法分子获取后进行攻击。本次我们就来了解下如何隐藏版本号。 操作12345678# vi /usr/local/nginx/conf/nginx.conf# 在http模块配置中增加server_tokens off;即可http &#123; #隐藏nginx版本信息 server_tokens off;&#125;# nginx -s reload 重载配置后，访问原有页面就发现版本号隐藏了。","tags":[{"name":"Nginx","slug":"Nginx","permalink":"https://cnkeep.github.io/tags/Nginx/"}]},{"title":"设置开机启动项","date":"2018-10-04T21:51:00.000Z","path":"2018/10/05/02-设置开机启动项/","text":"设置开机启动项要是想让nginx开机自启我们就需要进行相关配置了，通过yum安装的无需配置，只需要设置systemctl enable nginx即可，本次我们讨论的是通过压缩包安装的方式实现开机自启的方法。 编写nginx.service脚本 1234567891011121314151617$ vi /user/lib/systemc/system/nginx.service[Unit]Description=Nginx ServiceAfter=network.target[Service]#后台形式运行Type=forking#nginx安装绝对路径ExecStart=/usr/local/nginx/sbin/nginxExecStop=/usr/local/nginx/sbin/nginx -s stop#分配独立的临时空间PrivateTmp=true[Install]WantedBy=multi-user.target 2.建立软连接，映射到启动项目录1234567$ ln -s /usr/lib/systemd/system/nginx.service /etc/systemd/system/multi-user.target.wants/nginx.service ``` &gt; 3.系统配置刷新，开启启动项 ```text$ systemctl daemon-reload$ systemctl [start|stop|status] nginx","tags":[{"name":"Nginx","slug":"Nginx","permalink":"https://cnkeep.github.io/tags/Nginx/"}]},{"title":"安装nginx","date":"2018-10-04T18:27:00.000Z","path":"2018/10/05/01-安装nginx/","text":"Nginx是一款轻量级的网页服务器、反向代理服务器。相较于Apache、lighttpd具有占有内存少，稳定性高等优势。它最常的用途是提供反向代理服务。 在Centos下，yum源不提供nginx的安装，可以通过切换yum源的方法获取安装。目前很多像centos7系统已经自带这几个库，所以安装前可以先查看一下本地是否已经存在。存在可直接跳至第四步骤。需要使用安装包编译安装的，如下。以下命令均需root权限执行：首先安装必要的库（nginx 中gzip模块需要 zlib 库，rewrite模块需要 pcre 库，ssl 功能需要openssl库）。选定/usr/local为安装目录，以下具体版本号根据实际改变。 1.安装依赖库12# cd /usr/local/# yum -y install make gcc gcc-c++ zlib zlib-devel openssl openssl-devel pcre pcre-devel 2.安装nginx1234567# cd /usr/local/# sudo tar -zxvf nginx-1.8.0.tar.gz# cd nginx-1.8.0 # sudo ./configure --prefix=/usr/local/nginx #prefix指明安装位置,如果是使用安装包编译的上面几个依赖，需要在在--prefix后面接以下命令:--with-pcre=**** --with-zlib。# sudo make# sudo make install# sudo ln -s /usr/local/nginx/sbin/* /usr/local/sbin 设置软连接，使得可以再任意路径执行nginx命令 3.启动先测试一下配置文件是否正确：12345678910111213141516# /usr/local/nginx/sbin/nginx -t无问题可以启动：# /usr/local/nginx/sbin/nginx检查是否启动成功：打开浏览器访问此机器的 IP，如果浏览器出现 Welcome to nginx! 则表示 Nginx 已经安装并运行成功。部分命令如下：重启：# /usr/local/nginx/sbin/nginx –s reload停止：# /usr/local/nginx/sbin/nginx –s stop测试配置文件是否正常：# /usr/local/nginx/sbin/nginx –t强制关闭：# pkill nginx配置 以上安装方法nginx的配置文件位于123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051## 4.简单的示例 ```text server &#123; listen 5100; server_name localhost; set $root_path &quot;/home/wwwroot&quot;; #charset koi8-r; access_log /home/wwwlogs/host.access.log main; error_log /home/wwwlogs/error.log; location / &#123; root $root_path; index index.html index.htm; &#125; # proxy the Java scripts to JDK listening on 127.0.0.1:9000 location ~ ^/service/ &#123; proxy_set_header Host $http_host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header Cookie $http_cookie; proxy_cookie_path /service/ /; proxy_buffering off; proxy_connect_timeout 300s; proxy_send_timeout 300s; proxy_read_timeout 300s; proxy_pass http://127.0.0.1:9000; &#125; #error_page 404 /404.html; # redirect server error pages to the static page /50x.html # error_page 500 502 503 504 /50x.html; location = /50x.html &#123; root /usr/share/nginx/html; &#125; # deny access to .htaccess files, if Apache&apos;s document root # concurs with nginx&apos;s one # #location ~ /\\.ht &#123; # deny all; #&#125; &#125;","tags":[{"name":"Nginx","slug":"Nginx","permalink":"https://cnkeep.github.io/tags/Nginx/"}]},{"title":"01_存储引擎的分类","date":"2018-10-03T21:47:00.000Z","path":"2018/10/04/02-01_存储引擎的分类/","text":"","tags":[{"name":"mysql","slug":"mysql","permalink":"https://cnkeep.github.io/tags/mysql/"}]},{"title":"03_索引的建立&优化","date":"2018-10-03T20:19:00.000Z","path":"2018/10/04/01-03_索引的建立&优化/","text":"索引的建立&amp;优化建索引的几大规则 查询中与其他表关联的字段，外键关系建立索引; 频繁更新的字段不适合创建索引，因为每次更新还需要更新索引文件; 数据量大于500W或者单表数据大于2G时，此时索引的效果已经不明显了，这是就改考虑分库分表了; 尽量选择区分度高的列作为索引，区分度的公式为：count(distinct &lt;col&gt;)/count(*), 表示字段不重复的额比例，比例越大扫描的记录数越少; 尽量扩展索引，而不要新建索引。例如表中已经有a的索引，现在要加(a,b)的索引，那么只需要修改原有的a索引即可，索引维护也需要代价; 字符串建立索引时可指定长度，不一定非要将整个字段列作为索引; 索引优化 应尽量避免在 where 子句中使用!=或&lt;&gt;操作符，否则将引擎放弃使用索引而进行全表扫描; 能用between…and…就不要用in; 避免使用or来连接条件，否则将导致放弃索引采用索引扫描(使用union代替or); 依据最左前缀匹配原则，like字段使用时避免使用全模糊匹配(%word%)，而应尽量采用右模糊匹配(word%); 避免在where字句中对字段进行函数操作，这将导致放弃使用索引而使用全表扫描;","tags":[{"name":"mysql","slug":"mysql","permalink":"https://cnkeep.github.io/tags/mysql/"}]},{"title":"Mysql5.7-ONLY_FULL_GROUP_BY问题","date":"2018-10-03T18:57:00.000Z","path":"2018/10/04/02-Mysql5.7-ONLY_FULL_GROUP_BY问题/","text":"Mysql5.7-ONLY_FULL_GROUP_BY问题问题复现mysql 5.7之后，进行一些group by查询时，比如:1234567SELECT *, count(id) as count FROM `news` GROUP BY `group_id` ORDER BY `inputtime` DESC LIMIT 20;``` 就会报错： ```sql SELECT list is not in GROUP BY clause and contains nonaggregated column ‘news.id' which is not functionally dependent on columns in GROUP BY clause; this is incompatible with sql_mode=only_full_group_by. 分析与解决分析原因是mysql 5.7模式中，默认启用了ONLY_FULL_GROUP_BY。ONLY_FULL_GROUP_BY是Mysql提供的一个sql_mode,通过这个sql_mode来提供group by合法性的检查。其要求查询的字段必须全部都应该在group by分组条件内。 解决方案123SET @@sql_mode ='STRICT_TRANS_TABLES,NO_ZERO_IN_DATE,NO_ZERO_DATE,ERROR_FOR_DIVISION_BY_ZERO,NO_AUTO_CREATE_USER,NO_ENGINE_SUBSTITUTION';SET @@global.sql_mode ='STRICT_TRANS_TABLES,NO_ZERO_IN_DATE,NO_ZERO_DATE,ERROR_FOR_DIVISION_BY_ZERO,NO_AUTO_CREATE_USER,NO_ENGINE_SUBSTITUTION';SELECT @@global.sql_mode;","tags":[{"name":"mysql","slug":"mysql","permalink":"https://cnkeep.github.io/tags/mysql/"}]},{"title":"02_索引类型&种类","date":"2018-10-02T20:12:00.000Z","path":"2018/10/03/01-02_索引类型&种类/","text":"索引的类型 &amp; 种类 之前已经讨论过关于Innodb索引的实现原理了，那么我们今天就来了解一下mysql的索引到底有哪些 1.索引的类型 Mysql目前主要有以下几种索引类型： FullText(全文索引) Hash(哈希索引) B-Tree(B数索引) R-Tree(空间数据索引) 1.1FullText(全文索引) 介绍 即为全文索引，它的出现是为了解决WHERE name LIKE “%word%”这类针对文本的模糊查询效率较低的问题。它有如下的限制条件： 目前只有MyISAM引擎支持(Innodb5.6以上也支持); 只有 CHAR, VARCHAR, TEXT列上可以创建全文索引; 必须采用指定的函数才会生效:MATCH()…AGAINST 示例`sql– 模拟数据库DROP DATABASE IF EXISTS myisam_db;CREATE DATABASE myisam_db;USE myisam_db; – 文章表DROP TABLE IF EXISTS article;CREATE TABLE article (id INT ( 10 ) PRIMARY KEY AUTO_INCREMENT,title VARCHAR(100) NOT NULL COMMENT ‘标题’,content TEXT COMMENT ‘文章内容’) ENGINE = INNODB DEFAULT CHARSET = utf8 COMMENT ‘文章表’; – 在title，content列上建立全文索引ALTER TABLE article ADD FULLTEXT INDEX fulltext_article(title,content); – 插入模拟数据TRUNCATE TABLE article;INSERT INTO article(title,content) VALUES(‘title001标题’,’content内容1’);INSERT INTO article(title,content) VALUES(‘title002标题’,’content内容2’);INSERT INTO article(title,content) VALUES(‘title003标题’,’content内容3’);INSERT INTO article(title,content) VALUES(‘title004标题’,’content内容4’);INSERT INTO article(title,content) VALUES(‘title005标题’,’content内容5’); – 查询EXPLAIN SELECT FROM article WHERE title LIKE ‘t%’;** 1. ROW *** id: 1 select_type: SIMPLE TABLE: article PARTITIONS: NULL TYPE: ALLpossible_keys: fulltext_article KEY: NULL key_len: NULL ref: NULL ROWS: 5 filtered: 20.00 Extra: USING WHERE EXPLAIN SELECT FROM article WHERE MATCH(title,content) AGAINST(‘t c’);** 1. ROW *** id: 1 select_type: SIMPLE TABLE: article PARTITIONS: NULL TYPE: FULLTEXTpossible_keys: fulltext_article KEY: fulltext_article key_len: 0 ref: const ROWS: 1 filtered: 100.00 Extra: USING WHERE` 1.2Hash(哈希索引) 介绍 通过计算hash至一次定位; 由InnoDB存储引擎自己控制的, DBA无法外界干预; 只能用于等值查找，不能用于范围查找; 不支持排序; 无法避免全表扫描; 1.3B-Tree(B数索引) 介绍 在01-01_索引中已经介绍过索引的BTree结构，如下： 特点： 适合等值查找，范围顺序查找; 适合左前缀模糊查找; 1.4R-Tree(空间数据索引)不常用，忽略 2.索引的种类2.1按照使用类型分以下几种索引种类： 主键索引 单表中只能存在一个主键索引，因为其聚集顺序存储，查询效率非常高(最好保持主键为递增序列，减少分页)。 唯一索引 可以存在多个，列值唯一，不能为null, 加快查询效率。 普通索引 可以存在多个，加快查询效率(可能需要回表, 下文非聚集索引会提及)。 组合索引 多列值组成一个索引，专门用于组合搜索，其效率大于索引合并。 全文索引 分词搜索。 2.2按照存储实现分为2种： 聚集索引 如上图：聚集索引就是按照每张表的主键构造一颗B+树，同时叶子结点存放的即为整张表的行纪录数据（聚集索引的叶子结点也称为数据页）。聚集索引的这个特性也决定了索引组织表中数据也是索引的一部分，和B+树数据结构一样，每个数据页之间都通过一个双向链表来进行链接。 特点： 基于主键的查找速度非常快，范围查找快 非聚集索引 非聚集索引与聚集索引相比： 叶子结点并非数据结点 叶子结点为每一真正的数据行存储一个“键-指针”对 叶子结点中还存储了一个指针偏移量，根据页指针及指针偏移量可以定位到具体的数据行 类似的，在除叶结点外的其它索引结点，存储的也是类似的内容，只不过它是指向下一级的索引页的 当查找的列不在非聚集索引中时，需要再回表一次，这属于随机IO,性能较差","tags":[{"name":"mysql","slug":"mysql","permalink":"https://cnkeep.github.io/tags/mysql/"}]},{"title":"01_索引","date":"2018-10-02T19:37:00.000Z","path":"2018/10/03/01-01_索引/","text":"索引 参考：linhaifeng-第八篇：索引原理与慢查询优化参考：张洋-MySQL索引背后的数据结构及算法原理 【目录】1.介绍2.索引的原理 2.1索引的理念 2.2磁盘IO与预读取 2.3索引的数据结构 1.介绍 为什么要用索引 &nbsp;&nbsp;在数据库操作中，占据主要地位的当属Select操作了，系统80%的操作都来自于系统的查询操作。当数据量大之后，查询的性能将成为系统的瓶颈，这时候就需要一种快速且使用的规则去满足我们查询的需求，所以用索引就是为了提高我们系统的查询效率。索引主要做了两件事：查询和排序. 索引是什么 &nbsp;&nbsp;索引是存储引擎用于快速查找数据记录的一种数据结构。 索引就像字典中的音序表(MYSQL实际实现不是)，要查找某个字，先查找音序表就能快速找到页数，不必一页一页的查找。索引对于性能的影响尤为重要，索引查询优化也是一个程序员的必修课。 2.索引的原理 接下来我们就来了解一下Mysql的Innodb引擎是如何实现索引的 2.1索引的理念&nbsp;&nbsp;以前在学习数据结构时，关于查找算法，我们知道有Hash,二分法，其实质都是通过不断的缩小数据范围，减少需要扫描的数据和次数，数据库查询也是这种理念，但是是结合查询需求和IO考量而实现的。 2.2磁盘IO与预读取 &nbsp;&nbsp;说道数据存储我们不能不考量磁盘IO的影响，那我们来看看我们的磁盘是如何加载数据的。 &nbsp;&nbsp;上图是操作系统读取不同数据的性能对比，可以看到磁盘读取和内存读取差了好几个数据量，那么数据库几百万的数据读写，磁盘的性能读写优化也很重要。&nbsp;&nbsp;考虑到磁盘IO是非常昂贵的操作，计算机系统做了一些优化-预读取。当一次IO操作是，不止加载当前磁盘地址的数据进内存，而且会把相邻地址的数据也会读取到内存中，因为局部预读性原理告诉我们，当计算机访问一个地址的数据的时候，与其相邻的数据也会很快被访问到。每一次IO读取的数据我们称之为一页(page)。具体一页有多大数据跟操作系统有关，一般为4k或8k，也就是我们读取一页内的数据时候，实际上才发生了一次IO，这个理论对于索引的数据结构设计非常有帮助。 2.3索引的数据结构 &nbsp;&nbsp;Innodb存储引擎采用B+Tree`实现索引结构，Innodb的数据文件本身就是索引文件(MyISAM索引和数据文件是分离的)。 如图是Innodb主索引的示意图，它有如下特点： 每一个节点都对应着一个磁盘块，一个磁盘块的大小与操作系统的页大小有关，可以一次加载整个数据块到内存中; 这棵树的叶子节点data域保存了完整的数据记录，叶子节点之间连接为一条链。这种索引交聚集索引，因为InnoDB的数据文件本身要按主键聚集，所以InnoDB要求表必须有主键（MyISAM可以没有），如果没有显式指定，则MySQL系统会自动选择一个可以唯一标识数据记录的列作为主键，如果不存在这种列，则MySQL自动为InnoDB表生成一个隐含字段作为主键，这个字段长度为6个字节，类型为长整形。正是因为如此，使用索引查找非常快; 非叶子结点记录着关键字和指向下层数据块的指针，不存储数据记录，这样可以一次加载更多的关键字，减少IO，快速定位叶子节点; 这棵树一般在2~4层，所以IO次数不高 因为索引有序，且子节点相互连接，所以非常适合范围查找 查找过程 例如要查找30，先将根数据块加载到内存，查找判断子数据块的位置，再将子数据块加载进内存，继续查找，直到到达叶子节点，因为叶子节点直接存储着数据记录，所以直接读取即可。 索引注意的点 因为B+的这些性质，这就导致我们在编写sql时要注意以下： 索引字段尽量小：索引字段小每个数据块存储的关键字更多，减少IO,和分页的发生，更快找到叶子节点； 索引的最左匹配特性：当我们使用组合索引时，一定要注意索引的顺序，例如index(A,B,C),查询使用(A,B),(A,C),(A,B,C)均是可以使用到该索引的，但是查询使用(B),(C),(B,C)根据最左匹配规则是无法使用该索引的。 索引尽量保证有序：因为B+数是建立在排序上的，假如主键无序，可能要进行频繁的分页，同时也增加了查找的复杂度(不能使用二分法)。 选择性建立索引：因为索引在插入删除时也需要维护 下节介绍索引的分类","tags":[{"name":"mysql","slug":"mysql","permalink":"https://cnkeep.github.io/tags/mysql/"}]},{"title":"多项目依赖加载同名Class的问题排查","date":"2018-10-02T18:26:00.000Z","path":"2018/10/03/01-多项目依赖加载同名Class的问题排查/","text":"01-多项目依赖加载同名Class的问题排查1.问题概要&nbsp;&nbsp;旧平台有一个用户导入的功能，是基于tomcat比较老了，现在要将这个接口做成独立服务以供外界调用。当我开始接手时，就在想选什么技术，照搬旧系统？ What the fuck! 那么旧的技术架构，那么繁重，还是webapps老式的tomcat war包哎，排查问题太复杂了，最终还是决定选用springboot,采用内置servlet容器去重新发布服务。&nbsp;&nbsp;终于代码写完了，本地eclipse跑通了，打包发布到虚拟机中测试，运行不了？？？？？ 明明有的方法，却报方法不存在的错误…… 问题排查 bug第一步，日志看一看 字面看是Mybatis报的错，SimpleUser类中没有set方法，跳转到代码中查看，该方法是存在的，怎么回事。 猜想：是不是重名了？ 全文检索SimpleUser看看 还真有重名了，再看看项目依赖结构： 发现自己的项目依赖了web-common模块，而web-common模块中存在重名的SimpleUser类, 而web-common模块中的该类确实没有set方法。怀疑：因为重名加载了web-common中的class 验证 通过在jvm启动参数中添加配置，使其打印类加载日志。(-XX:+TraceClassLoading或-verbose:class) 通过日志检索，发现果然是因为加载了web-common下的类导致的。 解决方案 既然已经知道是因为类加载导致的，那我们就强制指定前加载我们正确的类。 12345之前的脚本：java -d64 -XX:MaxPermSize=192M -Xms500M -Xmx1000M -XX:+HeapDumpOnOutOfMemoryError -cp conf/:lib/* Starter 2&gt;&amp;1 &amp;修改后的脚本：java -d64 -XX:MaxPermSize=192M -Xms500M -Xmx1000M -XX:+HeapDumpOnOutOfMemoryError -cp conf/:lib/web-importer.jar:/lib/web-common.jar:lib/* Starter 2&gt;&amp;1 &amp; 启动服务，查看类加载日志问题解决 知识回顾jvm的类加载采用双亲委托模型，一旦类被类加载器加载一次后就不会再加载第二次，一旦我们项目中出现重名的类，就有可能因为类加载器的问题，导致bug出现。 拓展疑问 问什么之前基于war包的tomcat部署方式没有出现过这种问题呢？ tomcat重写了自己的类加载器，加载顺序如下： $java_home/lib 目录下的java核心api $java_home/lib/ext 目录下的java扩展jar包 java -classpath/-Djava.class.path所指的目录下的类与jar包 $CATALINA_HOME/common目录下按照文件夹的顺序从上往下依次加载 $CATALINA_HOME/server目录下按照文件夹的顺序从上往下依次加载 $CATALINA_BASE/shared目录下按照文件夹的顺序从上往下依次加载 我们的项目路径/WEB-INF/classes下的class文件 我们的项目路径/WEB-INF/lib下的jar文件 而旧项目的重写类都在classes目录下，所以优先加载，就没有问题喽~~~","tags":[{"name":"bugs","slug":"bugs","permalink":"https://cnkeep.github.io/tags/bugs/"}]},{"title":"Mysql的介绍和安装","date":"2018-10-02T16:37:00.000Z","path":"2018/10/03/01-Mysql的介绍和安装/","text":"Mysql的介绍与安装 标签：MySql, Linux 介绍MySQL 是最流行的关系型数据库管理系统，在 WEB 应用方面 MySQL 是最好的 RDBMS(Relational Database Management System：关系数据库管理系统)应用软件之一。 安装安装环境 Centos7 获取源官网地址 选择DOWNLOADS =&gt; Archives =&gt; MySQL Community Server选择相应版本下载，这里我下载5.7.17 开始安装 1.建立mysql用户 1234567891011#后面mysql就使用这个用户来运行（注意这也是mysql启动脚本中默认的用户，因此最好不要改名）。$ groupadd mysql$ useradd -r -g mysql mysql（使用-r参数表示mysql用户是一个系统用户，不能登录） #建立目录/usr/local/mysql，后面mysql就安装在这个目录下面。$ mkdir /usr/local/mysql /usr/local/mysql/data#将mysql及其下所有的目录所有者和组均设为mysql:$ cd /usr/local/mysql$ chown mysql:mysql -R . 2.解压 12345#安装mysql的依赖库$yum install libaio #将前面得到的mysql-5.7.11-linux-glibc2.5-x86_64.tar.gz解压至/usr/local/mysql目录下$ tar zxvf mysql-5.7.11-linux-glibc2.5-x86_64.tar.gz -C /usr/local/mysql 3.初始化 1234567891011121314151617181920$ /usr/local/mysql/bin/mysqld \\ --initialize --user=mysql \\ --datadir=/usr/local/mysql/data \\ --basedir=/usr/local/mysql 注意：1. data目录解压后没有，需要手动建立（见上文）；2. mysql5.7和之前版本不同，很多资料上都是这个命令...../scripts/mysql_install_db --user=mysql而5.7版本根本没有这个。初始化成功后出现如下信息：201x-xx-xxT07:10:13.583130Z 0 [Warning] TIMESTAMP with implicit DEFAULT value is deprecated. Please use --explicit_defaults_for_timestamp server option (see documentation for more details).201x-xx-xx T07:10:13.976219Z 0 [Warning] InnoDB: New log files created, LSN=45790201x-xx-xx T07:10:14.085666Z 0 [Warning] InnoDB: Creating foreign key constraint system tables.201x-xx-xx T07:10:14.161899Z 0 [Warning] No existing UUID has been found, so we assume that this is the first time that this server has been started. Generating a new UUID: 1fa941f9-effd-11e5-b67d-000c2958cdc8.201x-xx-xx T07:10:14.165534Z 0 [Warning] Gtid table is not ready to be used. Table &apos;mysql.gtid_executed&apos; cannot be opened.201x-xx-xx T07:10:14.168555Z 1 [Note] A temporary password is generated for root@localhost: q1SLew5T_6K, 注意最后一行，这也是和之有版本不同的地方，它给了root一个初始密码，后面要登录的时候要用到这个密码。 4.配置[可以跳过] 123456789101112131415161718192021$ cp /usr/local/mysql/support-files/my-default.cnf /etc/my.cnf$ vi /etc/my.cnf [client] socket=/var/lib/mysql/mysql.sock [mysqld] #用于找回密码 #skip-grant-tables #忽略表名大小写，linux区分大小写的，windows不区分 lower_case_table_names=1 sql_mode=NO_ENGINE_SUBSTITUTION,STRICT_TRANS_TABLES basedir=/usr/local/mysql datadir=/usr/local/mysql/data port=3306 socket=/var/lib/mysql/mysql.sock character-set-server=utf8如果不把my.cnf拷到/etc下，运行时会出现：mysqld: Can&apos;t change dir to &apos;/usr/local/mysql/data/&apos; (Errcode: 2 - No such file or directory)这样的出错提示，说明它没找到my.cnf中的配置；而去找了程序编译时的默认安装位置：/usr/local/mysql 5.启动 1234$ mysqld_safe&amp;#停止$ service mysqld stop 5.设置开机启动 123456789101112将&#123;mysql&#125;/ support-files/mysql.server 拷贝为/etc/init.d/mysql并设置运行权限 #cp mysql.server /etc/init.d/mysql#chmod +x /etc/init.d/mysql 把mysql注册为开机启动的服务#chkconfig --add mysql 【mysql开启和关闭】#/etc/init.d/mysql start#/etc/init.d/mysql stop#service mysql start 拓展 修改密码1234567$ mysqld_safe --skip-grant-tables &amp;root$ mysql -uroot -p&gt; use mysql&gt; update user set password=PASSWORD(&quot;123456&quot;) where user=&apos;root&apos;;发现报错：ERROR 1054 (42S22): Unknown column &apos;password&apos; in &apos;field list&apos;&gt; show create user //发现密码字段已经发生改变，变为`authentication_string` &gt; update user set authentication_string=password(&apos;123456&apos;) where user=&apos;root&apos;; 设置远程访问 123mysq&gt; use mysql;mysql&gt;grant all privileges on *.* to &apos;root&apos;@&apos;&apos; identified by &apos;123456&apos; with grant option;msyql&gt;flush privileges;","tags":[{"name":"mysql","slug":"mysql","permalink":"https://cnkeep.github.io/tags/mysql/"}]},{"title":"00_Mysql介绍","date":"2018-10-01T15:30:00.000Z","path":"2018/10/01/00-00_Mysql介绍/","text":"Mysql介绍","tags":[{"name":"mysql","slug":"mysql","permalink":"https://cnkeep.github.io/tags/mysql/"}]},{"title":"tcpdump的使用","date":"2018-07-18T16:13:00.000Z","path":"2018/07/19/19-tcpdump的使用/","text":"tcpdump抓包工具的使用1. 介绍&nbsp;&nbsp;面对一些线上的网络问题排查时，经常需要抓取网络数据包进行分析，windows有大名鼎鼎的winshark, 而在linux就少有图形化的抓包工具，幸好有tcpdump工具的存在。 &nbsp;&nbsp; 它可以使用定义的规则抓取网卡上的数据包，便于我们分析数据。 2. 抓包原理2.1 原理介绍 抓包原理 Linux抓包是通过注册一种虚拟的底层网络协议来完成对网络报文(准确的说是网络设备)消息的处理权。当网卡接收到一个网络报文之后，它会遍历系统中所有已经注册的网络协议，当抓包模块把自己伪装成一个网络协议的时候，系统在收到报文的时候就会给这个伪协议一次机会，让它来对网卡收到的报文进行一次处理，此时该模块就会趁机对报文进行窥探，也就是把这个报文完完整整的复制一份，假装是自己接收到的报文，汇报给抓包模块。 注意事项 必须使用root身份执行 要抓取其他主机的数据包，需要开启混杂模式，即抓取任何经过它的数据包，不管这个数据包是不是发给它或者是它发出的。一般而言，Unix不会让普通用户设置混杂模式，因为这样可以看到别人的信息，比如telnet的用户名和密码，这样会引起一些安全上的问题，所以只有root用户可以开启混杂模式，开启混杂模式的命令是：ifconfig en0 promisc, en0是你要打开混杂模式的网卡。 2.2 命令介绍1. 命令格式12345678tcpdump [ -AdDefIKlLnNOpqRStuUvxX ] [ -B buffer_size ] [ -c count ] [ -C file_size ] [ -G rotate_seconds ] [ -F file ] [ -i interface ] [ -m module ] [ -M secret ] [ -r file ] [ -s snaplen ] [ -T type ] [ -w file ] [ -W filecount ] [ -E spi@ipaddr algo:secret,... ] [ -y datalinktype ] [ -z postrotate-command ] [ -Z user ] [ expression ] 2. 选项介绍-A：以ASCII编码打印每个报文（不包括链路层的头），这对分析网页来说很方便-a：将网络地址和广播地址转变成名字-c&lt;数据包数目&gt;：在收到指定的包的数目后，tcpdump就会停止-C：用于判断用 -w 选项将报文写入的文件的大小是否超过这个值，如果超过了就新建文件（文件名后缀是1、2、3依次增加）-d：将匹配信息包的代码以人们能够理解的汇编格式给出-dd：将匹配信息包的代码以c语言程序段的格式给出-ddd：将匹配信息包的代码以十进制的形式给出-D：列出当前主机的所有网卡编号和名称，可以用于选项 -i-e：在输出行打印出数据链路层的头部信息-f：将外部的Internet地址以数字的形式打印出来-F&lt;表达文件&gt;：从指定的文件中读取表达式,忽略其它的表达式-i&lt;网络界面&gt;：监听主机的该网卡上的数据流，如果没有指定，就会使用最小网卡编号的网卡（在选项-D可知道，但是不包括环路接口），linux 2.2 内核及之后的版本支持 any 网卡，用于指代任意网卡-l：如果没有使用 -w 选项，就可以将报文打印到 标准输出终端（此时这是默认）-n：显示ip，而不是主机名-N：不列出域名-O：不将数据包编码最佳化-p：不让网络界面进入混杂模式-q：快速输出，仅列出少数的传输协议信息-r&lt;数据包文件&gt;：从指定的文件中读取包(这些包一般通过-w选项产生)-s&lt;数据包大小&gt;：指定抓包显示一行的宽度，-s0表示可按包长显示完整的包，经常和-A一起用，默认截取长度为60个字节，但一般ethernet MTU都是1500字节。所以，要抓取大于60字节的包时，使用默认参数就会导致包数据丢失-S：用绝对而非相对数值列出TCP关联数-t：在输出的每一行不打印时间戳-tt：在输出的每一行显示未经格式化的时间戳记-T&lt;数据包类型&gt;：将监听到的包直接解释为指定的类型的报文，常见的类型有rpc （远程过程调用）和snmp（简单网络管理协议）-v：输出一个稍微详细的信息，例如在ip包中可以包括ttl和服务类型的信息-vv：输出详细的报文信息-x:/-xx/-X/-XX：以十六进制显示包内容，几个选项只有细微的差别，详见man手册-w:&lt;数据包文件&gt;：直接将包写入文件中，并不分析和打印出来expression：用于筛选的逻辑表达式； 3. 常用选项 抓取指定数目的包(-c选项) 默认情况下tcpdump将一直抓包，直到按下”ctrl+c”中止，使用-c选项可以指定抓包的数量。 将抓到包写入文件中(-w选项) 使用-w选项，可将抓包记录到一个指定文件中，以供后续分析 读取tcpdump保存文件(-r选项) 对于保存的抓包文件，可以使用-r选项进行读取 抓包时不进行域名解析(-n选项) 默认情况下，tcpdump抓包结果中将进行域名解析，显示的是域名地址而非ip地址，使用-n选项，可指定显示ip地址。 显示完整的包(-s0) 4. 表达式介绍 表达式是一个正则表达式，tcpdump利用它作为过滤报文的条件，如果一个报文满足表达式的条件，则这个报文将会被捕获。如果没有给出任何条件，则网络上所有的信息包将会被截获。在表达式中一般如下几种类型的关键字: 类型关键字主要包括host，net，port，缺省是host 传输方向关键字主要包括src（源地址）, dst(目标地址) ,dst or src, dst and src, 缺省是src or dst 协议关键字主要包括fddi,ip ,arp,rarp,tcp,udp等类型 3. 示例1. 监视指定网络接口的数据包 12# tcpdump -i eth1 如果不指定网卡，默认tcpdump只会监视第一个网络接口上所有流过的数据包，一般是eth0，下面的例子都没有指定网络接口。 2. 监视指定主机的数据包12345678910111213141516171819202122232425截获所有210.27.48.1 的主机收到的和发出的所有的数据包 # tcpdump host 210.27.48.1``` ### 3. TCP连接 这里通过telnet工具连接redis测试tcp连接的报文抓取 ```text [root@localhost redis-5.0.3]# tcpdump -i eth1 -v -s 0 port 6379 tcpdump: listening on eth1, link-type EN10MB (Ethernet), capture size 262144 bytes[1] 21:56:41.602246 IP (tos 0x0, ttl 64, id 54118, offset 0, flags [DF], proto TCP (6), length 52)[2] 172.16.22.235.54586 &gt; localhost.localdomain.6379: Flags [S], cksum 0xbe8b (correct), seq 1436544059, win 64240, options [mss 1460,nop,wscale 8,nop,nop,sackOK], length 0[3] 21:56:41.602294 IP (tos 0x0, ttl 64, id 0, offset 0, flags [DF], proto TCP (6), length 52)[4] localhost.localdomain.6379 &gt; 172.16.22.235.54586: Flags [S.], cksum 0x85b9 (incorrect -&gt; 0xdaf5), seq 3525089865, ack 1436544060, win 29200, options [mss 1460,nop,nop,sackOK,nop,wscale 7], length 0[5] 21:56:41.602446 IP (tos 0x0, ttl 64, id 54119, offset 0, flags [DF], proto TCP (6), length 40)[6] 172.16.22.235.54586 &gt; localhost.localdomain.6379: Flags [.], cksum 0x85d3 (correct), ack 1, win 2053, length 0[7] 21:56:48.665457 IP (tos 0x0, ttl 64, id 54123, offset 0, flags [DF], proto TCP (6), length 40)[8] 172.16.22.235.54586 &gt; localhost.localdomain.6379: Flags [F.], cksum 0x85d2 (correct), seq 1, ack 1, win 2053, length 0[9] 21:56:48.665797 IP (tos 0x0, ttl 64, id 58678, offset 0, flags [DF], proto TCP (6), length 40)[10] localhost.localdomain.6379 &gt; 172.16.22.235.54586: Flags [F.], cksum 0x85ad (incorrect -&gt; 0x8cf1), seq 1, ack 2, win 229, length 0[11] 21:56:48.666220 IP (tos 0x0, ttl 64, id 54124, offset 0, flags [DF], proto TCP (6), length 40)[12] 172.16.22.235.54586 &gt; localhost.localdomain.6379: Flags [.], cksum 0x85d1 (correct), ack 2, win 2053, length 0 ^C 6 packets captured 16 packets received by filter 0 packets dropped by kernel 分析12345678[1]~[6] 3次握手 [7]~[12] 4次挥手拿[1]来看，proto： 指明协议是TCPlength：报文长度srcHost:srcPort &gt; destHost:destPort :指明源地址端口和目标地址端口Flags[*]: 指明包类型，S代表SYN; .代表ACK; F代表FIN","tags":[{"name":"Linux","slug":"Linux","permalink":"https://cnkeep.github.io/tags/Linux/"}]},{"title":"EOF的使用","date":"2018-07-18T13:59:00.000Z","path":"2018/07/18/18-EOF的使用/","text":"Linux下EOF的使用1. 介绍如果我们需要往一个文件里自动输入N行内容。如果是少数的几行内容，还可以用echo追加方式，但如果是很多行，那么单纯用echo追加的方式就显得复杂了，这时候就可以使用EOF结合cat命令进行内容的追加了。 2. 用法123&lt;&lt;EOF ...EOF 通过cat配合重定向能够生成文件并追加操作,在它之前先熟悉几个特殊符号:&lt; :输入重定向&gt; :输出重定向&gt;&gt; :输出重定向,进行追加,不会覆盖之前内容&lt;&lt; :标准输入来自命令行的一对分隔号的中间内容. 3.示例1234567# cat &lt;&lt;EOF&gt;test.sh&gt; ssss&gt; lllll&gt; EOF# more test.sh sssslllll","tags":[{"name":"Linux","slug":"Linux","permalink":"https://cnkeep.github.io/tags/Linux/"}]},{"title":"which_where_find","date":"2018-07-17T12:46:00.000Z","path":"2018/07/17/17-which_where_find/","text":"which, whereis, find的区别","tags":[{"name":"Linux","slug":"Linux","permalink":"https://cnkeep.github.io/tags/Linux/"}]},{"title":"挂载","date":"2018-07-16T09:42:00.000Z","path":"2018/07/16/16-挂载/","text":"https://blog.csdn.net/csh86277516/article/details/78844830https://blog.csdn.net/qq_39521554/article/details/79501714","tags":[{"name":"Linux","slug":"Linux","permalink":"https://cnkeep.github.io/tags/Linux/"}]},{"title":"proc目录探查系统信息","date":"2018-07-16T07:40:00.000Z","path":"2018/07/16/16-proc目录探查系统信息/","text":"/proc目录探查系统信息 标签：/proc 介绍/proc 文件系统下的多种文件提供的系统信息不是针对某个特定进程的, 而是能够在整个系统范围的上下文中使用。可以使用的文件随系统配置的变化而变化。命令procinfo 能够显示基于其中某些文件的多种系统信息。包含：内存，硬盘等 使用 CPU1$ cat /proc/cpuinfo Memory12$ cat /proc/meminfo$ free -m 硬盘12$ fdisk -l$ df -h 其他 123456netstat -lntp # 查看所有监听端口 netstat -antp # 查看所有已经建立的连接 w # 查看活动用户 id # 查看指定用户信息 last # 查看用户登录日志 rpm -qa # 查看所有安装的软件包","tags":[{"name":"Linux","slug":"Linux","permalink":"https://cnkeep.github.io/tags/Linux/"}]},{"title":"yum工具配置","date":"2018-07-15T06:27:00.000Z","path":"2018/07/15/15-yum工具配置/","text":"yum工具配置介绍yum是一款软件包管理器，能够从指定的服务器自动下载RPM包并且安装，还可以自动处理依赖性关系，yum提供了查找、安装、删除某一个，一组甚至全部软件包的命令，而且命令简洁而又好记。 常用命令介绍 1.查找与显示123456# 查找软件yum search &lt;package&gt;#显示安装包信息yum info &lt;package&gt;#显示已经安装和可以安装的程序包yum list 2.安装1yum install &lt;package&gt; 3.更新与升级12345#更新yum update [&lt;package&gt;]#升级程序yum upgrade &lt;package&gt; 4.移除程序1234#移除程序包yum remove &lt;package&gt;##查看依赖yum deplist &lt;package&gt; 5.缓存12345#缓存清除yum clean #生成缓存yum makecache 配置及目录介绍 1.配置文件/etc/yum.conf 1234567891011121314151617181920212223242526[main]cachedir=/var/cache/yum/$basearch/$releaseverkeepcache=0debuglevel=2logfile=/var/log/yum.logexactarch=1obsoletes=1gpgcheck=1plugins=1installonly_limit=5bugtracker_url=http://bugs.centos.org/set_project.php?project_id=23&amp;ref=http://bugs.centos.org/bug_report_page.php?category=yumdistroverpkg=centos-release# This is the default, if you make this bigger yum won&apos;t see if the metadata# is newer on the remote and so you&apos;ll &quot;gain&quot; the bandwidth of not having to# download the new metadata and &quot;pay&quot; for it by yum not having correct# information.# It is esp. important, to have correct metadata, for distributions like# Fedora which don&apos;t keep old packages around. If you don&apos;t like this checking# interupting your command line usage, it&apos;s much better to have something# manually check the metadata once an hour (yum-updatesd will do this).# metadata_expire=90m# PUT YOUR REPOS HERE OR IN separate files named file.repo# in /etc/yum.repos.d 简单介绍一下： cachedir: yum缓存目录，yum在此存储下载的rpm包和数据库 logfile: 日志文件 2.镜像仓库配置目录/etc/yum.repos.d/123$ ls /etc/yum.repos.d/Centos-7.repo CentOS-CR.repo CentOS-fasttrack.repo CentOS-Sources.repo docker-ce.repo_bakCentOS-Base.repo_bak CentOS-Debuginfo.repo CentOS-Media.repo CentOS-Vault.repo 该目录下面的配置文件指明了镜像的源地址配置，我们可以在此配置新的镜像源地址 3.插件等其他的配置文件目录/etc/yum/yum/12$ ls /etc/yum/yum/fssnap.d pluginconf.d protected.d vars version-groups.conf 这里重点介绍pluginconf.d/目录是相关插件的配置目录，后面介绍的fastestmirror插件配置文件就在这里 拓展添加源添加阿里的镜像源 123#配置域名解析# vi /etc/resolve.conf nameserver 8.8.8.8 方式一：12345678910#备份原有配置$ mv /etc/yum.repos.d/Centos-7.repo /etc/yum.repos.d/Centos-7.repo.bak# 下载阿里镜像配置$ sudo yum-config-manager \\ --add-repo \\ http://mirrors.aliyun.com/repo/Centos-7.repo#清理缓存$ sudo yum clean#构建缓存$ sudo yum makecache 方式二:123456789#备份原有配置$ mv /etc/yum.repos.d/Centos-7.repo /etc/yum.repos.d/Centos-7.repo.bak# 下载阿里镜像配置$ wget http://mirrors.aliyun.com/repo/Centos-7.repo$ mv Centos-7.repo /etc/yum.repos.d/Centos-7.repo#清理缓存$ sudo yum clean all#构建缓存$ sudo yum makecache 方式三：12345678910111213141516171819202122232425262728293031323334353637383940414243444546# 查看缓存存储路径$ yum-config-manager|grep base_persistdirpersistdir = /var/lib/yum/repos/x86_64/7base_persistdir = /var/lib/yum/repos/x86_64/7base_persistdir = /var/lib/yum/repos/x86_64/7$ cd /var/lib/yum/repos/x86_64/7$ vi /etc/yum.conf [main] cachedir=/var/cache/yum/$basearch/$releasever keepcache=0 debuglevel=2 logfile=/var/log/yum.log exactarch=1 obsoletes=1 gpgcheck=1 plugins=1 #将plugins的值修改为0 installonly_limit=5$ sudo yum install yum-plugin-fastestmirror$ vi /etc/yum/pluginconf.d/fastestmirror.conf [main] enabled=1 #配置为1 verbose=0 always_print_best_host = true socket_timeout=3 # Relative paths are relative to the cachedir (and so works for users as well # as root). hostfilepath=timedhosts.txt maxhostfileage=10 maxthreads=15 #exclude=.gov, facebook #include_only=.nl,.de,.uk,.ie $ vi /var/cache/yum/x86_64/7/timedhosts.txt #加入下列内容 mirrors.aliyuncs.com 99999999999 mirrors.cloud.aliyuncs.com 99999999999 mirrors.aliyun.com 2.03075098991#清理缓存$ sudo yum clean all#构建缓存$ sudo yum makecache","tags":[{"name":"Linux","slug":"Linux","permalink":"https://cnkeep.github.io/tags/Linux/"}]},{"title":"开机启动项","date":"2018-07-13T02:09:00.000Z","path":"2018/07/13/14-开机启动项/","text":"开机启动项 标签：Redis, systemctl, chkconfig参考：Linux实现开机自动运行普通用户脚本参考：systemctl管理Redis启动、停止、开机启动 前言最近在玩Redis时，因为是放在虚拟机里跑的，但是虚拟机需要经常关机和开机，导致每次都要手动重启redis, 但是我受够了想让它开机自启，于是就有了今天的内容。 设置开机自启方案方案一:使用/etc/rc.d/rc.local自启动脚本文件实现开机自动运行普通用户脚本。 把需要开机启动的脚本程序直接写入/etc/rc.d/rc.local文件中，这样子开机时就会自动执行这些脚本程序，运行对应的服务程序。需要在root环境下编辑。 方案二:使用chkconfig和/etc/init.d 我们都了解/etc/init.d目录下的所有文件都是脚本文件，这个目录下的脚本文件，在设置到开机自启动后，会在开机时自动执行。 1.root账号编写自启动脚本12345678$ vi /etc/init.d/redis#!/bin/bash# redis auto start scripts#chkconfig: 235 80 30 --235指定的启动级别，在哪写启动级别下启动；--80 启动的优先级；--30 关闭的优先级su /usr/local/app/redis-5.0.3/src/redis-server /usr/local/app/redis-5.0.3/redis.conf --daemonize no 等级0表示：表示关机等级1表示：单用户模式等级2表示：无网络连接的多用户命令行模式等级3表示：有网络连接的多用户命令行模式等级4表示：不可用等级5表示：带图形界面的多用户模式等级6表示：重新启动 2.添加执行权限1$ chmod +x /etc/init.d/redis 3.加入启动项配置1234#假如启动项$ chkconfig --add redis #设置开机启动$ chkconfig redis on 4.查看启动项1$ chkconfig --list 方案三(推荐):使用systemctl和/lib/systemd/system/ 1.编写脚本1234567891011121314$ vi /lib/systemd/system/redis.service#写入以下内容[Unit]Description=Redis_5.0.1After=network.target[Service]#redis安装绝对路径ExecStart=/usr/local/app/redis-5.0.3/src/redis-server /usr/local/app/redis-5.0.3/redis.conf --daemonize noExecStop=/usr/local/app/redis-5.0.3/src//redis-cli -h 127.0.0.1 -p 6379 shutdown[Install]WantedBy=multi-user.target [Unit] 表示这是基础信息 Description 是描述 After 是在那个服务后面启动，一般是网络服务启动后启动 [Service] 表示这里是服务信息 ExecStart 是启动服务的命令 ExecStop 是停止服务的指令 [Install] 表示这是是安装相关信息 WantedBy 是以哪种方式启动：multi-user.target表明当系统以多用户方式（默认的运行级别）启动时，这个服务需要被自动运行。 详细请移步至：CoreOS实践指南（八）：Unit文件详解 2.设置开机启动123456789$ ln -s /lib/systemd/system/redis.service /etc/systemd/system/multi-user.target.wants/redis.service#刷新配置$ systemctl daemon-reload#开启开机自启功能$ systemctl enable redis$ systemctl [start|stop|restart|status] redis 3.查看启动项1$ systemctl list-unit-files *","tags":[{"name":"Linux","slug":"Linux","permalink":"https://cnkeep.github.io/tags/Linux/"}]},{"title":"wc命令","date":"2018-07-11T23:02:00.000Z","path":"2018/07/12/13-wc命令/","text":"1234567891011wc -l #统计行数 -c, --bytes print the byte counts -m, --chars print the character counts -l, --lines print the newline counts --files0-from=文件 从指定文件读取以NUL 终止的名称，如果该文件被 指定为&quot;-&quot;则从标准输入读文件名 -L, --max-line-length 显示最长行的长度 -w, --words 显示单词计数 --help 显示此帮助信息并退出 --version 显示版本信息并退出","tags":[{"name":"Linux","slug":"Linux","permalink":"https://cnkeep.github.io/tags/Linux/"}]},{"title":"echo转义&&换行","date":"2018-07-10T20:57:00.000Z","path":"2018/07/11/12-echo转义&&换行/","text":"echo中的转义与换行经常使用echo命令，但是不知道怎么转义可不换行，今天用到了，做一下笔记。 1.原样输出(使用单引号)12345678910111213echo &apos;$name \\n end&apos;#output:# $name \\n end``` &gt; 2.显示转义字符(-e) ```textecho -e &apos;$name \\n end&apos;#output:# $name # end 3.不换行(\\c)12345echo -e &quot;$name \\n end \\c&quot; &amp;&amp; echo &quot;===&quot; #output:# $name # end ===","tags":[{"name":"Linux","slug":"Linux","permalink":"https://cnkeep.github.io/tags/Linux/"}]},{"title":"head&tail命令","date":"2018-07-10T16:44:00.000Z","path":"2018/07/11/11-head&tail命令/","text":"head &amp; tail命令命令功能&nbsp;&nbsp;倒序或者顺序查看文件内容 命令格式12$tail [filename]$head [filename] 常用参数示例 倒序查看100行1$tail -n 100 [filename] 倒序查看文件动态刷新，用于日志观察1$tail -f [filename] 查看文件前10行1$head -n 10 [filename]","tags":[{"name":"Linux","slug":"Linux","permalink":"https://cnkeep.github.io/tags/Linux/"}]},{"title":"vi命令","date":"2018-07-08T13:32:00.000Z","path":"2018/07/08/10-vi命令/","text":"vi命令命令功能&nbsp;&nbsp;vi命令是linux提供的强大的文本编辑工具，接下来我们就常用的功能做一下记录(非全部)。 vi查找 当你用vi打开一个文件后，因为文件太长，如何才能找到你所要查找的关键字呢？ /或者?,在命令模式下敲斜杆(/)这时在状态栏（也就是屏幕左下脚）就出现了 “/”然后输入你要查找的关键字敲回车就可以了。如果你要继续查找此关键字，敲字符n就可以继续查找了。值得注意的是“/”是向下查找，而“?”是向上查找，而在键盘定义上“?”刚好是“/”的上档符。 vi替换： vi/vim 中可以使用 ：s 命令来替换字符串以前只会使用一种格式来全文替换，这里只说部分 123456：s/vivian/sky/ 替换当前行第一个 vivian 为 sky：s/vivian/sky/g 替换当前行所有 vivian 为 sky：n,$s/vivian/sky/ 替换第 n 行开始到最后一行中每一行的第一个 vivian 为 sky：n,$s/vivian/sky/g 替换第 n 行开始到最后一行中每一行所有 vivian 为 sky n 为数字，若 n 为 .，表示从当前行开始到最后一行：%s/vivian/sky/（等同于：g/vivian/s//sky/）替换每一行的第一个 vivian 为 sky：%s/vivian/sky/g（等同于：g/vivian/s//sky/g）替换每一行中所有 vivian 为 sky 鼠标移动操作12345命令模式下``H,J,K,L``字符可以实现鼠标跳转，这几个键都是紧挨着的，操作多方便哦！ H:向前移动J:向下移动K:向上移动L:向后移动 行数操作12345#显示行号:set nu跳转到第10行:10 翻页12Ctrl+B: 上一页Ctrl+F: 下一页 复制，粘贴，撤销，删除1234yy: 复制当前行p: 粘贴u: 撤销更改dd: 删除","tags":[{"name":"Linux","slug":"Linux","permalink":"https://cnkeep.github.io/tags/Linux/"}]},{"title":"03_通过Dockerfile构建自己的镜像文件","date":"2018-07-07T15:24:00.000Z","path":"2018/07/07/01-03_通过Dockerfile构建自己的镜像文件/","text":"通过Dockerfile构建自己的镜像文件目录Dockerfile是什么Dockerfile指令介绍示例-构建jdk镜像Dockerfile是什么&emsp;&emsp;Dockerfile是由一系列命令和参数构成的脚本，这些命令应用于基础镜像并最终创建一个新的镜像。其产出为一个新的可以用于创建容器的镜像。 Dockerfile指令 FROM &emsp;&emsp;FROM指令用于指定当前镜像所使用的的基础镜像，一般写在文件开头，如果想自定义构建docker镜像，那么引用的基础镜像一般是：centos, debian, ubuntu。 指令语法：1234567FROM &lt;image&gt;FROM &lt;image&gt;:&lt;tag&gt;FROM &lt;image&gt;:&lt;digest&gt;示例：FROM centos:v1 MAINTAINER &emsp;&emsp;MAINTAINER用于描述当前Dockerfile的文件信息指令语法：12345MAINTAINER &lt;name&gt;示例：MAINTAINER LeiLi.Zhang &lt;zhangleili924@gmail.com&gt; 3.RUN &emsp;&emsp;运行指定命令，此命令只有在执行docker build构建镜像时才会执行，Dokcerfile中的命令每执行一条即产生一个新的镜像，当前命令总是在最新的镜像上执行，所以为了避免缓存之类的影响，应尽量将多条命令合并为一条执行，每条RUN指令将在当前镜像基础上执行指定命令，并提交为新的镜像。当命令较长时可以使用\\来换行。 指令格式：1234567RUN &lt;command&gt;示例：RUN yum update \\&amp;&amp; yum install openjdk-8-jdk -y \\&amp;&amp; yum clean all 4.CMD &emsp;&emsp;设置容器启动时要执行的命令，只有在执行run 或者 start时才会运行，假如有多条命令只会执行最后一条，执行会覆盖。指令格式：1CMD [&quot;java -version&quot;] 5.EXPOSE 设置容器的暴露端口，注意并不是指暴露到物理机上的端口号！！！指令语法：1EXPOSE port 6.ENV 此指令为设置环境变量指令语法：1234ENV &lt;key&gt; &lt;value&gt;示例：ENV JAVA_HOME /usr/local/jdk 7.ADD 该指令的功能是把宿主机文件复制到镜像中，目录会自动创建。指令格式：12345ADD &lt;src&gt; &lt;dest&gt; src可以是网络资源示例：ADD /usr/local/app/jdk /usr/lib/jdk 绝对路径方式：将宿主机/usr/local/app/jdk目录拷贝到镜像/usr/lib/jdk目录ADD jdk /usr/lib/jdk 相对路径方式：将相对当前Dockerfile文件路径下的jdk目录拷贝至惊醒/usr/lib/jdk目录 8.COPY 此命令与ADD命令功能相似，不同的是，src只能是本地文件，且文件路径是Dockerfile的相对路径指令格式：1234COPY &lt;src&gt; &lt;dest&gt;示例：COPY jdk /usr/lib/jdk 将相对当前Dockerfile文件路径下的jdk目录拷贝至惊醒/usr/lib/jdk目录 9.VLOUME &emsp;&emsp;设置你的卷，在启动容器的时候Docker会在/var/lib/docker/的下一级目录下创建一个卷，以保存你在容器中产生的数据。若没有申明则不会创建。(可以把此指令看成shell中的mkdir）此指令不是独立数据卷，数据会随着容器的停止而消失，如果想数据持久化，请参考docker 简单命令 在run启动容器是加-V 参数！ 指令格式：1VLOUME [&quot;&quot;] 10.WORKDIR 指定容器的工作目录，可以在构建镜像的时候使用，也可以在启动容器的时候使用，构建使用是通过WORKDIR将当前目录切换到指顶目录中，可以理解为shell的cd，启动容器的时候使用的意思为 docker run 启动容器时，默认进入到目录是WORKDIR 指定的。 指令语法：1WORKDIR /usr 11.ENTRYPOINT RNTRYPONT指令与CMD指令的作用类似，都是在容器启动时执行相关的指令，不同的是， CMD中的参数会被启动时指定的动态参数替换掉，而ENTRYPOINT不会被替换掉。 CMD和ENTRYPOINT同时存在时，CMD中的指令会被一般结合这两者进行配置，ENTRYPOINT设置指令，CMD设置参数, 例如： 123#DockerfileENTRYPOINT [&quot;/bin/ping&quot;,&quot;-c&quot;,&quot;3&quot;]CMD [&quot;localhost&quot;] /bin/ping -c 3 localhost 示例-构建jdk镜像这里示例制作一个基于nginx镜像的jdk镜像 1.下载jdk 这里使用的是jdk1.8.0_172，通过工具上传到服务器/usr/local/dock_file目录（请自行事先建立该目录）12$ls /usr/local/docker_fileDockerfile jdk1.8.0_172 2.创建Dockerfile文件1234567891011121314151617181920212223$ cd /usr/local/docker_file$ touch Dockerfile#vi Dockerfile追加以下内容：#这里的基础镜像为nginxFROM nginxMAINTAINER LeiLi.Zhang#切换镜像目录WORKDIR /usrRUN mkdir jdk#将宿主机当前目录下的jdk1.8.0_172拷贝至镜像的/user/jdk目录下ADD jdk1.8.0_172 /usr/jdk$ 设置环境变量ENV JAVA_HOME=/usr/jdkENV JRE_HOME=$JAVA_HOME/jreENV CLASSPATH=.:$CLASSPATH:$JAVA_HOME/bin/dt.jar:$JAVA_HOME/lib/tools.jarENV PATH=$PATH:$JAVA_HOME/binCMD [&quot;java&quot;, &quot;-version&quot;] 3.构建镜像文件 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556$ docker build -f Dockerfile -t nginx_jdk8:1.0 ./Sending build context to Docker daemon 388.7MBStep 1/10 : FROM nginx ---&gt; 06144b287844Step 2/10 : MAINTAINER LeiLi.Zhang ---&gt; Running in 9fd7dfb2fdd4Removing intermediate container 9fd7dfb2fdd4 ---&gt; 8a1b63746989Step 3/10 : WORKDIR /usr ---&gt; Running in 600d738f937fRemoving intermediate container 600d738f937f ---&gt; 337b6327788cStep 4/10 : RUN mkdir jdk ---&gt; Running in e6533a32955eRemoving intermediate container e6533a32955e ---&gt; d18d4dd904d9Step 5/10 : ADD jdk1.8.0_172 /usr/jdk_8 ---&gt; 5a537bbcd84cStep 6/10 : ENV JAVA_HOME=/usr/jdk_8 ---&gt; Running in 3db8397affdbRemoving intermediate container 3db8397affdb ---&gt; 596cf637b2a7Step 7/10 : ENV JRE_HOME=$JAVA_HOME/jre ---&gt; Running in c16754fc69e0Removing intermediate container c16754fc69e0 ---&gt; b4b8531e6846Step 8/10 : ENV CLASSPATH=.:$CLASSPATH:$JAVA_HOME/bin/dt.jar:$JAVA_HOME/lib/tools.jar ---&gt; Running in d0be05ab96cfRemoving intermediate container d0be05ab96cf ---&gt; 606d80d75a70Step 9/10 : ENV PATH=$PATH:$JAVA_HOME/bin ---&gt; Running in eedc61c6889bRemoving intermediate container eedc61c6889b ---&gt; f5f3ca76f4dbStep 10/10 : CMD [&quot;java&quot; ,&quot;-version&quot;] ---&gt; Running in 5101d2bc1ebcRemoving intermediate container 5101d2bc1ebc ---&gt; 0118d7df925aSuccessfully built 0118d7df925aSuccessfully tagged nginx_jdk8:1.0``` &gt; 4.查看镜像是否成功 ```text$ docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEnginx_jdk8 1.0 cb6b8f24a584 24 seconds ago 496MBredis latest c188f257942c 7 weeks ago 94.9MBtime69/docker_study centos_base 7121cccf893c 3 months ago 982MBhello-world latest 4ab4c602aa5e 4 months ago 1.84kBtomcat latest 7671687227db 4 months ago 463MBnginx latest 06144b287844 4 months ago 109MBcentos latest 5182e96772bf 5 months ago 200MB 4.使用创建的镜像 123456$ docker run -it --name jdk8 nginx_jdk8:1.0 /bin/bash$ java -versionjava version &quot;1.8.0_172&quot;Java(TM) SE Runtime Environment (build 1.8.0_172-b11)Java HotSpot(TM) 64-Bit Server VM (build 25.172-b11, mixed mode) 5.导出镜像123$ docker save -o jdk_1.8_nginx.tar nginx_jdk8:1.0当面目录生成xxx.tar 6.导入镜像1$ docker load -i jdk_1.8_nginx.tar","tags":[{"name":"Docker","slug":"Docker","permalink":"https://cnkeep.github.io/tags/Docker/"}]},{"title":"user_group_chmod命令","date":"2018-07-07T10:27:00.000Z","path":"2018/07/07/09-user_group_chmod命令/","text":"user相关 &amp; group相关 &amp; chmod命令 本文中的命令参数选项并非全部选项，仅包含常用选项 用户系统&emsp;&emsp;linux是一个多用户多任务的系统，它有用户，用户组的概念，一个用户必须属于一个组。 user相关 与用户相关的配置文件1234567891011121314151617181920212223242526272829303132333435/etc/passwd 用户的配置文件/etc/shadow 用户的影子口令``` &gt; 与用户相关的命令 ```text1. useradd 功能：新建用户 用法：useradd [选项] 登录 useradd -D useradd -D [选项] 选项： -b, --base-dir BASE_DIR 新账户的主目录的基目录 -d, --home-dir HOME_DIR 新账户的主目录 -e, --expiredate EXPIRE_DATE 新账户的过期日期 -f, --inactive INACTIVE 新账户的密码不活动期 -g, --gid GROUP 新账户主组的名称或 ID -G, --groups GROUPS 新账户的附加组列表 -m, --create-home 创建用户的主目录 -M, --no-create-home 不创建用户的主目录 -p, --password PASSWORD 加密后的新账户密码 -r, --system 创建一个系统账户 示例： $ useradd -g zll_group -G root zll #新建用户zll, 用户主组zll_group, 附组root $ passwd zll #设置密码 $ id zll #查看用户zll的组 uid=1122(zll_group) gid=1125(zll_group) groups=1125(zll_group),0(root) $ useradd -s /sbin/nologin test #创建一个不能登录的用户2. usermod 功能：修改用户3. userdel 功能：删除用户 group相关 group相关配置文件 1/etc/group group相关命令 123456789101112131415161. groupadd 功能：新建用户组 用法：groupadd [选项] 用户组 选项： -g GID 指定新用户组的组标识号（GID）。 -o 一般与-g选项同时使用，表示新用户组的GID可以与系统已有用户组的GID相同。 示例： $ groupadd zll_group2. groupmod 功能：修改用户组3. groupdel 功能：删除用户组 文件权限&emsp;&emsp;linux的文件系统拥有严格访问权限，文件的可读可写，所属人，所属组都有严格的限制，我们也可以人为控制修改，这时候就要用到chmod命令了。我们先看看文件都有什么权限：123456789101112131415161718192021222324252627282930311：代表文件类型，为目录是为d2: 代表文件的宿主权限，由三位表示，4：可读，2：可写，1：可执行 3：代表文件的所属组权限4：代表其他用户的权限5：代表文件的所属用户6：代表文件的所属组``` &gt; chmod命令可以修改文件的相关权限 ```text语法： chmod [-cfvR] [--help] [--version] mode file...参数说明： [ugoa...][[+-=][rwxX]...][,...] 其中： u 表示该文件的拥有者，g 表示与该文件的拥有者属于同一个群体(group)者，o 表示其他以外的人，a 表示这三者皆是。 + 表示增加权限、- 表示取消权限、= 表示唯一设定权限。 r 表示可读取，w 表示可写入，x 表示可执行，X 表示只有当该文件是个子目录或者该文件已经被设定过为可执行。 其他参数说明： -c : 若该文件权限确实已经更改，才显示其更改动作 -f : 若该文件权限无法被更改也不要显示错误讯息 -v : 显示权限变更的详细资料 -R : 对目前目录下的所有文件与子目录进行相同的权限变更(即以递回的方式逐个变更) --help : 显示辅助说明 --version : 显示版本 示例： $ chmod +x start.sh #增加执行权限 chown可以修改文件的饿所属者 12345678910111213141516171819语法 chown [-cfhvR] [--help] [--version] user[:group] file...参数 : user : 新的文件拥有者的使用者 ID group : 新的文件拥有者的使用者组(group) -c : 显示更改的部分的信息 -f : 忽略错误信息 -h :修复符号链接 -v : 显示详细的处理信息 -R : 处理指定目录以及其子目录下的所有文件 --help : 显示辅助说明 --version : 显示版本实例 #将文件 file1.txt 的拥有者设为 users 群体的使用者 runoob : $ chown runoob:users file1.txt #将目前目录下的所有文件与子目录的拥有者皆设为 users 群体的使用者 lamport : $ chown -R lamport:users *","tags":[{"name":"Linux","slug":"Linux","permalink":"https://cnkeep.github.io/tags/Linux/"}]},{"title":"sed命令","date":"2018-07-07T09:23:00.000Z","path":"2018/07/07/08-sed命令/","text":"sed命令[注：文中的命令参数列表至列出了常用的几个，并不是全部] sed命令 功能 &emsp;&emsp;sed是一种流编辑器，它是文本处理中非常中的工具，能够完美的配合正则表达式使用, 可以完成文件内容的批量替换，删除等。 使用方式及参数列表 1234567891011121314151617181920212223242526sed [options] &apos;command&apos; file(s)参数列表： g 表示行内全面替换。 p 表示打印行。 i 修改源文件 w 表示把行写入一个文件。 x 表示互换模板块中的文本和缓冲区中的文本。 y 表示把一个字符翻译为另外的字符（但是不用于正则表达式） \\1 子串匹配标记 &amp; 已匹配字符串标记 匹配模式： ^ 匹配行开始，如：/^sed/匹配所有以sed开头的行。 $ 匹配行结束，如：/sed$/匹配所有以sed结尾的行。 . 匹配一个非换行符的任意字符，如：/s.d/匹配s后接一个任意字符，最后是d。 * 匹配0个或多个字符，如：/*sed/匹配所有模板是一个或多个空格后紧跟sed的行。 [] 匹配一个指定范围内的字符，如/[ss]ed/匹配sed和Sed。 [^] 匹配一个不在指定范围内的字符，如：/[^A-RT-Z]ed/匹配不包含A-R和T-Z的一个字母开头，紧跟ed的行。 \\(..\\) 匹配子串，保存匹配的字符，如s/\\(love\\)able/\\1rs，loveable被替换成lovers。 &amp; 保存搜索字符用来替换其他字符，如s/love/**&amp;**/，love这成**love**。 \\&lt; 匹配单词的开始，如:/\\&lt;love/匹配包含以love开头的单词的行。 \\&gt; 匹配单词的结束，如/love\\&gt;/匹配包含以love结尾的单词的行。 x\\&#123;m\\&#125; 重复字符x，m次，如：/0\\&#123;5\\&#125;/匹配包含5个0的行。 x\\&#123;m,\\&#125; 重复字符x，至少m次，如：/0\\&#123;5,\\&#125;/匹配至少有5个0的行。 x\\&#123;m,n\\&#125; 重复字符x，至少m次，不多于n次，如：/0\\&#123;5,10\\&#125;/匹配5~10个0的行。 示例 12345678910111.选项-i,匹配file文件中每一行的第一个book替换为books： $ sed -i &apos;s/book/books/g&apos; file2.-n选项和p命令一起使用表示只打印那些发生替换的行： $ sed -n &apos;s/test/TEST/p&apos; file 3.使用后缀 /g 标记会替换每一行中的所有匹配： $ sed &apos;s/book/books/g&apos; file #不会修改源文件 4.替换文件中所有的6379为6380并生成新文件 $ sed &apos;s/6379/6380/g&apos; redis.conf &gt; redis-6380.conf","tags":[{"name":"Linux","slug":"Linux","permalink":"https://cnkeep.github.io/tags/Linux/"}]},{"title":"时间同步","date":"2018-07-06T08:22:00.000Z","path":"2018/07/06/07-时间同步/","text":"Centos7时间不同步总是早8小时转载-CentOS 7系统时间与实际时间差8个小时 问题场景&emsp;&emsp;自己安装在虚拟机中的Centos时间总是早8个小时，每次都需要我手动改，太麻烦，今天终于决定彻底结局它。 结局方案123456789101112131415161718192021222324252627282930313233$ timedatectl #查看时间 Local time: 三 2018-10-24 23:30:48 CST Universal time: 三 2018-10-24 15:30:48 UTC RTC time: 三 2018-10-24 15:30:48 Time zone: Asia/Shanghai (CST, +0800) NTP enabled: n/aNTP synchronized: no RTC in local TZ: no DST active: n/a $ ls /usr/share/zoneinfo/ #查看时区列表Africa Australia Cuba Etc GMT-0 Indian Kwajalein Navajo posix ROK UTCAmerica Brazil EET Europe GMT+0 Iran Libya NZ posixrules Singapore WETAntarctica Canada Egypt GB Greenwich iso3166.tab MET NZ-CHAT PRC Turkey W-SUArctic CET Eire GB-Eire Hongkong Israel Mexico Pacific PST8PDT UCT zone.tabAsia Chile EST GMT HST Jamaica MST Poland right Universal ZuluAtlantic CST6CDT EST5EDT GMT0 Iceland Japan MST7MDT Portugal ROC US$ rm /etc/localtime #删除原有的时区rm：是否删除符号链接 &quot;/etc/localtime&quot;？y$ sudo ln -s /usr/share/zoneinfo/Universal /etc/localtime #设置新的时区$ timedatectl Local time: 三 2018-10-24 15:46:55 UTC Universal time: 三 2018-10-24 15:46:55 UTC RTC time: 三 2018-10-24 15:46:54 Time zone: Universal (UTC, +0000) NTP enabled: n/aNTP synchronized: no RTC in local TZ: no DST active: n/a[root@localhost default]# date2018年 10月 24日 星期三 15:46:58 UTC","tags":[{"name":"Linux","slug":"Linux","permalink":"https://cnkeep.github.io/tags/Linux/"}]},{"title":"centos7防火墙","date":"2018-07-06T05:08:00.000Z","path":"2018/07/06/06-centos7防火墙/","text":"centos防火墙1234567891011121314151617181920212223242526272829303132333435363738391、firewalld的基本使用启动： systemctl start firewalld查看状态： systemctl status firewalld 停止： systemctl disable firewalld禁用： systemctl stop firewalld 2.systemctl是CentOS7的服务管理工具中主要的工具，它融合之前service和chkconfig的功能于一体。启动一个服务：systemctl start firewalld.service关闭一个服务：systemctl stop firewalld.service重启一个服务：systemctl restart firewalld.service显示一个服务的状态：systemctl status firewalld.service在开机时启用一个服务：systemctl enable firewalld.service在开机时禁用一个服务：systemctl disable firewalld.service查看服务是否开机启动：systemctl is-enabled firewalld.service查看已启动的服务列表：systemctl list-unit-files|grep enabled查看启动失败的服务列表：systemctl --failed3.配置firewalld-cmd查看版本： firewall-cmd --version查看帮助： firewall-cmd --help显示状态： firewall-cmd --state查看所有打开的端口： firewall-cmd --zone=public --list-ports更新防火墙规则： firewall-cmd --reload查看区域信息: firewall-cmd --get-active-zones查看指定接口所属区域： firewall-cmd --get-zone-of-interface=eth0拒绝所有包：firewall-cmd --panic-on取消拒绝状态： firewall-cmd --panic-off查看是否拒绝： firewall-cmd --query-panic 那怎么开启一个端口呢添加firewall-cmd --zone=public --add-port=80/tcp --permanent （--permanent永久生效，没有此参数重启后失效）重新载入firewall-cmd --reload查看firewall-cmd --zone= public --query-port=80/tcp删除firewall-cmd --zone= public --remove-port=80/tcp --permanent","tags":[{"name":"Linux","slug":"Linux","permalink":"https://cnkeep.github.io/tags/Linux/"}]},{"title":"启动脚本中1,2&是什么","date":"2018-07-06T00:57:00.000Z","path":"2018/07/06/05-启动脚本中1,2&是什么/","text":"启动脚本末尾的2&gt;&amp;1 &amp;的含义脚本中2&gt;&amp;1 &amp;的含义 &emsp;&emsp;我们总是在启动脚本中会发现这样的 “2&gt;&amp;1 &amp;”的奇怪部分，那这部分到底是什么含义呢？ linux标准输入输出流的标示&emsp;&emsp;我们知道设备都有输入输出流，那么linux中是如何去标示它们的呢？1230: 标准stdin输入流1：标准stdout输出流2：标准stderr错误流 举例分析 来看这样一个例子 command >2>&1 & ```12345678910我们对命令进行拆分： ![&amp;](images/how_about_&amp;.png) ```text1：我们执行的命令2：命令执行的打印信息流重定向的设备，**/dev/null**表示一个空设备，即重定向到它后不显示任何信息3：代表标准stderr流4：代表标准stdout输出流，这里的**&amp;**表示不把1当做文件看待，不添加&amp;则会把1当做文件看待5：表示以后台job的形式执行命令6：把stderr流重定向到stdout流","tags":[{"name":"Linux","slug":"Linux","permalink":"https://cnkeep.github.io/tags/Linux/"}]},{"title":"重定向符号区别","date":"2018-07-05T22:56:00.000Z","path":"2018/07/06/04-重定向符号区别/","text":"&gt; 和 &gt;&gt; 区别 功能 &emsp;&emsp;都表示重定向到新的设备。 区别 两者的区别主要发生在重定向的设备已经存在时： &gt;会覆盖源文件 &gt;&gt;不会覆盖源文件，而是追加到源文件末尾","tags":[{"name":"Linux","slug":"Linux","permalink":"https://cnkeep.github.io/tags/Linux/"}]},{"title":"lrzsz上传下载工具","date":"2018-07-05T20:44:00.000Z","path":"2018/07/06/03-lrzsz上传下载工具/","text":"lrzsz上传下载工具应用场景&emsp;&emsp;我们使用linux时，经常需要与本机完成文件的传输，我们可以使用xftp, 但是还有更简单的命令。 lrzsz的使用 123$ yum install lrzsz$ rz #弹框选择文件上传到linux当前目录 $ sz [file] #从Linux下载文件","tags":[{"name":"Linux","slug":"Linux","permalink":"https://cnkeep.github.io/tags/Linux/"}]},{"title":"02_Docker命令","date":"2018-07-05T15:21:00.000Z","path":"2018/07/05/01-02_Docker命令/","text":"命令列表 删除Docker12$ sudo yum remove docker-ce$ sudo rm -rf /var/lib/docker 搜索可用镜像1$ docker search [name] 拉取镜像123$ docker pull [name]:[version]其中version可以省略 查看镜像列表1$ docker images 运行容器123456789101112$ docket start [container] 运行一个已经存在的容器$ docker run -p 8080:80 --name test -d [image] 利用镜像image新建一个容器并启动$ docker run -d -i -t [image] /bin/bash 后台运行防止直接退出docker run 等价于 docker create + docker startrun: 利用镜像image创建一个容器并启动-p: 端口映射，容器内80端口映射到外部的8080端口--name: 容器命名-d: 后台运行-v: 文件映射 查看容器列表1$ docker ps -a 进入容器1$ docker exec -it [container] /bin/bash 停止容器1$ docker stop [container] 重启docker守护进程12$ systemctl daemon-reload$ systemctl restart docker 查看容器运行日志1$ docker logs [container] 查看容器信息1$ docker inspect [container] 移除容器1234$ docker rm [container]移除所有容器$ docker rm $(docker ps -a -q) 通过Dockerfile构建镜像1$ docker build -it [image] [path] 通过容器构建镜像1234567$ docker commit -a [author] [container] [repository[:tag]]示例：$ docker commit -a &quot;LeiLi.Zhang&quot; -m &quot;jdk8,nginx1.15,tomcat8.5.23&quot; centos_7 my:centos_base通过容器centos_7构建一个镜像-a 作者-m 提交信息 镜像的导出导入12345导出$ docker save -o xxxx.tar [image]导入$ docker load -i xxx.tar 容器的导入导出12$ docker export [container] &gt; xxx.tar$ cat xxx.tar | docker import - [res]:[tag]： DockerHub1234567拉取镜像$ docker login$ docker pull [name:tag]上传镜像$ docker tag [image:tag] [username/repository:tag] 将本地镜像重命名为标准的名称$ docker push [username/repository:tag] 查看容器/镜像的分层1$ docker history [image] 网络操作123456$ docker network [option]|[contain]# ls 查看网卡# inspect 查看网络信息# create 创建新的网卡# connect 连接到新的网卡","tags":[{"name":"Docker","slug":"Docker","permalink":"https://cnkeep.github.io/tags/Docker/"}]},{"title":"grep命令","date":"2018-07-03T16:30:00.000Z","path":"2018/07/04/02-grep命令/","text":"grep命令[注：文中的命令参数列表至列出了常用的几个，并不是全部] grep命令 功能 &emsp;&emsp;Linux系统中grep命令是一种强大的文本搜索工具，它能使用正则表达式搜索文本，并把匹配的行打印出来. 使用方式及参数列表 1234567891011121314151617181920212223242526272829grep [option] pattern file参数列表： -a --text #不要忽略二进制的数据。 -A&lt;显示行数&gt; --after-context=&lt;显示行数&gt; #除了显示符合范本样式的那一列之外，并显示该行之后的内容。 -b --byte-offset #在显示符合样式的那一行之前，标示出该行第一个字符的编号。 -B&lt;显示行数&gt; --before-context=&lt;显示行数&gt; #除了显示符合样式的那一行之外，并显示该行之前的内容。 -c --count #计算符合样式的列数。 -C&lt;显示行数&gt; --context=&lt;显示行数&gt;或-&lt;显示行数&gt; #除了显示符合样式的那一行之外，并显示该行之前后的内容。 -d &lt;动作&gt; --directories=&lt;动作&gt; #当指定要查找的是目录而非文件时，必须使用这项参数，否则grep指令将回报信息并停止动作。 -e&lt;范本样式&gt; --regexp=&lt;范本样式&gt; #指定字符串做为查找文件内容的样式。 -E --extended-regexp #将样式为延伸的普通表示法来使用。 -f&lt;规则文件&gt; --file=&lt;规则文件&gt; #指定规则文件，其内容含有一个或多个规则样式，让grep查找符合规则条件的文件内容，格式为每行一个规则样式。 -F --fixed-regexp #将样式视为固定字符串的列表。 -G --basic-regexp #将样式视为普通的表示法来使用。 -h --no-filename #在显示符合样式的那一行之前，不标示该行所属的文件名称。 -H --with-filename #在显示符合样式的那一行之前，表示该行所属的文件名称。 -i --ignore-case #忽略字符大小写的差别。 -l --file-with-matches #列出文件内容符合指定的样式的文件名称。 -L --files-without-match #列出文件内容不符合指定的样式的文件名称。 -n --line-number #在显示符合样式的那一行之前，标示出该行的列数编号。 -q --quiet或--silent #不显示任何信息。 -r --recursive #此参数的效果和指定“-d recurse”参数相同。 -s --no-messages #不显示错误信息。 -v --revert-match #显示不包含匹配文本的所有行。 -V --version #显示版本信息。 -w --word-regexp #只显示全字符合的列。 -x --line-regexp #只显示全列符合的列。 -y #此参数的效果和指定“-i”参数相同 示例 12345678910111213141516171. 进程查找 $ ps -ef|grep redis 2. 查找指定进程个数 $ ps -ef|grep -c svn 3. 从文件中读取关键词进行搜索 $ cat test.txt | grep -f test2.txt 4. 找出已u开头的行内容 $ cat test.txt |grep ^u 5. 去除注释和空格显示 $ cat redis.conf |grep -v &quot;#&quot;|grep -v &quot;^$&quot; &gt; redis-template.conf 6. 显示匹配行后面几行 $ grep -A &apos;options&apos; my.cnf","tags":[{"name":"Linux","slug":"Linux","permalink":"https://cnkeep.github.io/tags/Linux/"}]},{"title":"01_Docker介绍与安装","date":"2018-07-03T15:10:00.000Z","path":"2018/07/03/01-01_Docker介绍与安装/","text":"Docker目录什么是DockerDocker用来干什么Docker相关的点安装Docker什么是Docker&emsp;&emsp;不同的应用程序可能会有不同的应用环境，如果把他们依赖的软件都安装在一个服务器上就要调试很久，而且很麻烦，还会造成一些冲突。这个时候你就要隔离，我们可以在服务器上创建不同的虚拟机在不同的虚拟机上放置不同的应用，但是虚拟机开销比较高。docker可以实现虚拟机隔离应用环境的功能，并且开销比虚拟机小。 除此之外，它还可以像jvm屏蔽操作系统的底层细节，让我们的应用在不同系统部署变得更加方便。 总结就是：Docker是一个便携的应用容器, 我们的程序可以运行在容器之中。 Docker用来干什么 更快的运行服务，更高效的利用机器资源，更多的服务发布 屏蔽不同系统之间的差异，可以做到处处可部署运行，避免多次搭建环境(再也不会出现开发环境好好地，线上跑不起来了) 容器化带来了安全隔离，不在因为一个服务挂，导致所有服务挂 Docker相关的点Kubernetes, jenkins 安装Docker 本文采用Centos7作为服务器安装Docker, 详情参考官方：https://docs.docker.com/install/linux/docker-ce/centos/. 卸载docker旧版本12345678910$ sudo yum remove docker \\ docker-client \\ docker-client-latest \\ docker-common \\ docker-latest \\ docker-latest-logrotate \\ docker-logrotate \\ docker-selinux \\ docker-engine-selinux \\ docker-engine 安装工具类123$ sudo yum install -y yum-utils \\ device-mapper-persistent-data \\ lvm2 配置docker仓库1234567891011$ sudo yum-config-manager \\ --add-repo \\ https://download.docker.com/linux/centos/docker-ce.repo 会报以下错误： Loaded plugins: fastestmirror adding repo from: https://download.docker.com/linux/centos/docker-ce.repo grabbing file https://download.docker.com/linux/centos/docker-ce.repo to /etc/yum.repos.d/docker-ce.repo Could not fetch/save url https://download.docker.com/linux/centos/docker-ce.repo to file /etc/yum.repos.d/docker-ce.repo : [Errno 14] curl$ 35 - &quot;TCP connection reset by peer 这是由于国内访问不到docker官方镜像的缘故可以通过aliyun的源来完成：1234567$ sudo yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo 出现以下内容则表示docker仓库配置成功： Loaded plugins: fastestmirror adding repo from: http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo grabbing file http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo to /etc/yum.repos.d/docker-ce.repo repo saved to /etc/yum.repos.d/docker-ce.repo 参考：Docker CE 镜像源站 安装社区版docker 1234567891011$ sudo yum install docker-ce 内容如下： Installed: docker-ce.x86_64 0:18.03.0.ce-1.el7.centos Dependency Installed: audit-libs-python.x86_64 0:2.7.6-3.el7 checkpolicy.x86_64 0:2.5-4.el7 container-selinux.noarch 2:2.42-1.gitad8f0f7.el7 libcgroup.x86_64 0 libtool-ltdl.x86_64 0:2.4.2-22.el7_3 pigz.x86_64 0:2.3.3-1.el7.centos policycoreutils-python.x86_64 0:2.5-17.1.el7 python-IPy.noarch Complete! 验证是否安装成功123456789101112启动docker：$ sudo systemctl start docker验证docker:$ sudo docker run hello-world则会出现以下异常：Unable to find image &apos;hello-world:latest&apos; locallylatest: Pulling from library/hello-world9bb5a5d4561a: Pulling fs layerdocker: error pulling image configuration: Get https://dseasb33srnrn.cloudfront.net/registry-v2/docker/registry/v2/blobs/sha256/e3/e38bc07ac18eSee &apos;docker run --help&apos;. 镜像加速 上面提到的错误也是网络问题：国内无法访问dockerhub, 我们配置一下加速地址，登录https://www.daocloud.io 注册账号,使用Docker 加速器或者注册阿里云开发者获取加速地址 1$ curl -sSL https://get.daocloud.io/daotools/set_mirror.sh | sh -s http://b481a1dd.m.daocloud.io 该脚本可以将 –registry-mirror 加入到你的 Docker 配置文件 /etc/docker/daemon.json 中（该文件不存在可手动建立）。 12345678910111213141516171819202122232425262728测试是否成功$ docker run hello-worldUnable to find image &apos;hello-world:latest&apos; locallylatest: Pulling from library/hello-worldd1725b59e92d: Pull complete Digest: sha256:523e382ab1801f2a616239b1052bb7ee5a7cce6a06cfed27ccb93680eacad6efStatus: Downloaded newer image for hello-world:latestHello from Docker!This message shows that your installation appears to be working correctly.To generate this message, Docker took the following steps: 1. The Docker client contacted the Docker daemon. 2. The Docker daemon pulled the &quot;hello-world&quot; image from the Docker Hub. (amd64) 3. The Docker daemon created a new container from that image which runs the executable that produces the output you are currently reading. 4. The Docker daemon streamed that output to the Docker client, which sent it to your terminal.To try something more ambitious, you can run an Ubuntu container with: $ docker run -it ubuntu bashShare images, automate workflows, and more with a free Docker ID: https://hub.docker.com/For more examples and ideas, visit: https://docs.docker.com/get-started/ 目录结构默认情况下Docker的存放位置为：/var/lib/docker可以通过docker info命令查看 创建容器时通过-v 命令可以指定容器与宿主机之间的目录映射关系 启动Docker服务 pull命令从远程仓库拉取指定镜像(Image)，或者通过Dockerfile构建(build)新的镜像 使用run或者create+start命令，利用镜像生成容器并启动，一个镜像可以生成多个容器 镜像可以通过save,load命令导入导出，完成镜像的共享转移","tags":[{"name":"Docker","slug":"Docker","permalink":"https://cnkeep.github.io/tags/Docker/"}]},{"title":"maven私服搭建","date":"2018-07-02T16:22:00.000Z","path":"2018/07/03/01-maven私服搭建/","text":"使用Docker搭建maven私服 标签：maven, nexus3 参考： Nexus安装、使用说明、问题总结 从Maven私服获取依赖 Maven私有仓库: 发布release版本报错： Maven私服:Docker安装nexus3 前言作为一个Java程序员不可避免的要使用到maven仓库，但是我们经常遇见这样的情形： 网络受限，无法下载远程仓库的jar 公司内部的jar，无法获取，只能手动安装 远程现在速度太慢 针对以上的问题，我们就需要自己搭建一个Maven私服仓库。 介绍私服是架设在局域网的一种特殊的远程仓库，目的是代理远程仓库及部署第三方构件。有了私服之后，当 Maven 需要下载构件时，直接请求私服，私服上存在则下载到本地仓库；否则，私服请求外部的远程仓库，将构件下载到私服，再提供给本地仓库下载。 安装配置Nexus为了方便操作，我们直接采用docker安装 安装12345678910111213141516$ mkdir /home/nexus3# 搜索镜像$ docker search nexus3#下载镜像$ docker pull sonatype/nexus3# 启动$ docker run -id \\ --name=nexus3 \\ --privileged=true \\ --restart=always \\ -p 8081:8081 \\ -v /home/nexus3:/var/nexus-data \\ sonatype/nexus3 等待几分钟后，访问http://{ip}:8081/ 默认用户名admin，密码admin123 配置 1.配置镜像地址12345678910111213141516171819202122232425262728293031323334353637383940 &lt;mirrors&gt; &lt;!-- 私服镜像 --&gt;&lt;mirror&gt; &lt;id&gt;nexus&lt;/id&gt; &lt;mirrorOf&gt;*&lt;/mirrorOf&gt; &lt;name&gt;Human Readable Name for this Mirror.&lt;/name&gt; &lt;url&gt;http://172.16.22.136:8081/nexus/content/groups/public/&lt;/url&gt; &lt;/mirror&gt; &lt;!-- 阿里云镜像 --&gt; &lt;mirror&gt; &lt;id&gt;alimaven&lt;/id&gt; &lt;name&gt;aliyun maven&lt;/name&gt; &lt;url&gt;http://maven.aliyun.com/nexus/content/groups/public/&lt;/url&gt; &lt;mirrorOf&gt;central&lt;/mirrorOf&gt; &lt;/mirror&gt; &lt;/mirrors&gt; &lt;profiles&gt; &lt;profile&gt; &lt;id&gt;dev&lt;/id&gt; &lt;repositories&gt; &lt;repository&gt; &lt;id&gt;nexus&lt;/id&gt; &lt;url&gt;http://172.16.22.136:8081/repository/maven-public/&lt;/url&gt; &lt;releases&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/releases&gt; &lt;snapshots&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/snapshots&gt; &lt;/repository&gt; &lt;/repositories&gt; &lt;/profile&gt; &lt;/profiles&gt; &lt;activeProfiles&gt; &lt;activeProfile&gt;dev&lt;/activeProfile&gt; &lt;/activeProfiles&gt; 2.配置用户名密码123456789101112&lt;servers&gt; &lt;server&gt; &lt;id&gt;maven-releases&lt;/id&gt; &lt;username&gt;admin&lt;/username&gt; &lt;password&gt;admin123&lt;/password&gt; &lt;/server&gt; &lt;server&gt; &lt;id&gt;maven-snapshots&lt;/id&gt; &lt;username&gt;admin&lt;/username&gt; &lt;password&gt;admin123&lt;/password&gt; &lt;/server&gt;&lt;/servers&gt; 3.配置项目pom.xml12345678910111213&lt;!--私服仓库--&gt;&lt;distributionManagement&gt; &lt;repository&gt; &lt;id&gt;maven-releases&lt;/id&gt; &lt;name&gt;Nexus Release Repository&lt;/name&gt; &lt;url&gt;http://172.16.22.136:8081/repository/maven-releases/&lt;/url&gt; &lt;/repository&gt; &lt;snapshotRepository&gt; &lt;id&gt;maven-snapshots&lt;/id&gt; &lt;name&gt;Nexus Snapshot Repository&lt;/name&gt; &lt;url&gt;http://172.16.22.136:8081/repository/maven-snapshots/&lt;/url&gt; &lt;/snapshotRepository&gt;&lt;/distributionManagement&gt; 4.发布1$ mvn deploy 登录到nexus上就可以看到刚才打包的jar, 如果遇到：Return code is: 400, ReasonPhrase: Repository does not allow upd ating assets: maven-releases. 这是因为重复发布导致的，需要设置：","tags":[{"name":"Docker","slug":"Docker","permalink":"https://cnkeep.github.io/tags/Docker/"}]},{"title":"cd命令","date":"2018-07-02T12:26:00.000Z","path":"2018/07/02/01-cd命令/","text":"cd命令1.命令功能 切换当前目录至指定目录下 2.命令格式1cd [dirpath] 3.示例 显示当前目录12$pwd/home/zll 切换目录到当前用户home目录下12345678$cd ~或者$cd ``` &gt; 切换目录到上一次目录下 ```text$cd - 切换到父目录下1$cd ..","tags":[{"name":"Linux","slug":"Linux","permalink":"https://cnkeep.github.io/tags/Linux/"}]},{"title":"01_设计模式","date":"2018-06-22T22:19:00.000Z","path":"2018/06/23/09-01_设计模式/","text":"Mybatis中的设计模式 目录 前言 正文 Builder模式 动态代理模式 静态代理模式 适配器模式 装饰器模式","tags":[{"name":"Mybatis","slug":"Mybatis","permalink":"https://cnkeep.github.io/tags/Mybatis/"}]},{"title":"01_事务","date":"2018-06-22T19:12:00.000Z","path":"2018/06/23/08-01_事务/","text":"Mybatis事务管理[目录] 事务的特性事务的隔离级别介绍 事务带来的问题通过事务隔离来解决问题事务原理JDBC事务实现Mybatis事务实现啰嗦一下 事务的特性 事务的四个特性（ACID）已经是老生常谈的话题，想必各位已经都熟记于心，这里就帮大家快速的回忆一下: 原子性(Atomicity): 体现在同一个事务的操作要么一起成功，要么集体失败。 一致性(Consistent): 表现在事务执行前后系统是整体稳定的，比如对于出入账操作不会存在总资金的变化。 隔离性(Isolation): 变现在各个事务之间是相互隔离的，不会相互影响。 持久性(Durability): 事务一旦执行成功了，对于数据库的修改是永久性的。 事务的隔离级别 在讲事务的隔离级别之前，先分析一下事务的引入在并发的情况下会遇到什么样的问题。 事务带来的问题脏读 考虑转账的情况： 账号A, 开启事务1，自己账户减500，通知B,我已经转账你了，你看我的账号都减了; 账号B, 开启事务2，进行了查询操作，发现账户A确实减了(此时事务1还未提交，已经发生了脏读), 自己账户增加500，提交事务; 账号A, 因为一些问题，回滚事务; 最终发现账号A钱数没减，账号B钱数增加了500，但是这500事实上不存在; 像上面这样的情况，事务2读取到事务1未提交的数据，就发生了脏读。 不可重复读 同样是上面转账的情景，此外银行还有一个策略（当银行账户钱数少于500时即冻结账户）： 账号A, 开启事务1，自己账户减500，通知B,我已经转账你了，你看我的账号都减了; 账号B, 开启事务2，进行了查询操作，发现账户A确实减了，同时A的账户到达了最低值，异步锁定了账户A并通知A解锁; 账号A, 因为一些问题，回滚事务1，账号B一看A不转了，那我也不加了，回滚事务2; 最终发现账号A钱数没有小于500，但是A很奇怪，我的账户咋不能用了,投诉就来了; 像上面的情况，在一个事务里,因为另一个事务修改操作，导致同一个事务前后读取的数据不一致，就造成了不可重复读的问题。 幻读 这次不是转账了，是银行搞活动，存款大于1000的送100(想得美)： 事务1，查看当前存款大于1000的用户，给这些账号送钱; 事务2，存款1500，提交事务; 事务1，看看送钱的结果，我擦，咋有个用户没有送钱; 像上面的情况，在一个事务里，因为另一个事务删除或者插入的操作，导致同一个事务里的数据前后不一致，就造成了幻读的问题。 丢失更新 除了上面提到的这些问题，还有一种问题会出现，那就是丢失更新的问题： 皮一下 当你明白舍生取义，就会回来和我一起唱这首歌了,” 当当当。。。”（跑题了） 通过事务隔离来解决问题 既然事务带来这么多的问题，但是我们还要用事务，有没有什么好的办法去解决呢？ 答案是有的，这就要引入我们事务隔离机制了。根据处理的不同，事务的隔离级别分为四种： 隔离级别 特点 none read_uncommitted 读未提交：脏读、不可重复读、幻读均不能避免 read_committed 读已提交：避免脏读；不可重复读，幻读不能避免 repeatable_read 可重复读：避免脏读、不可重复读；幻读不能避免 serializable 序列化：避免脏读、不可重复读、幻读；不能解决丢失更新的问题 mysql默认的事务隔离级别是repeatable_read 事务原理JDBC事务实现 我们先来看看jdbc方式如何实现事务操作 1234567891011121314151617181920212223242526272829303132333435363738394041424344import java.sql.*;/*** jdk&gt;=1.7 */public class JdbcTransactionTest&#123; private static String driverClass = \"com.mysql.jdbc.Driver\"; private static String url = \"jdbc:mysql://localhost:3306/zhiyun_os?useUnicode=true&amp;characterEncoding=UTF-8&amp;useSSL=true\"; private static String userName = \"root\"; private static String password = \"123456\"; static &#123; Class.forName(driverClass); &#125; public static Connection getConnection() throws SQLException&#123; return DriverManager.getConnection(url,userName.password); &#125; public static void transactionTest()&#123; Connection connection = getConnection(); //需要关闭自动提交，否则数据库会为每一条sql建立一个新的事务 connection.setAutoCommit(false); try&#123; //update operation //insert operation //select operation //事务提交 connection.commit(); &#125;catch (Exception e)&#123; //异常，事务回滚 if(null!=connection)&#123; connection.rollback(); &#125; &#125;finally&#123; close(connection); &#125; &#125; public static void close(AutoCloseable closeable)&#123; if(null!=closeable)&#123; closeable.close(); &#125; &#125;&#125; 我们看到，事务是依托Connection操作完成的，那我们大胆的猜想一下，Mybatis肯定也是通过某种手段来按照标准jdbc的方式来操作事务，只是隐藏起来了，这里我们似乎闻到了一丝原理的气息，嘿嘿嘿~ Mybatis事务实现 先看下mybatis是如何操作事务的： mybati-config.xml123456789101112131415161718&lt;?xml version=\"1.0\" encoding=\"UTF-8\" ?&gt;&lt;!DOCTYPE configuration PUBLIC \"-//mybatis.org//DTD Config 3.0//EN\" \"http://mybatis.org/dtd/mybatis-3-config.dtd\"&gt;&lt;configuration&gt; &lt;environments default=\"development\"&gt; &lt;environment id=\"development\"&gt; &lt;transactionManager type=\"JDBC\" /&gt; &lt;dataSource type=\"POOLED\"&gt; &lt;property name=\"driver\" value=\"com.mysql.jdbc.Driver\" /&gt; &lt;property name=\"url\" value=\"jdbc:mysql://localhost:3306/test\" /&gt; &lt;property name=\"username\" value=\"root\" /&gt; &lt;property name=\"password\" value=\"123456\" /&gt; &lt;/dataSource&gt; &lt;/environment&gt; &lt;/environments&gt; &lt;mappers&gt; &lt;mapper resource=\"mapper/UserMapper.xml\" /&gt; &lt;/mappers&gt;&lt;/configuration&gt; 1234567891011121314151617181920public class MybatisTransactionTest&#123; public static void main(String[] args) throws IOException &#123; String resource = \"mybatis-config.xml\"; InputStream inputStream = Resources.getResourceAsStream(resource); SqlSessionFactory sqlSessionFactory = new SqlSessionFactoryBuilder().build(inputStream); try(SqlSession openSession = sqlSessionFactory.openSession())&#123; /** * openSession.insert(...); * openSession.update(...); * openSession.select(...); */ openSession.commit(); &#125;catch (Exception e)&#123; openSession.rollBack(); &#125; &#125;&#125; 回顾一下我们之前讲的，SqlSession是mybatis暴露给我们的外部接口，代表着一个数据库连接，一次回话，那我们可以简单的理解为对Connection的抽象(实际不是),我们看看mybatis是如何完成事务的，从openSession()开始：12345678910111213141516171819//DefalutSqlSessionFactoryprivate SqlSession openSessionFromDataSource(ExecutorType execType, TransactionIsolationLevel level, boolean autoCommit) &#123; Transaction tx = null; try &#123; final Environment environment = configuration.getEnvironment(); //获取事务工厂类 final TransactionFactory transactionFactory = getTransactionFactoryFromEnvironment(environment); //创建事务 tx = transactionFactory.newTransaction(environment.getDataSource(), level, autoCommit); //事务注入到Executor,我们知道Executor是真正的执行者 final Executor executor = configuration.newExecutor(tx, execType); return new DefaultSqlSession(configuration, executor, autoCommit); &#125; catch (Exception e) &#123; closeTransaction(tx); // may have fetched a connection so lets call close() throw ExceptionFactory.wrapException(\"Error opening session. Cause: \" + e, e); &#125; finally &#123; ErrorContext.instance().reset(); &#125; &#125; 我们发现在创建一个SqlSession会话时，就已经开启了一个事务，我们来看看openSession.commit()和rollBack()方法，直接去BaseExecutor中查看：1234567891011121314151617181920212223242526272829 @Overridepublic void commit(boolean required) throws SQLException &#123; if (closed) &#123; throw new ExecutorException(\"Cannot commit, transaction is already closed\"); &#125; //清理缓存 clearLocalCache(); flushStatements(); if (required) &#123; //事务提交 transaction.commit(); &#125;&#125;@Overridepublic void rollback(boolean required) throws SQLException &#123; if (!closed) &#123; try &#123; clearLocalCache(); flushStatements(true); &#125; finally &#123; if (required) &#123; //事务提交 transaction.rollback(); &#125; &#125; &#125;&#125; 于是我们想，事务又是如何提交和回滚的，我们想到了jdbc是通过Connection来操作的，其实Mybatis也是如此，我们来看实现类JdbcTransaction123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108public class JdbcTransaction implements Transaction &#123; protected Connection connection; protected DataSource dataSource; protected TransactionIsolationLevel level; protected boolean autoCommmit; public JdbcTransaction(DataSource ds, TransactionIsolationLevel desiredLevel, boolean desiredAutoCommit) &#123; dataSource = ds; level = desiredLevel; autoCommmit = desiredAutoCommit; &#125; @Override public Connection getConnection() throws SQLException &#123; if (connection == null) &#123; openConnection(); &#125; return connection; &#125; @Override public void commit() throws SQLException &#123; if (connection != null &amp;&amp; !connection.getAutoCommit()) &#123; if (log.isDebugEnabled()) &#123; log.debug(\"Committing JDBC Connection [\" + connection + \"]\"); &#125; connection.commit(); &#125; &#125; @Override public void rollback() throws SQLException &#123; if (connection != null &amp;&amp; !connection.getAutoCommit()) &#123; if (log.isDebugEnabled()) &#123; log.debug(\"Rolling back JDBC Connection [\" + connection + \"]\"); &#125; connection.rollback(); &#125; &#125; @Override public void close() throws SQLException &#123; if (connection != null) &#123; resetAutoCommit(); if (log.isDebugEnabled()) &#123; log.debug(\"Closing JDBC Connection [\" + connection + \"]\"); &#125; connection.close(); &#125; &#125; protected void setDesiredAutoCommit(boolean desiredAutoCommit) &#123; try &#123; if (connection.getAutoCommit() != desiredAutoCommit) &#123; if (log.isDebugEnabled()) &#123; log.debug(\"Setting autocommit to \" + desiredAutoCommit + \" on JDBC Connection [\" + connection + \"]\"); &#125; connection.setAutoCommit(desiredAutoCommit); &#125; &#125; catch (SQLException e) &#123; // Only a very poorly implemented driver would fail here, // and there's not much we can do about that. throw new TransactionException(\"Error configuring AutoCommit. \" + \"Your driver may not support getAutoCommit() or setAutoCommit(). \" + \"Requested setting: \" + desiredAutoCommit + \". Cause: \" + e, e); &#125; &#125; protected void resetAutoCommit() &#123; try &#123; if (!connection.getAutoCommit()) &#123; // MyBatis does not call commit/rollback on a connection if just selects were performed. // Some databases start transactions with select statements // and they mandate a commit/rollback before closing the connection. // A workaround is setting the autocommit to true before closing the connection. // Sybase throws an exception here. if (log.isDebugEnabled()) &#123; log.debug(\"Resetting autocommit to true on JDBC Connection [\" + connection + \"]\"); &#125; connection.setAutoCommit(true); &#125; &#125; catch (SQLException e) &#123; if (log.isDebugEnabled()) &#123; log.debug(\"Error resetting autocommit to true \" + \"before closing the connection. Cause: \" + e); &#125; &#125; &#125; protected void openConnection() throws SQLException &#123; if (log.isDebugEnabled()) &#123; log.debug(\"Opening JDBC Connection\"); &#125; connection = dataSource.getConnection(); if (level != null) &#123; connection.setTransactionIsolation(level.getLevel()); &#125; setDesiredAutoCommit(autoCommmit); &#125; @Override public Integer getTimeout() throws SQLException &#123; return null; &#125; &#125; 可以看到事务的操作最终还是回归到最原始的JDBC操作了，看到这里我想大家应该可以总结出来一些内容了吧：我们开启一次回话默认会开启一个新的事务，回话的提交和回滚最后还是通过事务的回滚和提交来操作的。那问题就归结到事务Transaction对象怎么通过TransactionFactory创建的。 我们来看看TransactionFactory的类图： 其中：ManagedTransactionFactory表示将事务委托给第三发服务去处理，比如JBoss，其生成的ManagedTransacton内部也是什么都不做。JdbcTransactionFactory表示通过jdbc的事务机制来实现事务，这也是我们常用的实现方式，当然与spring集成时使用的是SpringTransactionFactory其实也是通过jdbc实现的变种，这里不做介绍。至于使用JDBC还是Manager我们可以在mybatis-config.xml中配置： 总结一下 自此我们对Mybatis的事务就了解的差不多了，总结一下： Mybatis自身不处理事务，而是将提供事务接口TransactionFactory，由外界注入处理处理事务，只是其默认提供了JDBC和Managed两种模式。 新建一个回话就会开启一个事务，回话的提交和回滚最终都是由事务的提交和回滚完成的。 啰嗦一下 说道Mybatis事务不得不说Spring事务管理，简单说Spring集成Mybatis管理事务时，内部使用的还是标准JDBC去完成事务的操作，只是spring通过AOP在我们执行方法前后自动的完成了事务的操作。","tags":[{"name":"Mybatis","slug":"Mybatis","permalink":"https://cnkeep.github.io/tags/Mybatis/"}]},{"title":"01_应用实践-补充","date":"2018-06-20T16:05:00.000Z","path":"2018/06/21/07-01_应用实践-补充/","text":"应用实践-补充 目录 主键生成 批量插入时的主键获取","tags":[{"name":"Mybatis","slug":"Mybatis","permalink":"https://cnkeep.github.io/tags/Mybatis/"}]},{"title":"03_拓展-自定义类型转换器","date":"2018-06-20T12:01:00.000Z","path":"2018/06/20/06-03_拓展-自定义类型转换器/","text":"类型转换器-TypeHandler[目录] 前言自定义一个类型转化器 前言 我们知道Java有Java的数据类型，数据库也有数据库相应的数据类型，两者虽然不同，但是存在着某种对应关系。那Mybatis在插入插入数据时又是如何把Java数据类型转化为数据库对应的数据类型，查询时又是如何把数据库数据类型转化为Java数据类型返回给我们的呢? 这中间必然需要经过一个类型转化，Mybatis中提供了TypeHandler来完成类型的转化工作。 自定义一个类型转化器 在应用中我们经常会使用枚举类型来标示不同的状态，但在数据库存储时存储的是int数值，如果不采用自定义类型转化器，我们就需要在插入数据和查询数据时自己编写转化关系，这种方式虽然可以实现功能，但是太过低效，而且而不便于维护，接下来我们就利用Mybatis的自定义类型转换器来实现一个通用的枚举类型转化器。 实现TypeHandler接口 在Mybatis中要实现自己的TypeHandler就需要实现TypeHandler接口中定义的四个方法：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115 public interface TypeHandler&lt;T&gt; &#123; /** * 用于定义在Mybatis设置参数时该如何把Java类型的参数转换为对应的数据库类型 * @param ps 当前的PreparedStatement对象 * @param i 当前参数的位置 * @param parameter 当前参数的Java对象 * @param jdbcType 当前参数的数据库类型 * @throws SQLException */ void setParameter(PreparedStatement ps, int i, T parameter, JdbcType jdbcType) throws SQLException; /** * 用于在Mybatis获取数据结果集时如何把数据库类型转换为对应的Java类型 * @param rs 当前的结果集 * @param columnName 当前的字段名称 * @return 转换后的Java对象 * @throws SQLException */ T getResult(ResultSet rs, String columnName) throws SQLException; /** * 用于在Mybatis通过字段位置获取字段数据时把数据库类型转换为对应的Java类型 * @param rs 当前的结果集 * @param columnIndex 当前字段的位置 * @return 转换后的Java对象 * @throws SQLException */ T getResult(ResultSet rs, int columnIndex) throws SQLException; /** * 用于Mybatis在调用存储过程后把数据库类型的数据转换为对应的Java类型 * @param cs 当前的CallableStatement执行后的CallableStatement * @param columnIndex 当前输出参数的位置 * @return * @throws SQLException */ T getResult(CallableStatement cs, int columnIndex) throws SQLException; &#125;``` 我们可以选择实现该接口，但是直接实现该接口需要处理一些为空的情况，为了减少实现的复杂度，mybatis建议我们继承``BaseTypeHandler``类，实现其中的抽象方法即可： ```java /*** * 自定义枚举接口，用于返回枚举类型对应的数值类型，所有枚举类型需实现该接口 */ public interface HasIndexEnum &#123; int getIndex(); &#125; public class EnumTypeHandler&lt;T extends HasIndexEnum&gt; extends BaseTypeHandler&lt;T&gt; &#123; //存储枚举类型与数值类型的对应关系 private Map&lt;Integer, T&gt; enumCache = new HashMap&lt;&gt;(); /** * mybatis会传入Class生成TypeHandler */ public EnumTypeHandler(Class&lt;T&gt; clazz) &#123; if (!Enum.class.isAssignableFrom(clazz) &amp;&amp; HasIndexEnum.class.isAssignableFrom(clazz)) &#123; throw new UnsupportedOperationException(\"Class shound be enum and implements HasIndexEnum.class!\"); &#125; T[] constants = clazz.getEnumConstants(); for (T e : constants) &#123; enumCache.put(e.getIndex(), e); &#125; &#125; @Override public void setNonNullParameter(PreparedStatement ps, int i, T parameter, JdbcType jdbcType) throws SQLException &#123; ps.setInt(i, parameter.getIndex()); &#125; @Override public T getNullableResult(ResultSet rs, String columnName) throws SQLException &#123; Object value = rs.getObject(columnName); if (rs.wasNull()) &#123; return null; &#125; return convertOrException(value); &#125; @Override public T getNullableResult(ResultSet rs, int columnIndex) throws SQLException &#123; Object value = rs.getObject(columnIndex); if (rs.wasNull()) &#123; return null; &#125; return convertOrException(value); &#125; @Override public T getNullableResult(CallableStatement cs, int columnIndex) throws SQLException &#123; Object value = cs.getObject(columnIndex); if (cs.wasNull()) &#123; return null; &#125; return convertOrException(value); &#125; private T convertOrException(Object value) &#123; T e = this.enumCache.get((Integer) value); if (null == e) &#123; throw new IllegalArgumentException(\"Cannot convert \" + value); &#125; else &#123; return e; &#125; &#125; &#125;``` 这样一个通用的枚举类型的类型转换器就完成了，只要枚举类型实现了HasIndexEnum接口就可以使用该转换器处理。 ### 注册TypeHandler&gt; 在mapper-config中，注册你实现的转换器类，其中jdbcType可以指定的类型在Mybatis的枚举类org.apache.ibatis.type.JdbcType中有明确的定义，不能为该枚举以外的值，不然会出错。这里因为枚举中没有我们需要的XMLType类型，所以指定为UNDEFINED。（也可以不指定具体的类型，在使用时用typeHandler指定具体的类即可）： &lt;typeHandlers&gt; &lt;typeHandler handler=&quot;com.xxxx.handler.EnumTypeHandler&quot; jdbcType=&quot;INTEGER&quot; javaType=&quot;com.xxx.enums.XXXType&quot; /&gt; &lt;/typeHandlers&gt; ` 这样一个通用的枚举类型转化器就完成了，就可以帮助我们自动的完成类型转化工作了。","tags":[{"name":"Mybatis","slug":"Mybatis","permalink":"https://cnkeep.github.io/tags/Mybatis/"}]},{"title":"02_拓展-插件(Plugin)","date":"2018-06-20T10:25:00.000Z","path":"2018/06/20/06-02_拓展-插件(Plugin)/","text":"Mybatis插件(Plugin)目录 前言 前言 Mybatis利用接口将各个层进行分离的同时也提供了手段给予编程者去修改mybatis的默认行为，最经典的要属其插件的使用，本节我们就来研究一下。 介绍 Mybatis允许用户使用自定义拦截器对sql语句执行过程中的某一点进行拦截，我们可以拦截四大接口(Executor,StatementHandler,ParameterHandler,ResultSetHandler)的方法来实现某些功能。1.使用拦截器需要实现Interceptor接口,Interceptor是实现Plugin的核心，其定义如下：12345678910111213141516171819202122232425262728293031323334353637383940414243444546 public interface Interceptor &#123; //执行拦截逻辑的方法 Object intercept(Invocation var1) throws Throwable; //决定是否触发interceptor()方法 Object plugin(Object var1); //根据配置初始化Interceptor对象 void setProperties(Properties var1); &#125;``` 除了实现Interceptor接口外，还需要在注解中指明拦截的类和方法： ```java @Intercepts(&#123;@Signature(type = Executor.class, method = \"query\", args = &#123; MappedStatement.class, Object.class, RowBounds.class, ResultHandler.class &#125;)&#125;) public class TableExecutorInterceptor implements Interceptor &#123; private Logger logger = LoggerFactory.getLogger(TableExecutorInterceptor.class); /*** * 拦截后的操作处理 */ @Override public Object intercept(Invocation invocation) throws Throwable &#123; Object result = invocation.proceed(); logger.info(\"result:&#123;&#125;\",result); return result; &#125; @Override public Object plugin(Object target) &#123; if (target instanceof Executor) &#123; //返回代理对象 return Plugin.wrap(target, this); &#125; else &#123; return target; &#125; &#125; @Override public void setProperties(Properties properties) &#123; //初始化配置的参数 &#125; &#125; 针对注解格式做一下说明:123456789@Interceptor( &#123; @Signature( type=,//指明拦截的类的名称 method=,//拦截的方法名 args=&#123;&#125;//拦截的方法的参数列表，通过方法名+参数列表可以匹配指定的方法 ) &#125;) 2.配置文件中声明拦截器1234567891011121314151617181920212223242526&lt;?xml version=\"1.0\" encoding=\"UTF-8\" ?&gt;&lt;!DOCTYPE configuration PUBLIC \"-//mybatis.org//DTD Config 3.0//EN\" \"http://mybatis.org/dtd/mybatis-3-config.dtd\"&gt;&lt;configuration&gt; &lt;!-- &lt;environments default=\"development\"&gt; &lt;environment id=\"development\"&gt; &lt;transactionManager type=\"JDBC\" /&gt; &lt;dataSource type=\"POOLED\"&gt; &lt;property name=\"driver\" value=\"com.mysql.jdbc.Driver\" /&gt; &lt;property name=\"url\" value=\"jdbc:mysql://localhost:3306/test\" /&gt; &lt;property name=\"username\" value=\"root\" /&gt; &lt;property name=\"password\" value=\"123456\" /&gt; &lt;/dataSource&gt; &lt;/environment&gt; &lt;/environments&gt; --&gt; &lt;settings&gt; &lt;!-- 打印查询语句--&gt; &lt;setting name=\"logImpl\" value=\"STDOUT_LOGGING\" /&gt; &lt;/settings&gt; &lt;plugins&gt; &lt;plugin interceptor=\"test.mybatis.plugins.TablePrepareInterceptor\"&gt;&lt;/plugin&gt; &lt;/plugins&gt; &lt;mappers&gt; ...... &lt;/mappers&gt;&lt;/configuration&gt; 到此自定义的拦截器就配置好了，在Mybatis初始化时，会通过XMLConfigBuilder.pluginElement()方法解析mybatis-config.xml配置文件中定义的节点，得到相应的Interceptor对象以及配置的相应属性，之后会调用Interceptor.setProperties(properties)方法完成对Interceptor对象初始化配置，最后将Interceptor对象添加到Configuration.interceptorChain字段中保存。 拦截原理 Mybatis中有一个很重要的类-Configuration，该配置类为我们提供各种接口实例的获取，其中就包括四大接口，这四种接口对象都是通过newXXXX()方法获得的，如果配置了拦截器，就会在生成这些对象后，通过interceptorChain.pluginAll(…)来生成代理对象，最终返回的都是代理对象。例如创建Executor对象:123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051 public Executor newExecutor(Transaction transaction, ExecutorType executorType) &#123; executorType = executorType == null ? defaultExecutorType : executorType; executorType = executorType == null ? ExecutorType.SIMPLE : executorType; Executor executor; // 根据参数，选择合适的Executor if (ExecutorType.BATCH == executorType) &#123; executor = new BatchExecutor(this, transaction); &#125; else if (ExecutorType.REUSE == executorType) &#123; executor = new ReuseExecutor(this, transaction); &#125; else &#123; executor = new SimpleExecutor(this, transaction); &#125; // 根据配置决定是否开启二级缓存 if (cacheEnabled) &#123; executor = new CachingExecutor(executor); &#125; //通过InterceptorChain.pluginAll()方法创建Executor的代理对象 executor = (Executor) interceptorChain.pluginAll(executor); return executor; &#125;``` InterceptorChain中使用interceptors字段(ArrayList&lt;Interceptor&gt;类型)记录了mybatis-config.xml文件中配置的拦截器。在InterceptorChain.pluginAll()方法中会遍历该interceptors集合，并调用其中的每个元素的plugin()方法创建代理对象，创建代理对象的 代理对象，类似于洋葱一样，![](images/plugins.png) 具体的实现如下所示。```java public class InterceptorChain &#123; //保存配置的拦截器 private final List&lt;Interceptor&gt; interceptors = new ArrayList&lt;Interceptor&gt;(); //依次调用plugin生成代理对象 public Object pluginAll(Object target) &#123; for (Interceptor interceptor : interceptors) &#123; target = interceptor.plugin(target); &#125; return target; &#125; public void addInterceptor(Interceptor interceptor) &#123; interceptors.add(interceptor); &#125; public List&lt;Interceptor&gt; getInterceptors() &#123; return Collections.unmodifiableList(interceptors); &#125; &#125; 不当的使用Interceptor的plugin方法生成代理对象是危险的，因为这会改变Mybatis原有的执行流程，很有可能会导致程序无法运行，为此mybatis为我们提供了Plugin类的wrap方法来实现代理的生成，减少出错的可能，因此我们在重写plugin方法时推荐使用Plugin类。12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485public class Plugin implements InvocationHandler &#123; private Object target;//目标对象 private Interceptor interceptor;//拦截器 private Map&lt;Class&lt;?&gt;, Set&lt;Method&gt;&gt; signatureMap; private Plugin(Object target, Interceptor interceptor, Map&lt;Class&lt;?&gt;, Set&lt;Method&gt;&gt; signatureMap) &#123; this.target = target; this.interceptor = interceptor; this.signatureMap = signatureMap; &#125; public static Object wrap(Object target, Interceptor interceptor) &#123; //获取用户自定义Interceptor中@Signature注解的信息，getSignatureMap()方法负责处理@Signture注解 Map&lt;Class&lt;?&gt;, Set&lt;Method&gt;&gt; signatureMap = getSignatureMap(interceptor); Class&lt;?&gt; type = target.getClass(); ////获取目标类型实现的接口，正如前文所述，拦截器可以拦截4类对象都实现了相应的接口，这也是能使用JDK动态代理的方式创建对象的基础 Class&lt;?&gt;[] interfaces = getAllInterfaces(type, signatureMap); if (interfaces.length &gt; 0) &#123; //使用JDK动态代理的方式创建代理对象 return Proxy.newProxyInstance( type.getClassLoader(), interfaces, new Plugin(target, interceptor, signatureMap)); &#125; return target; &#125; //拦截方法入口 @Override public Object invoke(Object proxy, Method method, Object[] args) throws Throwable &#123; try &#123; //获取方法签名 Set&lt;Method&gt; methods = signatureMap.get(method.getDeclaringClass()); //判断是否需要拦截 if (methods != null &amp;&amp; methods.contains(method)) &#123; //需要拦截，则执行相应的拦截方法 return interceptor.intercept(new Invocation(target, method, args)); &#125; //不需要拦截，则执行原有的方法 return method.invoke(target, args); &#125; catch (Exception e) &#123; throw ExceptionUtil.unwrapThrowable(e); &#125; &#125; private static Map&lt;Class&lt;?&gt;, Set&lt;Method&gt;&gt; getSignatureMap(Interceptor interceptor) &#123; Intercepts interceptsAnnotation = interceptor.getClass().getAnnotation(Intercepts.class); // issue #251 if (interceptsAnnotation == null) &#123; throw new PluginException(\"No @Intercepts annotation was found in interceptor \" + interceptor.getClass().getName()); &#125; Signature[] sigs = interceptsAnnotation.value(); Map&lt;Class&lt;?&gt;, Set&lt;Method&gt;&gt; signatureMap = new HashMap&lt;Class&lt;?&gt;, Set&lt;Method&gt;&gt;(); for (Signature sig : sigs) &#123; Set&lt;Method&gt; methods = signatureMap.get(sig.type()); if (methods == null) &#123; methods = new HashSet&lt;Method&gt;(); signatureMap.put(sig.type(), methods); &#125; try &#123; Method method = sig.type().getMethod(sig.method(), sig.args()); methods.add(method); &#125; catch (NoSuchMethodException e) &#123; throw new PluginException(\"Could not find method on \" + sig.type() + \" named \" + sig.method() + \". Cause: \" + e, e); &#125; &#125; return signatureMap; &#125; private static Class&lt;?&gt;[] getAllInterfaces(Class&lt;?&gt; type, Map&lt;Class&lt;?&gt;, Set&lt;Method&gt;&gt; signatureMap) &#123; Set&lt;Class&lt;?&gt;&gt; interfaces = new HashSet&lt;Class&lt;?&gt;&gt;(); while (type != null) &#123; for (Class&lt;?&gt; c : type.getInterfaces()) &#123; if (signatureMap.containsKey(c)) &#123; interfaces.add(c); &#125; &#125; type = type.getSuperclass(); &#125; return interfaces.toArray(new Class&lt;?&gt;[interfaces.size()]); &#125;&#125; 最终方法会进入我们自定义的拦截器interceptor方法中，在该方法中实现自定义的内容，从而达到拦截的目的。 总结 总结一下拦截器的处理流程： SqlSession执行相应方法时，通过Configuration类获取对象的四大接口对象 Configuration根据config.xml中配置的plugin生成代理对象返回 当调用代理对象的方法时，进入Plugin对象的invoke方法，根据方法签名判断是否需要拦截该方法，不需要拦截则继续执行原方法；需要拦截的则进入自定义拦截器的intercept方法 继续执行","tags":[{"name":"Mybatis","slug":"Mybatis","permalink":"https://cnkeep.github.io/tags/Mybatis/"}]},{"title":"01_拓展-缓存(二级缓存)","date":"2018-06-18T09:59:00.000Z","path":"2018/06/18/06-01_拓展-缓存(二级缓存)/","text":"拓展-缓存(二级缓存) 目录 前言 正文 使用二级缓存 二级缓存原理分析 二级缓存的生命周期 总结 前言 上节我们针对一级缓存的原理做了探究，本节我们就一起来探究一下二级缓存。 二级缓存使用二级缓存 在mybatis配置文件分析中介绍过缓存的配置1.mapper-config.xml配置开启二级缓存1234567891011121314151617 &lt;?xml version=\"1.0\" encoding=\"UTF-8\" ?&gt; &lt;!DOCTYPE configuration PUBLIC \"-//mybatis.org//DTD Config 3.0//EN\" \"http://mybatis.org/dtd/mybatis-3-config.dtd\"&gt; &lt;configuration&gt; &lt;settings&gt; &lt;!-- 打印查询语句--&gt; &lt;setting name=\"logImpl\" value=\"STDOUT_LOGGING\" /&gt; &lt;!-- 二级缓存,默认true --&gt; &lt;setting name=\"cacheEnabled\" value=\"true\"/&gt; &lt;/settings&gt; &lt;mappers&gt; &lt;mapper resource=\"mapper/UserMapper.xml\" /&gt; &lt;/mappers&gt; &lt;/configuration&gt; ``` &gt; 2.在Mybatis的映射XML中配置cache或者 cache-ref 。 &lt;cache/&gt; 或 &lt;cache-ref namespace=&quot;mapper.UserMapper&quot;/&gt; 12345678910cache标签用于声明这个namespace使用二级缓存，并且可以自定义配置。 * type: cache使用的类型，默认是PerpetualCache，这在一级缓存中提到过。 * eviction: 定义回收的策略，常见的有FIFO，LRU。 * flushInterval: 配置一定时间自动刷新缓存，单位是毫秒 * size: 最多缓存对象的个数 * readOnly: 是否只读，若配置可读写，则需要对应的实体类能够序列化。 * blocking: 若缓存中找不到对应的key，是否会一直blocking，直到有对应的数据进入缓存。 &gt; 3.测试使用 @Test public void mapperTest() { SqlSessionFactory sqlSessionFactory = application.getBean(SqlSessionFactory.class); SqlSession sqlSession = sqlSessionFactory.openSession(); SqlSession sqlSession1 = sqlSessionFactory.openSession(); try { UserMapper userMapper = sqlSession.getMapper(UserMapper.class); UserMapper userMapper1 = sqlSession1.getMapper(UserMapper.class); User user = userMapper.getById(&quot;4f98966b-f7a9-401c-b327-aa0e401d47b4&quot;); //sqlSession.commit(); User user1 = userMapper1.getById(&quot;4f98966b-f7a9-401c-b327-aa0e401d47b4&quot;); } catch (Exception e) { e.printStackTrace(); } finally { sqlSession.close(); } } 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244执行发现报错： ![二级缓存需要实体序列化](images/two_level_cache_exception.png) 发现是因为二级缓存实体需要实现序列化，实现序列化后发现缓存没有生效，神奇了 ![二级缓存失效的错误](images/two_level_cache_invalid.png) 最后发现是因为第一个事务没有``commit``导致的，修改代码先提交事务1，再查询事务2，发现二级缓存生效，至于为什么要commit，先留个悬念，后面分析原理再说明 ### 二级缓存原理分析 Mybatis二级缓存的工作流程和前文提到的一级缓存类似，只是在一级缓存处理前，用CachingExecutor装饰了BaseExecutor的子类，实现了缓存的查询和写入功能。 ![CachingExecutor装饰Executor](images/create_cachingExecutor.png) &gt; 来看``CachingExecutor``类，这是一个装饰者模式的应用 ```java public class CachingExecutor implements Executor &#123; private Executor delegate;//装饰的对象 //管理缓存 private TransactionalCacheManager tcm = new TransactionalCacheManager(); /*** * 部分方法代码已经省略，这里只关注重要的方法.... */ public CachingExecutor(Executor delegate) &#123; this.delegate = delegate; delegate.setExecutorWrapper(this); &#125; @Override public void close(boolean forceRollback) &#123; try &#123; //是否回滚缓存 if (forceRollback) &#123; tcm.rollback(); &#125; else &#123; tcm.commit(); &#125; &#125; finally &#123; delegate.close(forceRollback); &#125; &#125; @Override public int update(MappedStatement ms, Object parameterObject) throws SQLException &#123; // 清除二级缓存 flushCacheIfRequired(ms); return delegate.update(ms, parameterObject); &#125; @Override public &lt;E&gt; List&lt;E&gt; query(MappedStatement ms, Object parameterObject, RowBounds rowBounds, ResultHandler resultHandler) throws SQLException &#123; BoundSql boundSql = ms.getBoundSql(parameterObject); CacheKey key = createCacheKey(ms, parameterObject, rowBounds, boundSql); return query(ms, parameterObject, rowBounds, resultHandler, key, boundSql); &#125; @Override public &lt;E&gt; List&lt;E&gt; query(MappedStatement ms, Object parameterObject, RowBounds rowBounds, ResultHandler resultHandler, CacheKey key, BoundSql boundSql) throws SQLException &#123; Cache cache = ms.getCache(); if (cache != null) &#123; /*根据Mapper.xml中sql节点配置的flushCache属性(select默认为false)判断是否需要清空缓存*/ flushCacheIfRequired(ms); /*根据Mapper.xml中sql节点配置的useCache属性(select默认为true)判断是否需要缓存查询结果*/ if (ms.isUseCache() &amp;&amp; resultHandler == null) &#123; ensureNoOutParams(ms, parameterObject, boundSql); //查询缓存 List&lt;E&gt; list = (List&lt;E&gt;) tcm.getObject(cache, key); if (list == null) &#123; list = delegate.&lt;E&gt; query(ms, parameterObject, rowBounds, resultHandler, key, boundSql); tcm.putObject(cache, key, list); // issue #578 and #116 &#125; return list; &#125; &#125; return delegate.&lt;E&gt; query(ms, parameterObject, rowBounds, resultHandler, key, boundSql); &#125; @Override public void commit(boolean required) throws SQLException &#123; /*提交事务*/ delegate.commit(required); /*保存缓存，这就是为什么多个session必须提交之后缓存才生效的原因*/ tcm.commit(); &#125; @Override public void rollback(boolean required) throws SQLException &#123; /*事务和缓存同时回滚*/ try &#123; delegate.rollback(required); &#125; finally &#123; if (required) &#123; tcm.rollback(); &#125; &#125; &#125; @Override public CacheKey createCacheKey(MappedStatement ms, Object parameterObject, RowBounds rowBounds, BoundSql boundSql) &#123; return delegate.createCacheKey(ms, parameterObject, rowBounds, boundSql); &#125; @Override public void clearLocalCache() &#123; /*清除一级缓存 */ delegate.clearLocalCache(); &#125; private void flushCacheIfRequired(MappedStatement ms) &#123; Cache cache = ms.getCache(); /*select默认为flushCache=false; update,delete为false. 即修改操作会清除缓存*/ if (cache != null &amp;&amp; ms.isFlushCacheRequired()) &#123; tcm.clear(cache); &#125; &#125; &#125;``` 通过对原有的Executor执行器做装饰来实现缓存的功能 &gt; 我们重点来看``TransactionalCacheManager``类 ```java public class TransactionalCacheManager &#123; //利用map来临时存储缓存 private Map&lt;Cache, TransactionalCache&gt; transactionalCaches = new HashMap&lt;Cache, TransactionalCache&gt;(); /*** * 这里只给出部分源代码.... */ public void putObject(Cache cache, CacheKey key, Object value) &#123; getTransactionalCache(cache).putObject(key, value); &#125; public void commit() &#123; for (TransactionalCache txCache : transactionalCaches.values()) &#123; txCache.commit(); &#125; &#125; public void rollback() &#123; for (TransactionalCache txCache : transactionalCaches.values()) &#123; txCache.rollback(); &#125; &#125; private TransactionalCache getTransactionalCache(Cache cache) &#123; TransactionalCache txCache = transactionalCaches.get(cache); if (txCache == null) &#123; //对Cache重新包装为TransactionalCache,可以像事务一样的操作缓存 txCache = new TransactionalCache(cache); transactionalCaches.put(cache, txCache); &#125; return txCache; &#125; &#125;``` &gt; ``TransactionlCache``类，它提供了类似于事务操作的方式来操作缓存 ```java public class TransactionalCache implements Cache &#123; //装饰源对象 private Cache delegate; //用于标示是否提交时先清除缓存 private boolean clearOnCommit; //临时缓存区，存储待提交的缓存信息 private Map&lt;Object, Object&gt; entriesToAddOnCommit; private Set&lt;Object&gt; entriesMissedInCache; public TransactionalCache(Cache delegate) &#123; this.delegate = delegate; this.clearOnCommit = false; this.entriesToAddOnCommit = new HashMap&lt;Object, Object&gt;(); this.entriesMissedInCache = new HashSet&lt;Object&gt;(); &#125; @Override public Object getObject(Object key) &#123; // issue #116 Object object = delegate.getObject(key); if (object == null) &#123; entriesMissedInCache.add(key); &#125; // issue #146 if (clearOnCommit) &#123; return null; &#125; else &#123; return object; &#125; &#125; @Override public void putObject(Object key, Object object) &#123; entriesToAddOnCommit.put(key, object); &#125; //提交缓存，即确认缓存信息需要保存 public void commit() &#123; if (clearOnCommit) &#123; delegate.clear(); &#125; flushPendingEntries(); reset(); &#125; public void rollback() &#123; unlockMissedEntries(); reset(); &#125; private void reset() &#123; clearOnCommit = false; entriesToAddOnCommit.clear(); entriesMissedInCache.clear(); &#125; //将缓存信息放入最终缓存区 private void flushPendingEntries() &#123; for (Map.Entry&lt;Object, Object&gt; entry : entriesToAddOnCommit.entrySet()) &#123; delegate.putObject(entry.getKey(), entry.getValue()); &#125; for (Object entry : entriesMissedInCache) &#123; if (!entriesToAddOnCommit.containsKey(entry)) &#123; delegate.putObject(entry, null); &#125; &#125; &#125; private void unlockMissedEntries() &#123; for (Object entry : entriesMissedInCache) &#123; try &#123; delegate.removeObject(entry); &#125; catch (Exception e) &#123; log.warn(\"Unexpected exception while notifiying a rollback to the cache adapter.\" + \"Consider upgrading your cache adapter to the latest version. Cause: \" + e); &#125; &#125; &#125; &#125; 我们看到TransactionalCacheManager和TransactionalCache组合提供了类似事务操作的方式来控制缓存的功能,当然mybatis不可能止步于此，它做的更深入：二级缓存使用了装饰器模式优雅的做了很多事情，如图： 装饰类的执行链：SynchroinzedCache -&gt; LoggingCache -&gt; SerializedCache -&gt; LruCache -&gt; PerpetualCache，来看看每个装饰器都是什么功能： SynchronizedCache: 同步的方式操作Cache，实现比较简单，直接使用synchronized修饰方法。 LoggingCache: 日志功能，装饰类，用于记录缓存的命中率，如果开启了DEBUG模式，则会输出命中率日志。 SerializedCache: 序列化功能，将值序列化后存到缓存中。该功能用于缓存返回一份实例的Copy，用于保存线程安全。 LruCache: 采用了Lru算法实现（内部使用LinkedHashMap实现），移除最近最少使用的缓存。 PerpetualCache: 作为最基础的缓存类，底层实现比较简单，直接使用了HashMap来存储数据。 二级缓存的生命周期 上面我们说到二级缓存采用了装饰器模式，其中存在LruCache装饰器，该装饰器就管理着我们的缓存有效性，所以从理论上讲二级缓存属于应用级别的。此外，二级缓存虽然可以在不同SqlSession之间共享缓存，但是还是有限制的，因为cache的作用域为同一个namespace，即在同一个namespace下才使用相同的缓存，不同的namespace使用的是不同的缓存。这样在多表级联且在多个namespace下时存在脏数据，实际上达到不能共享缓存的目的，我们可以使用cache-ref指定相同的namespace（不建议这么做）。同样的insert,update,delete操作时会清除原有缓存 总结 二级缓存的执行时流程： 根据配置生成包装的CachingExecutor来增强executor的功能 当sqlSession委托executor执行器执行查询时，判断是否存在缓存 存在时，判断是否开启了二级缓存，开启的话，获取具体的缓存信息，如果存在就直接返回; 不存在就查库并放入临时缓存区 不存在，直接查库(理论上配置开启了二级缓存是不会出现这种情况的) 提交事务，临时缓存真正存储到二级缓存区 二级缓存的特点： Mybatis二级缓存是基于namespace的，同一个namespace下的属于同一个缓存，不同的namespace使用不同的缓存，但是可以通过cache-ref解决。 Mybatis二级缓存的生命周期与应用一致，存储在Configuration中(实际在MappedStatement中两者是一样的)，失效清理策略可自行配置失效策略，默认是LRU。 Mybatis二级缓存的实体对象需要序列化。 Mybatis二级缓存是一个粗粒度的缓存，可以实现在不同sqlsession之间共享缓存。 Mybatis的二级缓存在多表查询时，极大可能会出现脏数据，有设计上的缺陷，且在分布式环境下因为默认缓存是在本地，必然会读到脏数据，还不容使用其他类似redis,memcache来的好些。 一二级缓存的处理流程：","tags":[{"name":"Mybatis","slug":"Mybatis","permalink":"https://cnkeep.github.io/tags/Mybatis/"}]},{"title":"01_拓展-缓存(一级缓存)","date":"2018-06-18T08:37:00.000Z","path":"2018/06/18/06-01_拓展-缓存(一级缓存)/","text":"拓展-缓存(一级缓存) 目录 前言 正文 使用一级缓存 一级缓存原理分析 一级缓存的声明周期 总结 前言 前面几节，我们针对mybatis的执行流程，sql配置做了介绍。我们知道，mybatis是一个高效的orm框架，除了前几届了解到的功能，为了提高查询性能，mybatis也提供了缓存的功能，本节我们就一起来探究一下。 正文 我们在系统的运行过程中，总是会出现这样的情况：多次执行完全相同的查询语句，尤其是那种数据变化不大，读多写少的场景，重复的查库会增加系统的开销，自然我们就想到了使用缓存。mybatis就为我们提供了一级缓存和二级缓存的功能（实际生产中建议关闭缓存功能，让更专业的工具去做缓存的功能），本节我们来了解一下一级缓存。 使用一级缓存 在mybatis配置文件分析中介绍过缓存的配置 配置开启一级缓存12345678910111213&lt;?xml version=\"1.0\" encoding=\"UTF-8\" ?&gt;&lt;!DOCTYPE configuration PUBLIC \"-//mybatis.org//DTD Config 3.0//EN\" \"http://mybatis.org/dtd/mybatis-3-config.dtd\"&gt;&lt;configuration&gt; &lt;settings&gt; &lt;!-- 打印查询语句--&gt; &lt;setting name=\"logImpl\" value=\"STDOUT_LOGGING\" /&gt; &lt;!-- 共有2个选项，SESSION和STATEMENT,默认SESSION --&gt; &lt;setting name=\"localCacheScope\" value=\"SESSION\"/&gt; &lt;/settings&gt; &lt;mappers&gt; &lt;mapper resource=\"mapper/UserMapper.xml\" /&gt; &lt;/mappers&gt;&lt;/configuration&gt; 我们在同一个sqlSession中执行相同的sql 12345678910111213141516@Testpublic void mapperTest() &#123; SqlSessionFactory sqlSessionFactory = application.getBean(SqlSessionFactory.class); SqlSession sqlSession = sqlSessionFactory.openSession(); try &#123; UserMapper userMapper = sqlSession.getMapper(UserMapper.class); User user = userMapper.getById(\"4f98966b-f7a9-401c-b327-aa0e401d47b4\"); userMapper.getById(\"4f98966b-f7a9-401c-b327-aa0e401d47b4\"); System.out.println(user); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; sqlSession.close(); &#125;&#125; 查看数据库执行，发现只执行了一次： 之后改变localCacheScope为STATEMENT,发现执行了多次: 接下来我们来分析一级缓存是如何实现的。 一级缓存原理分析 在Mybati执行流程分析我们分析过，通过SqlSession去执行查询，最终会委托给Executor,我们发现一级缓存就属于Session级别，在同一个Sessino会话中有效，并且缓存信息存储在Executor中，来看一下查询流程：12345678910111213141516171819202122232425262728293031323334353637383940414243//BaseExecutor.classpublic &lt;E&gt; List&lt;E&gt; query(MappedStatement ms, Object parameter, RowBounds rowBounds, ResultHandler resultHandler, CacheKey key, BoundSql boundSql) throws SQLException &#123; //去除多余代码，保留主要代码段 //判断是否需要清除缓存，可以在配置sql语句是执行flushCahe=true,默认为false if (queryStack == 0 &amp;&amp; ms.isFlushCacheRequired()) &#123; clearLocalCache(); &#125; List&lt;E&gt; list; //查询缓存 list = resultHandler == null ? (List&lt;E&gt;) localCache.getObject(key) : null; if (list != null) &#123; //缓存存在 handleLocallyCachedOutputParameters(ms, key, parameter, boundSql); &#125; else &#123; //缓存不存在，查库 list = queryFromDatabase(ms, parameter, rowBounds, resultHandler, key, boundSql); &#125; //判断是否缓存结果，理论上都会缓存，只是设置了localCacheScope=STATEMENT会清除缓存 if (configuration.getLocalCacheScope() == LocalCacheScope.STATEMENT) &#123; clearLocalCache(); &#125; return list;&#125; private &lt;E&gt; List&lt;E&gt; queryFromDatabase(MappedStatement ms, Object parameter, RowBounds rowBounds, ResultHandler resultHandler, CacheKey key, BoundSql boundSql) throws SQLException &#123; List&lt;E&gt; list; localCache.putObject(key, EXECUTION_PLACEHOLDER); try &#123; list = doQuery(ms, parameter, rowBounds, resultHandler, boundSql); &#125; finally &#123; localCache.removeObject(key); &#125; //一级缓存，缓存查询结果 localCache.putObject(key, list); if (ms.getStatementType() == StatementType.CALLABLE) &#123; localOutputParameterCache.putObject(key, parameter); &#125; return list; &#125; 我们来看看这个缓存的Key和实现原理，首先看Key的生成：12345678910111213141516171819202122232425262728293031323334353637383940//BaseExecutor.class/*** 使用id+offset+limit+sql+params来标示一次查询*/ public CacheKey createCacheKey(MappedStatement ms, Object parameterObject, RowBounds rowBounds, BoundSql boundSql) &#123; if (closed) &#123; throw new ExecutorException(\"Executor was closed.\"); &#125; CacheKey cacheKey = new CacheKey(); cacheKey.update(ms.getId()); cacheKey.update(Integer.valueOf(rowBounds.getOffset())); cacheKey.update(Integer.valueOf(rowBounds.getLimit())); cacheKey.update(boundSql.getSql()); List&lt;ParameterMapping&gt; parameterMappings = boundSql.getParameterMappings(); TypeHandlerRegistry typeHandlerRegistry = ms.getConfiguration().getTypeHandlerRegistry(); // mimic DefaultParameterHandler logic for (int i = 0; i &lt; parameterMappings.size(); i++) &#123; ParameterMapping parameterMapping = parameterMappings.get(i); if (parameterMapping.getMode() != ParameterMode.OUT) &#123; Object value; String propertyName = parameterMapping.getProperty(); if (boundSql.hasAdditionalParameter(propertyName)) &#123; value = boundSql.getAdditionalParameter(propertyName); &#125; else if (parameterObject == null) &#123; value = null; &#125; else if (typeHandlerRegistry.hasTypeHandler(parameterObject.getClass())) &#123; value = parameterObject; &#125; else &#123; MetaObject metaObject = configuration.newMetaObject(parameterObject); value = metaObject.getValue(propertyName); &#125; cacheKey.update(value); &#125; &#125; if (configuration.getEnvironment() != null) &#123; // issue #176 cacheKey.update(configuration.getEnvironment().getId()); &#125; return cacheKey; &#125; 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667public class CacheKey implements Cloneable, Serializable &#123; private List&lt;Object&gt; updateList;//用于存储参数列表 /** * 只关注重要的方法，其余的在此忽略.... */ //只是简单的将参数依次加入到List中 public void update(Object object) &#123; // 参数是Array，依次顺序追加之 if (object != null &amp;&amp; object.getClass().isArray()) &#123; int length = Array.getLength(object); for (int i = 0; i &lt; length; i++) &#123; Object element = Array.get(object, i); doUpdate(element); &#125; &#125; else &#123; doUpdate(object); &#125; &#125; private void doUpdate(Object object) &#123; int baseHashCode = object == null ? 1 : object.hashCode(); count++; checksum += baseHashCode; baseHashCode *= count; hashcode = multiplier * hashcode + baseHashCode; updateList.add(object); &#125; /** * 重要的方法：简单依次对比List中元素是否相等即可判断出sql是否相同 */ @Override public boolean equals(Object object) &#123; if (this == object) &#123; return true; &#125; if (!(object instanceof CacheKey)) &#123; return false; &#125; final CacheKey cacheKey = (CacheKey) object; if (hashcode != cacheKey.hashcode) &#123; return false; &#125; if (checksum != cacheKey.checksum) &#123; return false; &#125; if (count != cacheKey.count) &#123; return false; &#125; for (int i = 0; i &lt; updateList.size(); i++) &#123; Object thisObject = updateList.get(i); Object thatObject = cacheKey.updateList.get(i); if (thisObject == null) &#123; if (thatObject != null) &#123; return false; &#125; &#125; else &#123; if (!thisObject.equals(thatObject)) &#123; return false; &#125; &#125; &#125; return true; &#125;&#125; 可以看到，key是通过statementId+offset+limit+sql+params这几个条件决定，通过List将他们存储起来，对比相同只需要依次对比List中的元素是否相等即可判断是否为同一执行sql. 深入看缓存存储实现localCache,发现是PerpetualCache类，来一探究竟1234567891011121314151617181920212223public class PerpetualCache implements Cache &#123; private Map&lt;Object, Object&gt; cache = new HashMap&lt;Object, Object&gt;(); // 忽略一些方法，只关注重要的方法 @Override public void putObject(Object key, Object value) &#123; cache.put(key, value); &#125; @Override public Object getObject(Object key) &#123; return cache.get(key); &#125; @Override public Object removeObject(Object key) &#123; return cache.remove(key); &#125; @Override public void clear() &#123; cache.clear(); &#125;&#125; 恍然大悟，原来就是通过HashMap来存储sqlKey=result键值对。 一级缓存的声明周期 那么问题来了，既然有缓存，那缓存什么时候被清理呢？缓存的生命周期又是什么呢？看来我们还需要继续探索。我们继续在BaseExecutor中找线索,发现commit,rollback,close这三个方法中清除了缓存既然清理缓存是在这三个方法中，那这三个方法什么时候被调用呢？很容易通过SqlSession接口就知道如下调用规则： sql类型 调用方法 select close update commit,rollback,close delete commit,rollback,close 所以我们得出结论： 一级缓存声明周期是在session级别(因为session调用最终都会调用close方法清除缓存)。 一级缓存在执行update,delete操作时会被清除。 总结 一级缓存的流程： 对于某个Select Statement，根据该Statement生成key。 判断在Local Cache中,该key是否用对应的数据存在。 如果命中，则跳过查询数据库，继续往下走。 如果没命中： 去数据库中查询数据，得到查询结果 将key和查询到的结果作为key和value，放入Local Cache中 将查询结果返回 判断缓存级别是否为STATEMENT级别，如果是的话，清空本地缓存 一级缓存的特点： Mybatis一级缓存的生命周期和SqlSession一致，即只能在同一个sqlSession中共享缓存数据，存储在BaseExecutor中。 Mybatis的一级缓存是一个粗粒度的缓存，没有更新缓存和缓存过期的概念，同时只是使用了默认的hashmap，也没有做容量上的限定。 Mybatis的一级缓存最大范围是SqlSession内部，有多个SqlSession或者分布式的环境下，有操作数据库写的话，会引起脏数据，建议是把一级缓存的默认级别设定为Statement，即不使用一级缓存。 Mybatis的一级缓存不能人为干预，即无论设置了flushCache=true还是cacheScope=STATEMENT都不能阻止mybatis缓存查询结果，只能是控制在下一次查询时提前清空缓存，达到最终结果上的无缓存。","tags":[{"name":"Mybatis","slug":"Mybatis","permalink":"https://cnkeep.github.io/tags/Mybatis/"}]},{"title":"01_Mapper学习","date":"2018-06-18T06:28:00.000Z","path":"2018/06/18/05-01_Mapper学习/","text":"Mapper.xml学习前言 本节我们针对sql的配置映射文件Mapper.xml进行学习目录 基本元素（按照它们应该被定义的顺序）: select – 映射查询语句 sql – 可被其他语句引用的可重用语句块 insert – 映射插入语句 update – 映射更新语句 delete – 映射删除语句 parameterMap – 已废弃！老式风格的参数映射。内联参数是首选,这个元素可能在将来被移除，这里不会记录。 resultMap – 是最复杂也是最强大的元素，用来描述如何从数据库结果集中来加载对象。 cache-ref – 其他命名空间缓存配置的引用。 cache – 给定命名空间的缓存配置。 动态sql #{}和${}区别 动态表达式 正文基本元素select 查询语句是 MyBatis 中最常用的元素之一，光能把数据存到数据库中价值并不大，如果还能重新取出来才有用，多数应用也都是查询比修改要频繁。对每个插入、更新或删除操作，通常对应多个查询操作。这是 MyBatis 的基本原则之一，也是将焦点和努力放到查询和结果映射的原因。简单查询的 select 元素是非常简单的。比如：123&lt;select id=\"selectPerson\" parameterType=\"int\" resultType=\"hashmap\"&gt; SELECT * FROM PERSON WHERE ID = #&#123;id&#125;&lt;/select&gt; 这个语句被称作 selectPerson，接受一个 int（或 Integer）类型的参数，并返回一个 HashMap 类型的对象，其中的键是列名，值便是结果行中的对应值。 注意参数符号：#{id} 这就告诉 MyBatis 创建一个预处理语句参数，通过 JDBC，这样的一个参数在 SQL 中会由一个“?”来标识，并被传递到一个新的预处理语句中，就像这样：1234// Similar JDBC code, NOT MyBatis…String selectPerson = \"SELECT * FROM PERSON WHERE ID=?\";PreparedStatement ps = conn.prepareStatement(selectPerson);ps.setInt(1,id); select 元素有很多属性允许你配置，来决定每条语句的作用细节。123456789101112&lt;select id=\"selectPerson\" parameterType=\"int\" parameterMap=\"deprecated\" resultType=\"hashmap\" resultMap=\"personResultMap\" flushCache=\"false\" useCache=\"true\" timeout=\"10000\" fetchSize=\"256\" statementType=\"PREPARED\" resultSetType=\"FORWARD_ONLY\"&gt; 属性 描述 id 在命名空间中唯一的标识符，可以被用来引用这条语句。 parameterType 将会传入这条语句的参数类的完全限定名或别名。这个属性是可选的，因为 MyBatis 可以通过 TypeHandler 推断出具体传入语句的参数，默认值为 unset。 parameterMap 这是引用外部 parameterMap 的已经被废弃的方法。使用内联参数映射和 parameterType 属性。 resultType 从这条语句中返回的期望类型的类的完全限定名或别名。注意如果是集合情形，那应该是集合可以包含的类型，而不能是集合本身。使用 resultType 或 resultMap，但不能同时使用。 resultMap 外部 resultMap 的命名引用。结果集的映射是 MyBatis 最强大的特性，对其有一个很好的理解的话，许多复杂映射的情形都能迎刃而解。使用 resultMap 或 resultType，但不能同时使用。 flushCache 将其设置为 true，任何时候只要语句被调用，都会导致一级缓存和二级缓存都会被清空，默认值：false。 useCache 将其设置为 true，将会导致本条语句的结果被二级缓存，默认值：对 select 元素为 true。 timeout 这个设置是在抛出异常之前，驱动程序等待数据库返回请求结果的秒数。默认值为 unset（依赖驱动）。 fetchSize 这是尝试影响驱动程序每次批量返回的结果行数和这个设置值相等。默认值为 unset（依赖驱动）。 statementType STATEMENT，PREPARED 或 CALLABLE 的一个。这会让 MyBatis 分别使用 Statement，PreparedStatement 或 CallableStatement，默认值：PREPARED。 resultSetType FORWARD_ONLY，SCROLL_SENSITIVE 或 SCROLL_INSENSITIVE 中的一个，默认值为 unset （依赖驱动）。 databaseId 如果配置了 databaseIdProvider，MyBatis 会加载所有的不带 databaseId 或匹配当前 databaseId 的语句；如果带或者不带的语句都有，则不带的会被忽略。 resultOrdered 这个设置仅针对嵌套结果 select 语句适用：如果为 true，就是假设包含了嵌套结果集或是分组了，这样的话当返回一个主结果行的时候，就不会发生有对前面结果集的引用的情况。这就使得在获取嵌套的结果集的时候不至于导致内存不够用。默认值：false。 resultSets 这个设置仅对多结果集的情况适用，它将列出语句执行后返回的结果集并每个结果集给一个名称，名称是逗号分隔的。 insert, update 和 delete 数据变更语句 insert，update 和 delete 的实现非常接近：123456789101112131415161718192021222324&lt;insert id=\"insertAuthor\" parameterType=\"domain.blog.Author\" flushCache=\"true\" statementType=\"PREPARED\" keyProperty=\"\" keyColumn=\"\" useGeneratedKeys=\"\" timeout=\"20\"&gt;&lt;update id=\"updateAuthor\" parameterType=\"domain.blog.Author\" flushCache=\"true\" statementType=\"PREPARED\" timeout=\"20\"&gt;&lt;delete id=\"deleteAuthor\" parameterType=\"domain.blog.Author\" flushCache=\"true\" statementType=\"PREPARED\" timeout=\"20\"&gt; 属性 描述 id 命名空间中的唯一标识符，可被用来代表这条语句。 parameterType 将要传入语句的参数的完全限定类名或别名。这个属性是可选的，因为 MyBatis 可以通过 TypeHandler 推断出具体传入语句的参数，默认值为 unset。 parameterMap 这是引用外部 parameterMap 的已经被废弃的方法。使用内联参数映射和 parameterType 属性。 flushCache 将其设置为 true，任何时候只要语句被调用，都会导致本地缓存和二级缓存都会被清空，默认值：true（对应插入、更新和删除语句）。 timeout 这个设置是在抛出异常之前，驱动程序等待数据库返回请求结果的秒数。默认值为 unset（依赖驱动）。 statementType STATEMENT，PREPARED 或 CALLABLE 的一个。这会让 MyBatis 分别使用 Statement，PreparedStatement 或 CallableStatement，默认值：PREPARED。 useGeneratedKeys （仅对 insert 和 update 有用）这会令 MyBatis 使用 JDBC 的 getGeneratedKeys 方法来取出由数据库内部生成的主键（比如：像 MySQL 和 SQL Server 这样的关系数据库管理系统的自动递增字段），默认值：false。 keyProperty （仅对 insert 和 update 有用）唯一标记一个属性，MyBatis 会通过 getGeneratedKeys 的返回值或者通过 insert 语句的 selectKey 子元素设置它的键值，默认：unset。如果希望得到多个生成的列，也可以是逗号分隔的属性名称列表。 keyColumn （仅对 insert 和 update 有用）通过生成的键值设置表中的列名，这个设置仅在某些数据库（像 PostgreSQL）是必须的，当主键列不是表中的第一列的时候需要设置。如果希望得到多个生成的列，也可以是逗号分隔的属性名称列表。 databaseId 如果配置了 databaseIdProvider，MyBatis 会加载所有的不带 databaseId 或匹配当前 databaseId 的语句；如果带或者不带的语句都有，则不带的会被忽略。 sql 这个元素可以被用来定义可重用的 SQL 代码段，可以包含在其他语句中。它可以被静态地(在加载参数) 参数化. 不同的属性值通过包含的实例变化. 比如：1&lt;sql id=\"userColumns\"&gt; $&#123;alias&#125;.id,$&#123;alias&#125;.username,$&#123;alias&#125;.password &lt;/sql&gt; 这个 SQL 片段可以被包含在其他语句中，例如：1234567&lt;select id=\"selectUsers\" resultType=\"map\"&gt; select &lt;include refid=\"userColumns\"&gt;&lt;property name=\"alias\" value=\"t1\"/&gt;&lt;/include&gt;, &lt;include refid=\"userColumns\"&gt;&lt;property name=\"alias\" value=\"t2\"/&gt;&lt;/include&gt; from some_table t1 cross join some_table t2&lt;/select&gt; 属性值也可以被用在 include 元素的 refid 属性里（）或 include 内部语句中（${prefix}Table），例如：1234567891011121314151617&lt;sql id=\"sometable\"&gt; $&#123;prefix&#125;Table&lt;/sql&gt;&lt;sql id=\"someinclude\"&gt; from &lt;include refid=\"$&#123;include_target&#125;\"/&gt;&lt;/sql&gt;&lt;select id=\"select\" resultType=\"map\"&gt; select field1, field2, field3 &lt;include refid=\"someinclude\"&gt; &lt;property name=\"prefix\" value=\"Some\"/&gt; &lt;property name=\"include_target\" value=\"sometable\"/&gt; &lt;/include&gt;&lt;/select&gt; resultMap 结果集映射是mybatis最复杂的部分，mybatis为我们做了很多的工作,给出一个复杂的例子:123456789101112131415161718192021222324252627282930313233343536373839 public class Author&#123; private Integer id; private String name; //getter and setter &#125; public class Comment&#123; private Integer id; private String content; //getter and setter &#125; public class Blog&#123; private Integer id; private Author author; private List&lt;Comment&gt; commentList; private String title; //getter and setter &#125;``` ```xml &lt;resultMap id=\"detailedBlogResultMap\" type=\"Blog\"&gt; &lt;id property=\"id\" column=\"id\"/&gt; &lt;constructor&gt; &lt;idArg column=\"blog_id\" javaType=\"int\"/&gt; &lt;/constructor&gt; &lt;result property=\"title\" column=\"blog_title\"/&gt; &lt;association property=\"author\" javaType=\"Author\"&gt; &lt;id property=\"id\" column=\"author_id\"/&gt;&gt; &lt;result property=\"name\" column=\"author_name\"/&gt; &lt;/association&gt; &lt;collection property=\"commentList\" ofType=\"Comment\"&gt; &lt;id property=\"id\" column=\"comment_id\"/&gt; &lt;result property=\"content\" column=\"content\"/&gt; &lt;/collection&gt; &lt;discriminator javaype=\"int\" column=\"type\" &gt; &lt;case value=\"\" resultType=\"\"/&gt; &lt;/discriminator&gt; &lt;/resultMap&gt; resultMap constructor - 用于在实例化类时，注入结果到构造方法中 idArg - ID 参数;标记出作为 ID 的结果可以帮助提高整体性能 arg - 将被注入到构造方法的一个普通结果 id – 一个 ID 结果;标记出作为 ID 的结果可以帮助提高整体性能 result – 注入到字段或 JavaBean 属性的普通结果 association – 一个复杂类型的关联;许多结果将包装成这种类型嵌套结果映射 – 关联可以指定为一个 resultMap 元素，或者引用一个 collection – 一个复杂类型的集合嵌套结果映射 – 集合可以指定为一个 resultMap 元素，或者引用一个 discriminator – 使用结果值来决定使用哪个 resultMap case – 基于某些值的结果映射嵌套结果映射,一个 case 也是一个映射它本身的结果,因此可以包含很多相 同的元素，或者它可以参照一个外部的 resultMap。 动态sql#{}和${}区别 有时候我们会在xml的sql中使用到这两者但是这两者有什么区别呢？#{}: 属于动态参数，会进行预编译，而且进行类型匹配，用于变量替换 ${}: 不会进行类型匹配，用于字符串拼接举个例子：123456&lt;select id=\"findByName1\"&gt; select * from user where name=#&#123;params.name&#125;&lt;/select&gt;&lt;select id=\"findByName2\"&gt; select * from user where id=$&#123;parms.name&#125;&lt;/select&gt; 上面2个sql最终的结果都为：select * from user where name = &#39;zhangsan&#39; &#39;,但是findByName1中的参数会进行预编译参数替换为?,防止sql注入；而findByName2只是简单的字符串替换，使用时应当注意sql注入的问题,但是使用order by或者将表名作为参数传递进来是只能使用${}. 动态表达式 MyBatis 采用功能强大的基于 OGNL 的表达式来提供了动态sql的功能，主要包含: if choose(when, otherwise) trim (where, set) foreach if1234567&lt;select id=\"findUserList\"&gt; select * from user where state=0 &lt;if test=\"path != null\"&gt; and path=#&#123;param.path&#125; &lt;/if&gt; &lt;/select&gt; 这句话意味着，当传入的参数中path存在则筛选条件追加path字段，反之则忽略。 choose 有时我们不想应用到所有的条件语句，而只想从中择其一项。针对这种情况，MyBatis 提供了 choose 元素，它有点像 Java 中的 switch 语句。123456789101112&lt;select id=\"findUserList\"&gt; select * from user where name=#&#123;name&#125; &lt;choose&gt; &lt;when test=\"state != null and state != 0\"&gt; state = #&#123;state&#125; &lt;/when&gt; &lt;otherwise&gt; state = 0 &lt;/otherwise&gt; &lt;/choose&gt;&lt;/select&gt; 上面的sql意味着，假如存在state字段，则查询参数拼接该字段，否则拼接为默认的参数值。 trim(where)123456789101112131415&lt;select id=\"findActiveBlogLike\" resultType=\"Blog\"&gt; SELECT * FROM BLOG &lt;where&gt; &lt;if test=\"state != null\"&gt; state = #&#123;state&#125; &lt;/if&gt; &lt;if test=\"title != null\"&gt; AND title like #&#123;title&#125; &lt;/if&gt; &lt;if test=\"author != null and author.name != null\"&gt; AND author_name like #&#123;author.name&#125; &lt;/if&gt; &lt;/where&gt;&lt;/select&gt; where 元素只会在至少有一个子元素的条件返回 SQL 子句的情况下才去插入“WHERE”子句。而且，若语句的开头为“AND”或“OR”，where 元素也会将它们去除。如果 where 元素没有按正常套路出牌，我们可以通过自定义 trim 元素来定制 where 元素的功能。比如，和 where 元素等价的自定义 trim 元素为：12345678 &lt;trim prefix=\"WHERE\" prefixOverrides=\"AND |OR \"&gt; ... &lt;/trim&gt;``` &gt; prefixOverrides 属性会忽略通过管道分隔的文本序列（注意此例中的空格也是必要的）。它的作用是移除所有指定在 prefixOverrides 属性中的内容，并且插入 prefix 属性中指定的内容。 ##### foreach &gt; 动态 SQL 的另外一个常用的操作需求是对一个集合进行遍历，通常是在构建 IN 条件语句的时候。比如： SELECT * FROM POST P WHERE ID in #{item} &lt;/foreach&gt; ` foreach 中的collection取值可以为(list,array,map)","tags":[{"name":"Mybatis","slug":"Mybatis","permalink":"https://cnkeep.github.io/tags/Mybatis/"}]},{"title":"02_接口方式执行流程解读","date":"2018-06-17T05:09:00.000Z","path":"2018/06/17/04-02_接口方式执行流程解读/","text":"接口方式执行分析###前言 上节我们针对id方式mybatis的执行流程进行了分析，但是有的小伙伴就问了，我们都是用Mapper接口，那原理是啥，这节我们就分析一下。 正文mapper接口方式的调用示例 先生成mapper.xml文件配置我们的sql12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152&lt;?xml version=\"1.0\" encoding=\"UTF-8\" ?&gt;&lt;!DOCTYPE mapper PUBLIC \"-//mybatis.org//DTD Mapper 3.0//EN\" \"http://mybatis.org/dtd/mybatis-3-mapper.dtd\"&gt;&lt;mapper namespace=\"test.mybatis.mapper.UserMapper\"&gt; &lt;resultMap type=\"test.entity.User\" id=\"user\"&gt; &lt;id column=\"id\" property=\"id\" javaType=\"String\"/&gt; &lt;result column=\"name\" property=\"name\"/&gt; &lt;/resultMap&gt; &lt;select id=\"findById\" resultMap=\"user\"&gt; select * from user where id = #&#123;id&#125; &lt;/select&gt;&lt;/mapper&gt;``` &gt;* 提供xml对应的接口 ```java package test.entity; public class User&#123; private String id; private String name; public void setId(String id)&#123; this.id = id; &#125; public String getId()&#123; return id; &#125; public void setName(String name)&#123; this.name = name; &#125; public String getName()&#123; return name; &#125; @Override public boolean equals(Object o) &#123; if (this == o) return true; if (o == null || getClass() != o.getClass()) return false; if (!super.equals(o)) return false; User user = (User) o; if (!id.equals(user.id)) return false; return name.equals(user.name); &#125; @Override public int hashCode() &#123; int result = super.hashCode(); result = 31 * result + id.hashCode(); result = 31 * result + name.hashCode(); return result; &#125; &#125; 1234package test.mybatis.mapper;public interface User&#123; User findById(Integer id);&#125; 注意：这里接口的包名要和xml的namespace一致，方法名要和select结点的id一致 调用 1234567891011public class MybatisMapperDemo&#123; public static void main(String[] args)&#123; String resource = \"mybatis-config.xml\"; InputStream inputStream = Resources.getResourceAsStream(resource); SqlSessionFactory sqlSessionFactory = new SqlSessionFactoryBuilder().build(inputStream); SqlSession openSession = sqlSessionFactory.openSession(); UserMapper mapper = openSession.getMapper(UserMapper.class); User user = mapper.findById(\"001\"); System.out.println(user); &#125;&#125; 源码分析 我们发现接口的包名和什么的都有限制，那先猜想一下是否就是通过接口的类名+方法名去生成id，从而转化到上一节的执行流程中，我们从源码中去分析一下是否是我们想的那样。 获取Mapper接口123456//DefaultSqlSession @Override public &lt;T&gt; T getMapper(Class&lt;T&gt; type) &#123; //调用Configuration获取 return configuration.&lt;T&gt;getMapper(type, this); &#125; mapper接口的获取最终委托给MapperProxyFactory去生成代理对象12345678910111213 // MapperRegistry.classpublic &lt;T&gt; T getMapper(Class&lt;T&gt; type, SqlSession sqlSession) &#123; final MapperProxyFactory&lt;T&gt; mapperProxyFactory = (MapperProxyFactory&lt;T&gt;) knownMappers.get(type); if (mapperProxyFactory == null) &#123; throw new BindingException(\"Type \" + type + \" is not known to the MapperRegistry.\"); &#125; try &#123; //委托给指定的MapperProxyFactory去生成 return mapperProxyFactory.newInstance(sqlSession); &#125; catch (Exception e) &#123; throw new BindingException(\"Error getting mapper instance. Cause: \" + e, e); &#125;&#125; 12345678910 //MapperProxyFactory.classpublic T newInstance(SqlSession sqlSession) &#123; final MapperProxy&lt;T&gt; mapperProxy = new MapperProxy&lt;T&gt;(sqlSession, mapperInterface, methodCache); return newInstance(mapperProxy); &#125; protected T newInstance(MapperProxy&lt;T&gt; mapperProxy) &#123; //JDK动态代理实现 return (T) Proxy.newProxyInstance(mapperInterface.getClassLoader(), new Class[] &#123; mapperInterface &#125;, mapperProxy);&#125; 最终使用JDK的动态代理技术生成Mappe接口的代理对象，我们知道jdk动态代理的方法入口是InvocationHandler接口，即MapperProxy类，来分析一下该类。123456789101112131415161718192021222324252627282930313233343536373839public class MapperProxy&lt;T&gt; implements InvocationHandler, Serializable &#123; private static final long serialVersionUID = -6424540398559729838L; private final SqlSession sqlSession; private final Class&lt;T&gt; mapperInterface; private final Map&lt;Method, MapperMethod&gt; methodCache; public MapperProxy(SqlSession sqlSession, Class&lt;T&gt; mapperInterface, Map&lt;Method, MapperMethod&gt; methodCache) &#123; this.sqlSession = sqlSession; this.mapperInterface = mapperInterface; this.methodCache = methodCache; &#125; @Override public Object invoke(Object proxy, Method method, Object[] args) throws Throwable &#123; //如果是从Object基类中声明的方法则直接执行，即过滤hashCode,equals，getClass等方法 if (Object.class.equals(method.getDeclaringClass())) &#123; try &#123; return method.invoke(this, args); &#125; catch (Throwable t) &#123; throw ExceptionUtil.unwrapThrowable(t); &#125; &#125; //通过MapperMethod去执行方法 final MapperMethod mapperMethod = cachedMapperMethod(method); return mapperMethod.execute(sqlSession, args); &#125; private MapperMethod cachedMapperMethod(Method method) &#123; MapperMethod mapperMethod = methodCache.get(method); if (mapperMethod == null) &#123; mapperMethod = new MapperMethod(mapperInterface, method, sqlSession.getConfiguration()); methodCache.put(method, mapperMethod); &#125; return mapperMethod; &#125;&#125; 接着看MapperMethod类1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768public class MapperMethod &#123; private final SqlCommand command; private final MethodSignature method; public MapperMethod(Class&lt;?&gt; mapperInterface, Method method, Configuration config) &#123; this.command = new SqlCommand(config, mapperInterface, method); this.method = new MethodSignature(config, mapperInterface, method); &#125; public Object execute(SqlSession sqlSession, Object[] args) &#123; Object result; if (SqlCommandType.INSERT == command.getType()) &#123; Object param = method.convertArgsToSqlCommandParam(args); result = rowCountResult(sqlSession.insert(command.getName(), param)); &#125; else if (SqlCommandType.UPDATE == command.getType()) &#123; Object param = method.convertArgsToSqlCommandParam(args); result = rowCountResult(sqlSession.update(command.getName(), param)); &#125; else if (SqlCommandType.DELETE == command.getType()) &#123; Object param = method.convertArgsToSqlCommandParam(args); result = rowCountResult(sqlSession.delete(command.getName(), param)); &#125; else if (SqlCommandType.SELECT == command.getType()) &#123; if (method.returnsVoid() &amp;&amp; method.hasResultHandler()) &#123; executeWithResultHandler(sqlSession, args); result = null; &#125; else if (method.returnsMany()) &#123; result = executeForMany(sqlSession, args); &#125; else if (method.returnsMap()) &#123; result = executeForMap(sqlSession, args); &#125; else if (method.returnsCursor()) &#123; result = executeForCursor(sqlSession, args); &#125; else &#123; Object param = method.convertArgsToSqlCommandParam(args); result = sqlSession.selectOne(command.getName(), param); &#125; &#125; else if (SqlCommandType.FLUSH == command.getType()) &#123; result = sqlSession.flushStatements(); &#125; else &#123; throw new BindingException(\"Unknown execution method for: \" + command.getName()); &#125; if (result == null &amp;&amp; method.getReturnType().isPrimitive() &amp;&amp; !method.returnsVoid()) &#123; throw new BindingException(\"Mapper method '\" + command.getName() + \" attempted to return null from a method with a primitive return type (\" + method.getReturnType() + \").\"); &#125; return result; &#125; private &lt;E&gt; Object executeForMany(SqlSession sqlSession, Object[] args) &#123; List&lt;E&gt; result; Object param = method.convertArgsToSqlCommandParam(args); if (method.hasRowBounds()) &#123; RowBounds rowBounds = method.extractRowBounds(args); result = sqlSession.&lt;E&gt;selectList(command.getName(), param, rowBounds); &#125; else &#123; result = sqlSession.&lt;E&gt;selectList(command.getName(), param); &#125; // issue #510 Collections &amp; arrays support if (!method.getReturnType().isAssignableFrom(result.getClass())) &#123; if (method.getReturnType().isArray()) &#123; return convertToArray(result); &#125; else &#123; return convertToDeclaredCollection(sqlSession.getConfiguration(), result); &#125; &#125; return result; &#125; //其他的方法暂时省略+&#125; MapperMethod的execute()方法，根据sql的不同类型(insert,update,delete,select),执行相应的操作，最终还是通过SqlSession执行，又回到上一节的流程中。我们梳理一下流程: 通过MapperProxyFactory生成代理对象 通过接口调用的方式最终都进入到MapperProxy的invoke方法中 调用MapperMethod的execute方法 execute方法中通过sql的不同类型分类执行，最终还是通过SqlSession+id的方式执行","tags":[{"name":"Mybatis","slug":"Mybatis","permalink":"https://cnkeep.github.io/tags/Mybatis/"}]},{"title":"01_执行流程解读","date":"2018-06-15T03:30:00.000Z","path":"2018/06/15/04-01_执行流程解读/","text":"执行流程分析 既然mybatis是针对JDBC操作的封装，那我们接下来就拨看云雾见阳光的看看mybatis是如何一步一步的做这些事情的，我们以查询为例。 引言 先看一下基础的jdbc操作有那几个步骤： Step1. 获取连接Step2. 获取执行动态sqlStep3. 构建预编译对象PrepareStatementStep4. 设置动态参数Step5. 通过连接发送执行请求到数据库Step6. 数据库返回结果集，编辑解析返回结果Step7. 关闭连接 我们如何用mybatis做数据库查询呢？1234567891011121314151617181920212223 public class MybatisDemo&#123; public static void main(String[] args)&#123; String resource = \"mybatis-config.xml\"; InputStream inputStream = Resources.getResourceAsStream(resource); SqlSessionFactory sqlSessionFactory = new SqlSessionFactoryBuilder().build(inputStream); try( SqlSession openSession = sqlSessionFactory.openSession() )&#123; User user = openSession.&lt;User&gt;selectOne(\"com.study.mybatis.mapper.UserMapper.select\", 1); System.out.println(user); &#125; &#125; &#125;``` &gt; 我们一步一步来解读mybatis具体的查询流程（以id方式为例，Mapper接口方式后面分析） ### 正文#### 1.获取SqlSession &gt; 在mybatis中连接被抽象为SqlSession,但是并不是真正意义上的connection，其通过SqlSessionFactoy.openSession()获取SqlSession。 #### 2.获取执行sql &gt; 在mybatis中我们是通过mapper.xml来配置sql语句的，而mybatis会通过解析将这些sql语句映射为MappedStatement,一个MappedStatement维护了一条&lt;select|update|delete|insert&gt;节点的封装 &gt; 我们通过namespace+id去获取一个MappedStatement //Configuration.class public MappedStatement getMappedStatement(String id, boolean validateIncompleteStatements) { if (validateIncompleteStatements) { buildAllStatements(); } return mappedStatements.get(id); }12#### 3. 调用查询方法 //DefaultSqlSession.class@Override public List selectList(String statement, Object parameter, RowBounds rowBounds) { try { MappedStatement ms = configuration.getMappedStatement(statement); return executor.query(ms, wrapCollection(parameter), rowBounds, Executor.NO_RESULT_HANDLER); } catch (Exception e) { throw ExceptionFactory.wrapException(“Error querying database. Cause: “ + e, e); } finally { ErrorContext.instance().reset(); } } 1&gt; SqlSession的查询其实质是委托给Executor去执行了，接着来看*Executor*的query方法 @Override public &lt;E&gt; List&lt;E&gt; query(MappedStatement ms, Object parameter, RowBounds rowBounds, ResultHandler resultHandler) throws SQLException { BoundSql boundSql = ms.getBoundSql(parameter); CacheKey key = createCacheKey(ms, parameter, rowBounds, boundSql); return query(ms, parameter, rowBounds, resultHandler, key, boundSql); } @SuppressWarnings(&quot;unchecked&quot;) @Override public &lt;E&gt; List&lt;E&gt; query(MappedStatement ms, Object parameter, RowBounds rowBounds, ResultHandler resultHandler, CacheKey key, BoundSql boundSql) throws SQLException { ErrorContext.instance().resource(ms.getResource()).activity(&quot;executing a query&quot;).object(ms.getId()); if (closed) { throw new ExecutorException(&quot;Executor was closed.&quot;); } if (queryStack == 0 &amp;&amp; ms.isFlushCacheRequired()) { clearLocalCache(); } List&lt;E&gt; list; try { queryStack++; //查询缓存 list = resultHandler == null ? (List&lt;E&gt;) localCache.getObject(key) : null; if (list != null) { handleLocallyCachedOutputParameters(ms, key, parameter, boundSql); } else { //缓存不存在，查库 list = queryFromDatabase(ms, parameter, rowBounds, resultHandler, key, boundSql); } } finally { queryStack--; } if (queryStack == 0) { for (DeferredLoad deferredLoad : deferredLoads) { deferredLoad.load(); } // issue #601 deferredLoads.clear(); //清除二级缓存 if (configuration.getLocalCacheScope() == LocalCacheScope.STATEMENT) { // issue #482 clearLocalCache(); } } return list; } @Override public &lt;E&gt; List&lt;E&gt; query(Statement statement, ResultHandler resultHandler) throws SQLException { PreparedStatement ps = (PreparedStatement) statement; ps.execute(); return resultSetHandler.&lt;E&gt; handleResultSets(ps); } private &lt;E&gt; List&lt;E&gt; queryFromDatabase(MappedStatement ms, Object parameter, RowBounds rowBounds, ResultHandler resultHandler, CacheKey key, BoundSql boundSql) throws SQLException { List&lt;E&gt; list; localCache.putObject(key, EXECUTION_PLACEHOLDER); try { list = doQuery(ms, parameter, rowBounds, resultHandler, boundSql); } finally { localCache.removeObject(key); } localCache.putObject(key, list); if (ms.getStatementType() == StatementType.CALLABLE) { localOutputParameterCache.putObject(key, parameter); } return list; } 1&gt; BaseExecutor提供模板方法，不同的子类实现不同的doQuery回调接口，我们就看SimpleExecutor类 @Override public &lt;E&gt; List&lt;E&gt; doQuery(MappedStatement ms, Object parameter, RowBounds rowBounds, ResultHandler resultHandler, BoundSql boundSql) throws SQLException { Statement stmt = null; try { Configuration configuration = ms.getConfiguration(); //生成StatementHandler代理对象 StatementHandler handler = configuration.newStatementHandler(wrapper, ms, parameter, rowBounds, resultHandler, boundSql); //利用ParameterHandler完成参数的注入，返回Statement对象 stmt = prepareStatement(handler, ms.getStatementLog()); return handler.&lt;E&gt;query(stmt, resultHandler); } finally { closeStatement(stmt); } } 123456789101112&gt; sql交由相应的StatementHandler代理对象执行``` @Override public &lt;E&gt; List&lt;E&gt; query(Statement statement, ResultHandler resultHandler) throws SQLException &#123; String sql = boundSql.getSql(); //执行sql statement.execute(sql); //交由ResultSetHandler完成结果集的映射，最后返回 return resultSetHandler.&lt;E&gt;handleResultSets(statement); &#125; 总结 自此一个简单的执行流程就分析结束了 ，我们来梳理一下流程（如图）: 1.加载配置文件 2.通过SqlSessionFactroy获取SqlSession 3.通过SqlSession调用方法，传入mapper.xml配置的namespace+id, 动态参数列表 4.SqlSession委托给Executor代理对象去执行 5.调用相应的ParameterHandler代理对象完成参数的注入 6.委托StatementHandler代理对象完成数据库执行 7.委托ResultSetHandler代理对象完成结果集的映射操作","tags":[{"name":"Mybatis","slug":"Mybatis","permalink":"https://cnkeep.github.io/tags/Mybatis/"}]},{"title":"mybatis-config的配置介绍","date":"2018-06-13T00:30:00.000Z","path":"2018/06/13/03-mybatis-config的配置介绍/","text":"mybatis-config.xml配置介绍介绍 mybatis-config.xml作为Mybatis的配置文件，提供了很多配置，而且这些配置存在加载顺序，需要特别注意，这里说明一下: properties(属性配置节点) settings (全局配置参数) typeAiases(类型别名) typeHandlers(类型转换器) objectFactory (对象工厂) plugins (插件) environments (环境集合属性对象) mappers(xml映射器)) properties属性 将外部配置文件的属性导入，这样就可以在文件内部使用外部配置文件中的属性值了. 例如配置jdbc.properties12345678910111213141516171819202122232425 jdbc.driver=com.mysql.jdbc.Driver jdbc.url=jdbc:mysql://localhost:3306/jdbc jdbc.username=root jdbc.password=123456``` &gt; * 在mybatis-Config.xml中加载db.properties ```xml &lt;properties resource=\"db.properties\"&gt; &lt;!-- properties中还可以配置一些属性名和属性值,此处的优先加载 --&gt; &lt;!-- &lt;property name=\"driver\" value=\"\"/&gt; --&gt; &lt;/properties&gt; &lt;!-- 和spring整合后 environments配置将废除--&gt; &lt;environments default=\"development\"&gt; &lt;environment id=\"development\"&gt; &lt;!-- 使用jdbc事务管理,事务控制由mybatis管理--&gt; &lt;transactionManager type=\"JDBC\" /&gt; &lt;!-- 数据库连接池,由mybatis管理--&gt; &lt;dataSource type=\"POOLED\"&gt; &lt;property name=\"driver\" value=\"$&#123;jdbc.driver&#125;\" /&gt; &lt;property name=\"url\" value=\"$&#123;jdbc.url&#125;\" /&gt; &lt;property name=\"username\" value=\"$&#123;jdbc.username&#125;\" /&gt; &lt;property name=\"password\" value=\"$&#123;jdbc.password&#125;\" /&gt; &lt;/dataSource&gt; &lt;/environment&gt; &lt;/environments&gt; 注意： 在 properties元素内定义的属性优先读取。 然后读取properties元素中resource或url加载的属性，它会覆盖已读取的同名属性。 最后读取parameterType传递的属性，它会覆盖已读取的同名属性。 settings全局属性 mybatis 运行时可以调整一些参数，比如开启二级缓存 设置参数 描述 有效值 默认值 cacheEnabled 全局地开启或关闭配置文件中的所有映射器已经配置的任何缓存。即二级缓存 true | false true lazyLoadingEnabled 延迟加载的全局开关。当开启时，所有关联对象都会延迟加载。 特定关联关系中可通过设置fetchType属性来覆盖该项的开关状态。 true | false false aggressiveLazyLoading 当开启时，任何方法的调用都会加载该对象的所有属性。否则，每个属性会按需加载（参考lazyLoadTriggerMethods). true | false false (true in ≤3.4.1) multipleResultSetsEnabled 是否允许单一语句返回多结果集（需要兼容驱动）。 true | false true useColumnLabel 使用列标签代替列名。不同的驱动在这方面会有不同的表现， 具体可参考相关驱动文档或通过测试这两种不同的模式来观察所用驱动的结果。 true | false true useGeneratedKeys 允许 JDBC 支持自动生成主键，需要驱动兼容。 如果设置为 true 则这个设置强制使用自动生成主键，尽管一些驱动不能兼容但仍可正常工作（比如 Derby）。 true | false False autoMappingBehavior 指定 MyBatis 应如何自动映射列到字段或属性。 NONE 表示取消自动映射；PARTIAL 只会自动映射没有定义嵌套结果集映射的结果集。 FULL 会自动映射任意复杂的结果集（无论是否嵌套）。 NONE, PARTIAL, FULL PARTIAL autoMappingUnknownColumnBehavior 指定发现自动映射目标未知列（或者未知属性类型）的行为。 NONE: 不做任何反应 WARNING: 输出提醒日志 (‘org.apache.ibatis.session.AutoMappingUnknownColumnBehavior’ 的日志等级必须设置为 WARN) FAILING: 映射失败 (抛出 SqlSessionException) NONE, WARNING, FAILING NONE defaultExecutorType 配置默认的执行器。SIMPLE 就是普通的执行器；REUSE 执行器会重用预处理语句（prepared statements）； BATCH 执行器将重用语句并执行批量更新。 SIMPLE REUSE BATCH SIMPLE defaultStatementTimeout 设置超时时间，它决定驱动等待数据库响应的秒数。 任意正整数 Not Set (null) defaultFetchSize 为驱动的结果集获取数量（fetchSize）设置一个提示值。此参数只可以在查询设置中被覆盖。 任意正整数 Not Set (null) safeRowBoundsEnabled 允许在嵌套语句中使用分页（RowBounds）。如果允许使用则设置为false。 true | false False safeResultHandlerEnabled 允许在嵌套语句中使用分页（ResultHandler）。如果允许使用则设置为false。 true | false True mapUnderscoreToCamelCase 是否开启自动驼峰命名规则（camel case）映射，即从经典数据库列名 A_COLUMN 到经典 Java 属性名 aColumn 的类似映射。 true | false False localCacheScope MyBatis 利用本地缓存机制（Local Cache）防止循环引用（circular references）和加速重复嵌套查询。 默认值为 SESSION，这种情况下会缓存一个会话中执行的所有查询。 若设置值为 STATEMENT，本地会话仅用在语句执行上，对相同 SqlSession 的不同调用将不会共享数据。 SESSION | STATEMENT SESSION jdbcTypeForNull 当没有为参数提供特定的 JDBC 类型时，为空值指定 JDBC 类型。 某些驱动需要指定列的 JDBC 类型，多数情况直接用一般类型即可，比如 NULL、VARCHAR 或 OTHER。 JdbcType 常量. 大多都为: NULL, VARCHAR and OTHER OTHER lazyLoadTriggerMethods 指定哪个对象的方法触发一次延迟加载。 用逗号分隔的方法列表。 equals,clone,hashCode,toString defaultScriptingLanguage 指定动态 SQL 生成的默认语言。 一个类型别名或完全限定类名。 org.apache.ibatis.scripting.xmltags.XMLLanguageDriver defaultEnumTypeHandler 指定 Enum 使用的默认 TypeHandler 。 (从3.4.5开始) 一个类型别名或完全限定类名。 org.apache.ibatis.type.EnumTypeHandler callSettersOnNulls 指定当结果集中值为 null 的时候是否调用映射对象的 setter（map 对象时为 put）方法，这对于有 Map.keySet() 依赖或 null 值初始化的时候是有用的。注意基本类型（int、boolean等）是不能设置成 null 的。 true | false false returnInstanceForEmptyRow 当返回行的所有列都是空时，MyBatis默认返回null。 当开启这个设置时，MyBatis会返回一个空实例。 请注意，它也适用于嵌套的结果集 (i.e. collectioin and association)。（从3.4.2开始） true | false false logPrefix 指定 MyBatis 增加到日志名称的前缀。 任何字符串 Not set logImpl 指定 MyBatis 所用日志的具体实现，未指定时将自动查找。 查找顺序：SLF4J | LOG4J | LOG4J2 | JDK_LOGGING | COMMONS_LOGGING | STDOUT_LOGGING | NO_LOGGING Not set proxyFactory 指定 Mybatis 创建具有延迟加载能力的对象所用到的代理工具。 CGLIB | JAVASSIST JAVASSIST (MyBatis 3.3 or above) vfsImpl 指定VFS的实现 自定义VFS的实现的类全限定名，以逗号分隔。 Not set useActualParamName 允许使用方法签名中的名称作为语句参数名称。 为了使用该特性，你的工程必须采用Java 8编译，并且加上-parameters选项。（从3.4.1开始） true | false true configurationFactory 指定一个提供Configuration实例的类。 这个被返回的Configuration实例用来加载被反序列化对象的懒加载属性值。 这个类必须包含一个签名方法static Configuration getConfiguration(). (从 3.2.3 版本开始) 类型别名或者全类名. Not set typeAliases类型别名 记得我们在***Mapper.xml配置resultMap节点时需要配置type即需要映射的实体类类型，但是项目中一般实体类都在相同的目录下，这样写太过繁杂于是就有了简化的配置就别名配置，例如：123456789 &lt;typeAliases&gt; &lt;typeAlias alias=\"Blog\" type=\"domain.blog.Blog\"/&gt; &lt;/typeAliases&gt;``` &gt; 也可以配置包名,Mybatis会自动在包名下搜索需要的java bean 对象 ```xml &lt;typeAliases&gt; &lt;package name=\"domain.blog\"/&gt; &lt;/typeAliases&gt; typeHandlers类型转换器 mybatis在预处理语句中设置参数和从结果集中取出参数时都会使用类型转换器将获取的值以合适的方式转换为java类型。mybatis内置了大部分的类型转换器。这些类型转换器实现了TypeHandler接口，如果我们需要自己实现自定义的类型转换器，可以实现该接口，但是这个接口方法太多，我们一般继承BaseTypeHandler抽象类即可。例如我们实现一个枚举类型的转换器1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162 public interface HasIndexEnum &#123; int getIndex(); &#125; public class EnumTypeHandler&lt;T extends HasIndexEnum&gt; extends BaseTypeHandler&lt;T&gt; &#123; private Map&lt;Integer, T&gt; enumCache = new HashMap&lt;&gt;(); public EnumTypeHandler(Class&lt;T&gt; clazz) &#123; if (!Enum.class.isAssignableFrom(clazz) &amp;&amp; HasIndexEnum.class.isAssignableFrom(clazz)) &#123; throw new UnsupportedOperationException(\"Class shound be enum and implements HasIndexEnum.class!\"); &#125; T[] constants = clazz.getEnumConstants(); for (T e : constants) &#123; enumCache.put(e.getIndex(), e); &#125; &#125; @Override public void setNonNullParameter(PreparedStatement ps, int i, T parameter, JdbcType jdbcType) throws SQLException &#123; ps.setInt(i, parameter.getIndex()); &#125; @Override public T getNullableResult(ResultSet rs, String columnName) throws SQLException &#123; Object value = rs.getObject(columnName); if (rs.wasNull()) &#123; return null; &#125; return convertOrException(value); &#125; @Override public T getNullableResult(ResultSet rs, int columnIndex) throws SQLException &#123; Object value = rs.getObject(columnIndex); if (rs.wasNull()) &#123; return null; &#125; return convertOrException(value); &#125; @Override public T getNullableResult(CallableStatement cs, int columnIndex) throws SQLException &#123; Object value = cs.getObject(columnIndex); if (cs.wasNull()) &#123; return null; &#125; return convertOrException(value); &#125; private T convertOrException(Object value) &#123; T e = this.enumCache.get((Integer) value); if (null == e) &#123; throw new IllegalArgumentException(\"Cannot convert \" + value); &#125; else &#123; return e; &#125; &#125; &#125;``` &gt; 然后在mybatis-config.xml中配置该转换器 1234567891011121314151617181920212223242526### objectFactory对象工厂 &gt; 不怎么常用，略 ### plugins插件 &gt; MyBatis 允许你在已映射语句执行过程中的某一点进行拦截调用。默认情况下，MyBatis 允许使用插件来拦截的方法调用包括： &gt; * Executor (update, query, flushStatements, commit, rollback, getTransaction, close, isClosed) &gt; * ParameterHandler (getParameterObject, setParameters) &gt; * ResultSetHandler (handleResultSets, handleOutputParameters) &gt; * StatementHandler (prepare, parameterize, batch, update, query)&gt; 通过 MyBatis 提供的强大机制，使用插件是非常简单的，只需实现 &lt;code&gt;Interceptor&lt;/code&gt; 接口，并指定想要拦截的方法签名即可 ```java // ExamplePlugin.java @Intercepts(&#123;@Signature( type= Executor.class, method = \"update\", args = &#123;MappedStatement.class,Object.class&#125;)&#125;) public class ExamplePlugin implements Interceptor &#123; public Object intercept(Invocation invocation) throws Throwable &#123; return invocation.proceed(); &#125; public Object plugin(Object target) &#123; return Plugin.wrap(target, this); &#125; public void setProperties(Properties properties) &#123; &#125; &#125; 1234567891011121314151617181920212223242526272829303132333435363738394041 &lt;!-- mybatis-config.xml --&gt; &lt;plugins&gt; &lt;plugin interceptor=\"org.mybatis.example.ExamplePlugin\"&gt; &lt;property name=\"someProperty\" value=\"100\"/&gt; &lt;/plugin&gt; &lt;/plugins&gt;``` &gt; 像一些分页插件，日志插件，分表插件都是基于插件实现的，后面我们会做详细的原理介绍，在此不作过多赘述。 ### environments 配置环境 &gt; mybatis与spring整合后，该配置就变得十分简单，在此不做介绍了。 ### mappers(映射器)&gt; 所谓的映射器就是指明我们编写的mapper.xml sql文件的位置的，便于mybatis去做关联。 &gt; 我们可以配置具体的文件，也可以配置整个包路径，同时还支持表达式符号，例如： ```xml &lt;!-- 使用相对于类路径的资源引用 --&gt; &lt;mappers&gt; &lt;mapper resource=\"org/mybatis/builder/AuthorMapper.xml\"/&gt; &lt;mapper resource=\"org/mybatis/builder/BlogMapper.xml\"/&gt; &lt;mapper resource=\"org/mybatis/builder/PostMapper.xml\"/&gt; &lt;/mappers&gt; &lt;!-- 使用完全限定资源定位符（URL） --&gt; &lt;mappers&gt; &lt;mapper url=\"file:///var/mappers/AuthorMapper.xml\"/&gt; &lt;mapper url=\"file:///var/mappers/BlogMapper.xml\"/&gt; &lt;mapper url=\"file:///var/mappers/PostMapper.xml\"/&gt; &lt;/mappers&gt; &lt;!-- 使用映射器接口实现类的完全限定类名 --&gt; &lt;mappers&gt; &lt;mapper class=\"org.mybatis.builder.AuthorMapper\"/&gt; &lt;mapper class=\"org.mybatis.builder.BlogMapper\"/&gt; &lt;mapper class=\"org.mybatis.builder.PostMapper\"/&gt; &lt;/mappers&gt; &lt;!-- 将包内的映射器接口实现全部注册为映射器,要求包名与xml文件的目录名一致 --&gt; &lt;mappers&gt; &lt;package name=\"org.mybatis.builder\"/&gt; &lt;/mappers&gt; 到此Mybatis的配置文件就介绍完了，下一节我们开始讨论学习Mybatis 的执行流程和原理","tags":[{"name":"Mybatis","slug":"Mybatis","permalink":"https://cnkeep.github.io/tags/Mybatis/"}]},{"title":"pom引入lib目录下jar","date":"2018-06-12T08:29:00.000Z","path":"2018/06/12/04-pom引入lib目录下jar/","text":"pom文件引入lib目录下jar介绍当我们在使用springboot带给我们的便利的同时，也偶尔会遇到一些问题。比如无需安装而引入项目lib目录下的jar, 这可以通过pom配置做到 1234567&lt;dependency&gt; &lt;groupId&gt;htmlunit&lt;/groupId&gt; &lt;artifactId&gt;htmlunit&lt;/artifactId&gt; &lt;version&gt;2.21-OSGi&lt;/version&gt; &lt;scope&gt;system&lt;/scope&gt; &lt;systemPath&gt;$&#123;project.basedir&#125;/libs/htmlunit-2.21-OSGi.jar&lt;/systemPath&gt; &lt;/dependency&gt;","tags":[{"name":"others","slug":"others","permalink":"https://cnkeep.github.io/tags/others/"}]},{"title":"从实例入门","date":"2018-06-11T22:52:00.000Z","path":"2018/06/12/02-从实例入门/","text":"从Mybatis实例看原理[目录] 前言mybatis结构写一个Hello Mybatis与Spring集成 前言 上一节针对Mybatis中主要的几个类进行了列举，我们再来回顾一下 Configuration: Mybatis的配置信息都在该类中 SqlSession: 作为MyBatis工作的主要顶层API，表示和数据库交互的会话，完成必要数据库增删改查功能，类似于connection Executor: 执行器，是Mybatis 调度的核心，负责SQL语句的生成和查询缓存的维护 StatementHandler: 封装了JDBC Statement操作，负责对JDBC statement 的操作，如设置参数、将Statement结果集转换成List集合。 ParameterHandler: 负责将用户传递的参数转换成JDBC statement锁需要的参数。 ResultHandler: 负责将JDBC返回的ResultSet结果集装换为List集合 TypeHandler: 负责java数据类型和JDBC数据类型之间的映射和转化 MappedStatement: MappedStatement维护了一条&lt;select|update|delete|insert&gt;节点的封装 SqlSource: 负责根据用户传递的parameterObject，动态地生成SQL语句，将信息封装到BoundSql对象中，并返回 BoundSql: 表示动态生成的SQL语句以及相应的参数信息 mybatis结构介绍mybatis的架构图: Mybatis的功能架构分为三层： API接口层：提供给外部使用的接口API，开发人员通过这些本地API来操纵数据库。接口层一接收到调用请求就会调用数据处理层来完成具体的数据处理。 数据处理层：负责具体的SQL查找、SQL解析、SQL执行和执行结果映射处理等。它主要的目的是根据调用的请求完成一次数据库操作。 基础支撑层：负责最基础的功能支撑，包括连接管理、事务管理、配置加载和缓存处理，这些都是共用的东西，将他们抽取出来作为最基础的组件。为上层的数据处理层提供最基础的支撑。 MyBatis整个项目的包结构如下： 1.annotation 本包定义了Mybatis框架中的24个注解: Cache,Mapper,Select等。 2.binding 映射绑定Mapper接口与mapper.xml。 3.builder 解析Mybatis的配置文件和映射文件，包括Xml格式和Annotation格式2种配置。 4.cache 本包包含了Mybatis框架的缓存接口定义和实现。 缓存实现为PerpetualCache类，它直接实现了Cache接口，其它缓存类实现采用装饰模式实现。一个个包装起来，形成一个链，典型的就是SynchronizedCache-&gt;LoggingCache-&gt;SerializedCache-&gt;LruCache-&gt;PerpetualCache，通过链起来达到功能增加。 缓存框架按照 Key-Value方式存储，Key的生成采取规则为：[hashcode:checksum:mappedStementId:offset:limit:executeSql:queryParams]。 5.datasource 数据源相关接口和类。 6.exceptions 本包定义了Mybatis框架中的异常。 7.executor Mybatis最后中的四个接口(Executor,StatementHandler,ParameterHandler,ResultSetHandler)以及实现。 8.io 本包主要包含了资源加载和访问相关的类。 9.jdbc JDBC和SQL相关的类。 10.logging 把日志抽象成Log接口，该接口有7种实现。 1.Apache Commons Logging 2.JDBC Logging 3.Java Util Logging 4.Log4j 5.No Logging 6.Slf4J 7.Stdout 11.mapping Mybatis配置文件, 映射文件相关的类。 12.ognl ongl表达式处理 13.parsing 解析配置文件的核心类和接口。 14.plugin 插件相关接口和类。 15.reflection 反射处理相关类。 16.scripting 脚本解析相关类。 17.session 会话相关类，提供对外核心接口. 18.transaction Transaction接口是对事务的抽象，有2种实现方式： 1.JdbcTransaction,jdbc:手动管理 2.ManagedTransaction,managed:container manage the full lifecycle of the transaction TransactionFactory接口定义了生成Transaction接口(实现类)的若干方法。 该接口有2种实现方式： 1.JdbcTransactionFactory,Creates {@link JdbcTransaction} instances。 2.ManagedTransactionFactory，Creates {@link ManagedTransaction} instances。 本包主要依赖了Mybatis session包的TransactionIsolationLevel和exceptions包的PersistenceException。 Mybatis的其它包大量引用了本包中的类和接口，即严重依赖于本包。 19.type 类型转换处理，包含了类型处理器接口TypeHandler，父类BaseTypeHandler,以及若干个子类。 Hello Mybatis 还是老套路，先来玩一玩Mybatis，看一个Demo参照官网,需要以下几个步骤：1.引入依赖12345&lt;dependency&gt; &lt;groupId&gt;org.mybatis&lt;/groupId&gt; &lt;artifactId&gt;mybatis&lt;/artifactId&gt; &lt;version&gt;x.x.x&lt;/version&gt;&lt;/dependency&gt; 2.配置mybatis-config.xml123456789101112131415161718&lt;?xml version=\"1.0\" encoding=\"UTF-8\" ?&gt;&lt;!DOCTYPE configuration PUBLIC \"-//mybatis.org//DTD Config 3.0//EN\" \"http://mybatis.org/dtd/mybatis-3-config.dtd\"&gt;&lt;configuration&gt; &lt;environments default=\"development\"&gt; &lt;environment id=\"development\"&gt; &lt;transactionManager type=\"JDBC\"/&gt; &lt;dataSource type=\"POOLED\"&gt; &lt;property name=\"driver\" value=\"$&#123;driver&#125;\"/&gt; &lt;property name=\"url\" value=\"$&#123;url&#125;\"/&gt; &lt;property name=\"username\" value=\"$&#123;username&#125;\"/&gt; &lt;property name=\"password\" value=\"$&#123;password&#125;\"/&gt; &lt;/dataSource&gt; &lt;/environment&gt; &lt;/environments&gt; &lt;mappers&gt; &lt;mapper resource=\"org/mybatis/example/BlogMapper.xml\"/&gt; &lt;/mappers&gt;&lt;/configuration&gt; 3.编写Mapper接口注：该接口不用提供实现类，只是提供一个映射关系，mybatis会自动解析该映射关系来调用相关的sql语句12345package org.mybatis.example;public interface BlogMapper&#123; Blog selectBlog(@org.apache.ibatis.annotations.Param(\"id\")Integer id);&#125; 4.编写mapper.xml注意：该xml文件需要在配置文件中的mappers节点下注册1234567&lt;?xml version=\"1.0\" encoding=\"UTF-8\" ?&gt;&lt;!DOCTYPE mapper PUBLIC \"-//mybatis.org//DTD Mapper 3.0//EN\" \"http://mybatis.org/dtd/mybatis-3-mapper.dtd\"&gt;&lt;mapper namespace=\"org.mybatis.example.BlogMapper\"&gt; &lt;select id=\"selectBlog\" resultType=\"Blog\"&gt; select * from Blog where id = #&#123;id&#125; &lt;/select&gt;&lt;/mapper&gt; 5.加载配置文件,构建SqlSessionFactory123String resource = \"org/mybatis/example/mybatis-config.xml\";InputStream inputStream = Resources.getResourceAsStream(resource);SqlSessionFactory sqlSessionFactory = new SqlSessionFactoryBuilder().build(inputStream); 6.从SqlSessionFactory中获取会话SqlSession12345678910111213141516// 采用直接指定sqlid的方式SqlSession session = sqlSessionFactory.openSession();try &#123; Blog blog = (Blog) session.selectOne(\"org.mybatis.example.BlogMapper.selectBlog\", 101);&#125; finally &#123; session.close();&#125;//采用接口的方式SqlSession session = sqlSessionFactory.openSession();try &#123; BlogMapper mapper = session.getMapper(BlogMapper.class); Blog blog = mapper.selectBlog(101);&#125; finally &#123; session.close();&#125; 自此一个简单的Demo就完成了 与Spring集成 在开发中我们一般不会单独使用Mybatis,都是利用Spring强大的IOC和AOP来帮助我们快速的构建项目，我们来看看如何与Spring集成先看看版本之间的对应关系1.添加依赖12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061 &lt;!-- 导入database依赖,使用druid作连接池 --&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;druid&lt;/artifactId&gt; &lt;version&gt;1.1.3&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.mybatis&lt;/groupId&gt; &lt;artifactId&gt;mybatis-spring&lt;/artifactId&gt; &lt;version&gt;1.3.1&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;5.1.44&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.mybatis&lt;/groupId&gt; &lt;artifactId&gt;mybatis&lt;/artifactId&gt; &lt;version&gt;3.4.1&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-jdbc&lt;/artifactId&gt; &lt;version&gt;4.3.10.RELEASE&lt;/version&gt; &lt;/dependency&gt;``` &gt; 2.配置SqlSessionFactory &gt; 在spring的配置文件application-context.xml中配置如下内容 ```xml &lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt; &lt;!-- 引入配置文件 --&gt; &lt;beans xmlns=\"http://www.springframework.org/schema/beans\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd\"&gt; &lt;!-- 引入配置文件 --&gt; &lt;bean id=\"propertyConfigurer\" class=\"org.springframework.beans.factory.config.PropertyPlaceholderConfigurer\"&gt; &lt;property name=\"location\" value=\"classpath:/mybatis/db.properties\"/&gt; &lt;/bean&gt; &lt;!-- dataSource配置 --&gt; &lt;bean id=\"dataSource\" class=\"com.alibaba.druid.pool.DruidDataSource\" init-method=\"init\" destroy-method=\"close\"&gt; &lt;property name=\"driverClassName\" value=\"$&#123;jdbc.driverClassName&#125;\"/&gt; &lt;property name=\"url\" value=\"$&#123;jdbc.url&#125;\"/&gt; &lt;property name=\"username\" value=\"$&#123;jdbc.username&#125;\"/&gt; &lt;property name=\"password\" value=\"$&#123;jdbc.password&#125;\"/&gt; &lt;property name=\"filters\" value=\"log4j\"/&gt; &lt;property name=\"maxActive\" value=\"5\"/&gt; &lt;property name=\"initialSize\" value=\"1\"/&gt; &lt;property name=\"maxWait\" value=\"6000\"/&gt; &lt;/bean&gt; &lt;!-- mybatis配置,mapper.xml文件扫描 --&gt; &lt;bean id=\"sessionFactory\" class=\"org.mybatis.spring.SqlSessionFactoryBean\"&gt; &lt;property name=\"configLocation\" value=\"classpath:/mybatis/mybatis-config.xml\"/&gt; &lt;property name=\"dataSource\" ref=\"dataSource\"/&gt; &lt;/bean&gt; &lt;/beans&gt; 3.使用`java package com.company.repo; import org.apache.ibatis.session.; import org.springframework.beans.factory.annotation.; /** * 抽象 */ public abstract class AbstractRepository{ protected SqlSessionFactory sqlSessionFactory; @Autowired @Qualifier(&quot;sqlSessionFactory&quot;) public void setSqlSessionFacto(SqlSessionFactory sqlSessionFactory){ this.sqlSessionFactory = sqlSessionFactory; } } ```java @org.springframework.stereotype.Repository public class BlogRepository extends AbstactRepository{ public Blog findById(Integer id){ try(org.apache.ibatis.session.SqlSession sqlSession = sqlSessionFactory.openSession()){ BlogMapper mapper = sqlSession.getMapper(BlogMapper.class); return mapper.findById(id); } } } 这样一个与Spring集成，简单的使用实例就完成了，复杂的就看各位的了。下一节将针对配置文件mybatis-config.xml进行说明","tags":[{"name":"Mybatis","slug":"Mybatis","permalink":"https://cnkeep.github.io/tags/Mybatis/"}]},{"title":"入门介绍","date":"2018-06-11T18:40:00.000Z","path":"2018/06/12/01-入门介绍/","text":"Mybatis入门介绍应用场景 最开始学习jdbc知识的时候，考虑一下我们是如何编写程序的？大概分为几个步骤: 配置基本参数 获取连接资源 编译sql 设置参数 执行sql 获取查询结果集 遍历结果集 释放连接资源 我们写个最简单的例子看一下：`javaimport java.sql.;import java.util.; public class JDBCUtil{ private static String url; private static String username; private static String pwd; private static String driverClass; //step 1: static { url = &quot;jdbc:mysql://127.0.0.1:3306/jdbc?useUnicode=true&amp;characterEncoding=UTF-8&amp;autoReconnect=true&amp;failOverReadOnly=false&quot;; username = &quot;root&quot;; pwd = &quot;123456&quot;; driverClass = &quot;com.mysql.jdbc.Driver&quot;; } public static Connection getConnection() throws ClassNotFoundException, SQLException { Class.forName(driverClass); return java.sql.DriverManager.getConnection(url, username, pwd); } /** * 查询接口 * * @param sql 执行的查询sql,参数使用?占位 * @param params 占位符对应的参数列表 * @return List&lt;Map&lt;String,Object&gt;&gt; 查询结果，每行数据采用map存储，key=columnName,value = columnValue */ public static List&lt;Map&lt;String,Object&gt;&gt; query(String sql, java.util.List&lt;Object&gt; params) throws SQLException, ClassNotFoundException { //step 2: java.sql.Connection connection = getConnection(); //step 3: // 这里使用预编译的statement，防止sql注入 java.sql.PreparedStatement preparedStatement = connection.prepareStatement(sql); //step 4: // 设置参数 invokeParams(preparedStatement, params); //step 5 and 6: java.sql.ResultSet resultSet = preparedStatement.executeQuery(); //step 7: // 解析查询结果 List&lt;Map&lt;String,Object&gt;&gt; result = parseResult(resultSet); //step 8: // 关闭资源 close(resultSet, preparedStatement, connection); return result; } /** * 参数设置 * * @param preparedStatement * @param params * @throws SQLException */ public static void invokeParams(java.sql.PreparedStatement preparedStatement, java.util.List params) throws SQLException { if (null == preparedStatement || null == params) throw new IllegalArgumentException(&quot;[NullPointerException],params can not be null&quot;); ParameterMetaData parameterMetaData = preparedStatement.getParameterMetaData(); int parameterCount = parameterMetaData.getParameterCount(); if (parameterCount &lt; params.size()) { throw new IllegalArgumentException(&quot;params&apos;s size is worn, the size should be &quot; + parameterCount); } for (int index = 1; index &lt;= parameterCount; index++) { preparedStatement.setObject(index, params.get(index-1)); } } /** * 结果解析 * * @param resultSet */ private static List&lt;Map&lt;String, Object&gt;&gt; parseResult(ResultSet resultSet) throws SQLException { List&lt;Map&lt;String, Object&gt;&gt; resultList = new LinkedList&lt;Map&lt;String, Object&gt;&gt;(); ResultSetMetaData metaData = resultSet.getMetaData(); int columnCount = metaData.getColumnCount(); while (resultSet.next()) { Map&lt;String, Object&gt; rowData = new HashMap&lt;String, Object&gt;(); for (int index = 1; index &lt;= columnCount; index++) { String key = metaData.getColumnLabel(index); Object value = resultSet.getObject(index); rowData.put(key, value); } resultList.add(Collections.unmodifiableMap(rowData)); } return resultList; } private static void close(ResultSet resultSet, PreparedStatement preparedStatement, Connection connection) { try { if (null != resultSet) resultSet.close(); if (null != preparedStatement) preparedStatement.close(); if (null != connection) connection.close(); } catch (SQLException e) { e.printStackTrace(); } } public static void main(String[] args) throws SQLException, ClassNotFoundException { String sql = &quot;select * from customer where id=?&quot;; List&lt;Object&gt; params = new LinkedList&lt;Object&gt;(); params.add(1); List&lt;Map&lt;String, Object&gt;&gt; query = query(sql, params); System.out.println(query); } } ` 我们可以看到就简单的查询就这么多复杂的代码，既要连接数据库，又要设置参数，遍历结果集，而且其中很大一部分都是通用的，每次都写太麻烦了。有人就想到写一些工具类,来处理这些事情，类似于模板模式，同时在利用反射机制，就可以直接注入对象，结果集也可以直接反射成对象。完美！这种工具有一个高大上的名称叫ORM(对象关系映射)，Mybatis就是这么一个工具，节省开发工作量。 利弊 比较常用的ORM框架就是Hibernate和Mybatis,那么各自有什么优缺点呢？ Hibernate Hibernate的DAO层开发比MyBatis简单，Mybatis需要维护SQL和结果映射。 Hibernate对对象的维护和缓存要比MyBatis好，对增删改查的对象的维护要方便。 Hibernate数据库移植性很好，MyBatis的数据库移植性不好，不同的数据库需要写不同SQL。 Hibernate有更好的二级缓存机制，可以使用第三方缓存。MyBatis本身提供的缓存机制不佳。 强大、方便、高效、复杂、间接、全自动化 Mybatis Mybatis可以进行更为细致的SQL优化，可以减少查询字段。 Mybatis容易掌握，而Hibernate门槛较高。 小巧、方便、高效、简单、直接、半自动化 主要的几个类 Configuration: Mybatis的配置信息都在该类中 SqlSession: 作为MyBatis工作的主要顶层API，表示和数据库交互的会话，完成必要数据库增删改查功能，类似于connection Executor: 执行器，是MyBatis 调度的核心，负责SQL语句的生成和查询缓存的维护 StatementHandler: 封装了JDBC Statement操作，负责对JDBC statement 的操作，如设置参数、将Statement结果集转换成List集合。 ParameterHandler: 负责将用户传递的参数转换成JDBC statement锁需要的参数。 ResultHandler: 负责将JDBC返回的ResultSet结果集装换为List集合 TypeHandler: 负责java数据类型和JDBC数据类型之间的映射和转化 MappedStatement: MappedStatement维护了一条&lt;select|update|delete|insert&gt;节点的封装 SqlSource: 负责根据用户传递的parameterObject，动态地生成SQL语句，将信息封装到BoundSql对象中，并返回 BoundSql: 表示动态生成的SQL语句以及相应的参数信息","tags":[{"name":"Mybatis","slug":"Mybatis","permalink":"https://cnkeep.github.io/tags/Mybatis/"}]},{"title":"catalog","date":"2018-06-11T16:12:00.000Z","path":"2018/06/12/00-catalog/","text":"路线： 入门介绍 重要的几个接口： Configuration: Mybatis的配置信息都在该类中 SqlSession: 作为MyBatis工作的主要顶层API，表示和数据库交互的会话，完成必要数据库增删改查功能，类似于connection Executor: 执行器，是MyBatis 调度的核心，负责SQL语句的生成和查询缓存的维护 StatementHandler: 封装了JDBC Statement操作，负责对JDBC statement 的操作，如设置参数、将Statement结果集转换成List集合。 ParameterHandler: 负责将用户传递的参数转换成JDBC statement锁需要的参数。 ResultHandler: 负责将JDBC返回的ResultSet结果集装换为List集合 TypeHandler: 负责java数据类型和JDBC数据类型之间的映射和转化 MappedStatement: MappedStatement维护了一条&lt;select|update|delete|insert&gt;节点的封装 SqlSource: 负责根据用户传递的parameterObject，动态地生成SQL语句，将信息封装到BoundSql对象中，并返回 BoundSql: 表示动态生成的SQL语句以及相应的参数信息 实例操作，多库连接，动态创建SqlSessionFactory,与spring整合 提供Demo 配置文件mybatis-config.xml,内部结点的作用，顺序 执行流程，各个相关类 MapperProxy.invoke(…) Mapper接口的代理类, MapperMethod.execute(…) 按照sql类型分类执行 DefaultSqlSession.selectList(…) Executor.query(…) parameterHandler.parameterize()//设置参数 查询缓存是否存在 Executor.doQuery(…) statementHandler.query(…) prepareStatement.execute();//jdbc执行 resultSetHandler.handleResultSets() 处理结果 mapper.xml节点学习，动态sql 拓展：缓存 插件，分库分表，日志 个人实践：分页，分表，日志，TypeHandler抽象 总结，设计模式 大量的jdk静态代理使用，Plugin类似于洋葱模型生成代理模式 Builder模式，例如SqlSessionFactoryBuilder、XMLConfigBuilder、XMLMapperBuilder、XMLStatementBuilder、CacheBuilder； 工厂模式，例如SqlSessionFactory、ObjectFactory、MapperProxyFactory； 单例模式，例如ErrorContext和LogFactory； 代理模式，Mybatis实现的核心，比如MapperProxy、ConnectionLogger，用的jdk的动态代理；还有executor.loader包使用了cglib或者javassist达到延迟加载的效果； 组合模式，例如SqlNode和各个子类ChooseSqlNode等； 模板方法模式，例如BaseExecutor和SimpleExecutor，还有BaseTypeHandler和所有的子类例如IntegerTypeHandler； 适配器模式，例如Log的Mybatis接口和它对jdbc、log4j等各种日志框架的适配实现； 装饰者模式，例如Cache包中的cache.decorators子包中等各个装饰者的实现； 迭代器模式，例如迭代器模式PropertyTokenizer；","tags":[{"name":"Mybatis","slug":"Mybatis","permalink":"https://cnkeep.github.io/tags/Mybatis/"}]},{"title":"bower使用","date":"2018-06-11T07:58:00.000Z","path":"2018/06/11/03-bower使用/","text":"bower使用1.介绍在做前端相关的系统时，少不了各种js, css的库文件引用，各种版本的额依赖关系管理也是让人头疼，Linux有yum管理工具，那bower也是同样的包管理工具，用于管理前端各种的版本依赖和下载。 2.使用2.1bower安装 需要系统已安装nodejs1npm install bower -g 2.2基础命令 基础命令可以通过bower help查看 命令简介： cache: bower 缓存管理 help: 显示 Bower 命令的帮助信息 home: 通过浏览器打开一个包的 github 发布页 info: 查看包的信息 init: 创建 bower.json 文件 install: 安装包到项目 link: 在本地 bower 库建立一个项目链接 list: 列出项目已安装的包 lookup: 根据包名查询包的 URL prune: 删除项目无关的包 register: 注册一个包 search: 搜索包 update: 更新项目的包 uninstall: 删除项目的包 2.3使用bower下载库作为各种库的管理工具，bower 主要就是对它们下载和管理。举例，我们新建了一个项目，我们的项目需要如下前端库：jquery 1.11.1、bootstrap。针对项目我们可以依次执行如下命令：12bower install jquery#1.11.1 -savebower install bootstrap -save 执行完命令后，bower都会自动将对应的库文件下载到bower_components/目录下(包含依赖库) 2.4bower.json`文件我们在项目中使用时，因为库文件众多，都会采取配置文件指定的方式。可以通过bower init生成改配置文件：12345678910111213141516171819202122232425262728293031323334353637383940414243&#123; \"name\":\"\", //必须，如果需要注册包，则该包名唯一。 \"description\":\"\", //可选，包描述 \"main\":[], //可选，入口文件，bower本身不使用，供第三方构建工具会使用 //每种文件类型只能有一个。 \"ignore\":[], //可选，文件或目录列表。bower安装的时候将忽略该列表中的文件。 //bower是从git仓库或压缩包下载一个包，里面的文件并不一定全部需要。 \"dependencies\":[], //依赖包，name:value，value可以是包的semver //range(版本号范围)，也可以直接是一个包的git地址或压缩包地址。 \"devDependencies\":[], //开发依赖包，仅仅在开发过程中测试或者编译文档用，部署生产环境是不需要。 //格式和dependencies 相同 \"resolutions\":[], //包引用冲突自动使用该模块指定的包版本 //格式和dependencies 相同 \"overrides\" :[ //这个也很关键，可以覆盖一个包中的默认设置，比如main里面设定的入口文件 \"package-name\":&#123; //这样可以根据需要，让第三方工具只打包需要的文件。 \"main\":[] &#125; ], \"moduleType\":\"\", //可选，指定包采用那种模块化方式(globals,amd,node,es6,yui) \"private\":Boolean, //是否公开发布当前包,如果只是使用bower来管理项目的包，设置为true. \"license\":\"\", //授权方式(GPL-3.0,CC-BY-4.0.....) \"keywords\":[], //可选，方便注册后容易被其他人搜索到。 \"authors\":[], //作者列表 \"homepage\":[], //主页，包介绍页 \"repository\":&#123; //包所在仓库。 \"type\": \"git\", \"url\": \"git://github.com/foo/bar.git\" &#125;,｝ 版本的指定 ~1.2.3 ~1.2 ~1 ~代表最小的补丁更新范围 ~1.2.3 := &gt;=1.2.3 &lt;1.(2+1).0 := &gt;=1.2.3 &lt;1.3.0 ~1.2 := &gt;=1.2.0 &lt;1.(2+1).0 := &gt;=1.2.0 &lt;1.3.0 (Same as 1.2.x) ~1 := &gt;=1.0.0 &lt;(1+1).0.0 := &gt;=1.0.0 &lt;2.0.0 (Same as 1.x) ~0.2.3 := &gt;=0.2.3 &lt;0.(2+1).0 := &gt;=0.2.3 &lt;0.3.0 ~0.2 := &gt;=0.2.0 &lt;0.(2+1).0 := &gt;=0.2.0 &lt;0.3.0 (Same as 0.2.x) ~0 := &gt;=0.0.0 &lt;(0+1).0.0 := &gt;=0.0.0 &lt;1.0.0 (Same as 0.x) #: 代表具体的版本号 ^1.2.3 ^0.2.5 ^0.0.4§ ^ 代表从左开始第一个非零数为基础版本 ^1.2.3 := &gt;=1.2.3 &lt;2.0.0 ^0.2.3 := &gt;=0.2.3 &lt;0.3.0 ^0.0.3 := &gt;=0.0.3 &lt;0.0.4","tags":[{"name":"others","slug":"others","permalink":"https://cnkeep.github.io/tags/others/"}]},{"title":"安装jar到本地","date":"2018-06-11T06:21:00.000Z","path":"2018/06/11/02-安装jar到本地/","text":"手动安装jar到本地仓库12345678910111213141516171819&lt;dependency&gt; &lt;groupId&gt;com.oracle&lt;/groupId&gt; &lt;artifactId&gt;ojdbc14&lt;/artifactId&gt; &lt;version&gt;10.2.0.1.0&lt;/version&gt;&lt;/dependency&gt;#执行命令#-Dfile=jar包存放位置#-DgroupId=&lt;groupId&gt;#-DartifactId=&lt;artifactId&gt;#-Dversion=&lt;version&gt;#-Dpackaging=&lt;jar&gt; mvn install:install-file \\-Dfile=C:\\Users\\zll\\Desktop\\ojdbc14-10.2.0.1.0.jar \\-DgroupId=com.oracle \\-DartifactId=ojdbc14 \\-Dversion=10.2.0.1.0 \\-Dpackaging=jar","tags":[{"name":"others","slug":"others","permalink":"https://cnkeep.github.io/tags/others/"}]},{"title":"正则表达式","date":"2018-06-10T05:19:00.000Z","path":"2018/06/10/01-正则表达式/","text":"正则表达式介绍&nbsp;&nbsp;正则表达式描述了一种字符串匹配的模式，类似于一种通用模型，可以用来检查一个串是否含有某种子串、将匹配的子串替换或者从某个子串中取出符合某个条件的子串等。最近在研究git钩子时有用到，在此做笔记记录一下，如有错误，欢迎指正。 特殊字符所谓特殊字符，就是一些有特殊含义的字符，若要匹配这些特殊字符，必须要先使用反斜杠转义，下面列出正则表达式中的特殊字符： 例如要匹配字符串中是否包含()就需要这样写： 1\\(\\w+\\) 限定符限定符用来指定正则表达式的一个给定组件必须要出现多少次才能满足匹配。有 * 或 + 或 ? 或 {n} 或 {n,} 或 {n,m} 共6种。 正则表达式的限定符有： 定位符定位符使您能够将正则表达式固定到行首或行尾。它们还使您能够创建这样的正则表达式，这些正则表达式出现在一个单词内、在一个单词的开头或者一个单词的结尾。 定位符用来描述字符串或单词的边界，^ 和 $ 分别指字符串的开始与结束，\\b 描述单词的前或后边界，\\B 表示非单词边界。 正则表达式的定位符有： 元符号 示例 字符组1[a-zA-Z0-9]:表示大小写或者数字，区间采用横杠分开 枚举1(fix|feature|doc): 表示枚举fix,feature,doc其中任意一个满足即可 首尾匹配1^a\\w+z$ : ``^``表示匹配头部，``$``表示匹配尾部 git 提交规范 1(fix|feature|doc)+:(\\(\\w+\\))\\s+(\\w+): 满足&lt;type&gt;:(&lt;module&gt;) &lt;digest&gt;","tags":[{"name":"others","slug":"others","permalink":"https://cnkeep.github.io/tags/others/"}]},{"title":"XShell快捷键","date":"2018-04-15T15:52:00.000Z","path":"2018/04/15/006-XShell快捷键/","text":"XShell快捷键 清屏： ctrl+L 切换tab: ctrl+Tab 新建连接： ctrl+O 复制： ctrl+Insert 粘贴： shift+Insert","tags":[{"name":"tools","slug":"tools","permalink":"https://cnkeep.github.io/tags/tools/"}]},{"title":"idea安装lombok插件","date":"2018-04-15T15:29:00.000Z","path":"2018/04/15/005-idea安装lombok插件/","text":"idea安装lombok插件介绍 Lombok是一个可以通过简单的注解形式来帮助我们简化消除一些必须有但显得很臃肿的Java代码的工具，通过使用对应的注解，可以在编译源码的时候生成对应的方法。 官方地址:https://projectlombok.org/ github地址:https://github.com/rzwitserloot/lombok idea安装lombok插件尝试在线安装(本人操作失败) 尝试离线安装 获取资源包 https://github.com/mplushnikov/lombok-intellij-plugin/releases 或者 http://plugins.jetbrains.com/plugin/6317-lombok-plugin 下载对应的版本 安装 依次进入IDEA–&gt;Settings/Preferences–&gt;Plugins 选择install plugin from disk选中下载的插件，等待安装完成即可。 使用引入依赖&lt;dependency&gt; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt; &lt;artifactId&gt;lombok&lt;/artifactId&gt; &lt;version&gt;1.16.18&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt; &lt;/dependency&gt; 使用在对应的累活方法上使用对应的注解即可 Lombok有哪些注解 @Setter @Getter @Data @Log(这是一个泛型注解，具体有很多种形式) @AllArgsConstructor @NoArgsConstructor @EqualsAndHashCode @NonNull @Cleanup @ToString @RequiredArgsConstructor @Value @SneakyThrows @Synchronized 官网介绍","tags":[{"name":"tools","slug":"tools","permalink":"https://cnkeep.github.io/tags/tools/"}]},{"title":"eclipse中maven项目jar包不会自动下载解决办法","date":"2018-04-14T12:04:00.000Z","path":"2018/04/14/004-eclipse中maven项目jar包不会自动下载解决办法/","text":"eclipse中maven项目jar包不会自动下载解决办法 Eclipse中maven从远程仓库中下载jar包有时会很慢，有些甚至进度停止不动，这个时候我们可能会终止当前下载，但是终止jar包下载后会出现一个问题，再次打开Eclipse时，你会发现提示你项目中依赖的jar包找不到. 如下图所示，项目右键打—》Build Path—》Configure Build Path 打开项目的 Java Build Path 在 Libraries 页签下Maven Dependenicies 你会发现报错提示 依赖的jar包 missing如下图所示 此时我们可以通过如下方案解决: 找到我们的本地maven仓库目录 我的是 H:\\Java\\maven\\Repository 搜索出该目录下的*lastUpdated.properties文件并删除，如下图所示，可以通过模糊搜索匹配出这样的文件 Maven 更新当前项目，maven就会继续下载缺失的依赖jar包，直至缺失jar包下载完成，上述问题就解决了。 自动删除脚本 1234567891011# 1. 新建clean.bat# 2. 输入以下内容，修改REPOSITORY_PATH为自己的本地仓库路径# 3. 执行脚本set REPOSITORY_PATH=E:\\maven\\reporem 正在搜索...for /f &quot;delims=&quot; %%i in (&apos;dir /b /s &quot;%REPOSITORY_PATH%\\*lastUpdated&quot;&apos;) do ( del /s /q %%i)rem 搜索完毕pause","tags":[{"name":"tools","slug":"tools","permalink":"https://cnkeep.github.io/tags/tools/"}]},{"title":"windows激活","date":"2018-04-14T08:40:00.000Z","path":"2018/04/14/003-windows激活/","text":"windows激活系统安装完毕后，首先以管理员身份打开CMD命令行窗口，按下Win+X，选择命令提示符(管理员)。说明：kms.xspace.in是kms服务器地址，可能会失效，如果激活失败，可以自行搜索kms服务器地址，将kms.xspace.in替换成新的地址即可，比如换成kms.03k.org123456789win10专业版用户请依次输入：slmgr /ipk W269N-WFGWX-YVC9B-4J6C9-T83GXslmgr /skms kms.xspace.inslmgr /atowin10企业版用户请依次输入：slmgr /ipk NPPR9-FWDCX-D2C8J-H872K-2YT43slmgr /skms kms.xspace.inslmgr /ato 网上搜了一大堆。一点作用没有。结果看到这句话 换了一下还真成功了~将kms.xspace.in替换成新的地址即可，比如换成kms.03k.org 查看到期时间：slmgr.vbs -xpr 来自 https://tieba.baidu.com/p/5592942485?red_tag=0307692371&amp;traceid=","tags":[{"name":"tools","slug":"tools","permalink":"https://cnkeep.github.io/tags/tools/"}]},{"title":"idea快捷键","date":"2018-04-12T08:21:00.000Z","path":"2018/04/12/002-idea快捷键/","text":"Idea快捷键 激活 License Server: http://idea.iteblog.com/key.php 360打开 https://licensez.com/ 快捷键 切换工作空间： ctrl+alt+]/] 搜索类中的方法： ctrl+F12 搜索类： ctrl+N or 双击shift 替换： ctrl+R 全局搜索： ctrl+shift+R 格式化代码： ctrl+alt+L 移除不用的import: ctrl+alt+O 查看注释： ctrl+Q 提示： ctrl+shift+space,alt+/ 建议： alt+enter 调用层次： ctrl+H 重写、覆盖方法： ctrl+o 删除一行： ctrl+x 复制一行： ctrl+V/D 移动一行： alt+shift+up/down 快速surround by: ctrl+alt+T 大小写切换: ctrl+shift+U get/set: alt+insert 去除多余import: ctrl+alt+O 查看最近打开的文件: ctrl+E 自动补全分号，大括号: ctrl+shift+enter 查看方法的实现: ctrl+alt+B","tags":[{"name":"tools","slug":"tools","permalink":"https://cnkeep.github.io/tags/tools/"}]},{"title":"idea配置全局代码注释模板","date":"2018-04-11T07:57:00.000Z","path":"2018/04/11/001-idea配置全局代码注释模板/","text":"idea设置全局注释模板 作为程序员代码要与代码规范，规范中必不可少的就是代码注释了，但是每次都手敲太麻烦，今天我们就来看看如何利用idea来配置自动生成代码注释。 配置新建class时自动生成注释 1.进入setting设置，搜索template关键词，出现下面俩个选项，就是我们要操作的功能： 2.选中File and Code Template, 在右侧Files页面菜单下，找到Class, Interface, Enum, AnnotationType(idea支持直接检索) 3.在最右侧的编辑区，填写需要的注释格式，也可以添加相关变量。 4.新建Class文件，就会发现自动生成了代码注释。 配置快捷键自动生成注释&nbsp;&nbsp;上面的方式只会在新建的Class文件中自动添加注释，但是对已经存在的文件无能为了，需要问么手动添加，我们可以采用全局替换去实现，但是，这里我提供一种快捷键的方式，在引申到自动生成Logger代码段上。 1.进入setting设置，搜索template关键词 2.选中Live Templates 3.点击右侧加号，添加我们自定义的快捷操作： 4.选中Template Group,新建custom分组，再新建Live Template 5.配置相关规则 Abbreviation:填写我们的匹配触发规则，这里填写了/**,即但我们键盘输入/**时会触发生成注释； Description:填写说明，便于查看具体功能； Teamplate Text:填写具体的生成规则 6.配置应用的对象，我么选择最下面的No applicable contexts yet. Define, 选择java 7.Class文件中输入/**然后按Tab键就会生成注释。 配置自动生成Logger对象&nbsp;&nbsp;我们编写业务代码经常要记录日志，每个类中都要写Logger属性，能不能手动生成呢？ 可以的，利用快捷操作，就行输入sout自动生成打印语句一样。 1.进入setting设置，搜索template关键词 2.选中Live Templates 3.点击右侧加号，添加我们自定义的快捷操作： 4.选中Template Group,新建custom分组，再新建Live Template 5.配置相关规则 Abbreviation: 我们填写logger, 只要页面键入logger就会出现提示，回车后就可自动生成Logger对象。 Description:填写说明，便于查看具体功能； Teamplate Text: private static final Logger LOGGER = LoggerFactory.getLogger($CLASS$.class); 6.配置应用的对象，我么选择最下面的No applicable contexts yet. Define, 选择java 7.Class文件中输入logger然后出现提示，回车即可生成Logger对象。","tags":[{"name":"tools","slug":"tools","permalink":"https://cnkeep.github.io/tags/tools/"}]},{"title":"hook钩子","date":"2018-03-16T19:44:00.000Z","path":"2018/03/17/02-hook钩子/","text":"Git中的hook钩子程序原文: Git中文官网-8.3 自定义 Git - Git 钩子 Git钩子&nbsp;&nbsp;Git能在特定的重要动作发生时触发自定义的脚本，俗称钩子(hooks)，类似于事件回调机制。Git中存在两组这样的钩子： 客户端钩子：由诸如提交和合并这样的操作所调用。 服务端钩子：作用域诸如接收被推送的提交这样的联网操作。 安装钩子&nbsp;&nbsp;当我们clone或者init一个仓库后，会在目录下生成.git目录，该目录即我们的git管理目录，其内部存在一个hooks的子目录，该目录下默认防止了一些以.sample结尾的示例脚本，当我们要激活一个钩子时，只需要去掉后缀，改写脚本即可。脚本支持多种语言,可以在首行指定#!/usr/bin/env XXX 这些脚本只对当前项目发挥作用，而且每次都需要从其他仓库复制一份进来，我们可以修改全局的配置（以windows为例），在git安装目录下的mingw64\\share\\git-core\\templates\\hooks目录下存在着全局脚本，我们可以修改这里的脚本，这样每次init后当前仓库中的脚本就会从全局中复制过来。 客户端钩子 pre-commit: 执行git commit命令时触发，常用于检查代码风格 prepare-commit-msg: commit message编辑器呼起前default commit message创建后触发，常用于生成默认的标准化的提交说明 commit-msg: 开发者编写完并确认commit message后触发，常用于校验提交说明是否标准 post-commit: 整个git commit完成后触发，常用于邮件通知、提醒 applypatch-msg: 执行git am命令时触发，常用于检查命令提取出来的提交信息是否符合特定格式 pre-applypatch: git am提取出补丁并应用于当前分支后，准备提交前触发，常用于执行测试用例或检查缓冲区代码 post-applypatch: git am提交后触发，常用于通知、或补丁邮件回复（此钩子不能停止git am过程） pre-rebase: 执行git rebase命令时触发 post-rewrite: 执行会替换commit的命令时触发，比如git rebase或git commit –amend post-checkout: 执行git checkout命令成功后触发，可用于生成特定文档，处理大二进制文件等 post-merge: 成功完成一次 merge行为后触发 pre-push: 执行git push命令时触发，可用于执行测试用例 pre-auto-gc: 执行垃圾回收前触发 服务端钩子 pre-receive: 当服务端收到一个push操作请求时触发，可用于检测push的内容 update: 与pre-receive相似，但当一次push想更新多个分支时，pre-receive只执行一次，而此钩子会为每一分支都执行一次 post-receive: 当整个push操作完成时触发，常用于服务侧同步、通知 这些钩子有如下的关系(颜色深的为常用钩子)： 示例：提交后自动maven打包 如果目录下存在pom.xml则打包(需已安装maven)，不考虑打包失败的情况, 修改post-commit文件`text #!/bin/sh# An example hook script to prepare a packed repository for use overdumb transports.# To enable this hook, rename this file to “post-update”.if [ -f “pom.xml” ];then mvn clean -Dmaven.test.skip=true package installelse echo “[!Error]not found pom.xml” echo “==========================”fi`","tags":[{"name":"Git","slug":"Git","permalink":"https://cnkeep.github.io/tags/Git/"}]},{"title":"gitlab搭建","date":"2018-03-15T16:38:00.000Z","path":"2018/03/16/01-gitlab搭建/","text":"Docker方式安装gitlab 标签： Gitlab，Docker 开发中如果我们不希望代码托管在第三方平台，我们就可以自己搭建一套git服务端，这里采用流行的Gitlab和Docker搭建 1.安装gitlab1.1 环境介绍 Linux Centos, Linux localhost.localdomain 3.10.0-514.10.2.el7.x86_64 #1 SMP Fri Mar 3 00:04:05 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux docker Docker version 18.09.0, build 4d60db4 内存&amp;硬盘 内存&gt;4G, 硬盘&gt;20G 1.2 下载安装 1.镜像下载&amp;安装 拉取镜像1$ docker pull gitlab/gitlab-ce 建立容器映射文件夹 1234# 提前建立映射文件夹$ mkdir /home/gitlab/config$ mkdir /home/gitlab/data$ mkdir /home/gitlab/logs 启动容器 123456789101112131415$ docker run \\ --d \\ --privileged=true \\ --p 8443:443 \\ --p 8090:8090 \\ #web访问端口 --p 4422:4422 \\ #ssh访问端口 --name gitlab \\ --restart unless-stopped \\ -v /home/gitlab/config:/etc/gitlab \\ -v /home/gitlab/logs:/var/log/gitlab \\ -v /home/gitlab/data:/var/opt/gitlab \\ v /etc/localtime:/etc/localtime \\ gitlab/gitlab-ce:latest \\# 启动较为缓慢，需要等大约2分钟 2.配置端口123456789101112131415$ vi /home/gitlab/config/gitlab.rb## 修改http方式的端口密码 external_url &apos;http://172.16.22.135:8090&apos;## 修改ssh端口gitlab_rails[&apos;gitlab_shell_ssh_port&apos;] = 4422# 修改ssh端口$ docker exec -it gitlab /bin/bash$ vi /assets/sshd_config#修改ssh端口Port 4422#重启配置$gitlab-ctl reconfigure 参见：Gitlab SSH端口不生效 配置邮件服务 邮件服务配置 阿里云邮箱配置12345678910$ vi /home/gitlab/config/gitlab.rbgitlab_rails[&apos;smtp_enable&apos;] = truegitlab_rails[&apos;smtp_address&apos;] = &quot;smtp.mxhichina.com&quot;gitlab_rails[&apos;smtp_port&apos;] = 465gitlab_rails[&apos;smtp_user_name&apos;] = &quot;&lt;your user_name&gt;&quot;gitlab_rails[&apos;smtp_password&apos;] = &quot;&lt;your passwd&gt;&quot;gitlab_rails[&apos;smtp_domain&apos;] = &quot;&lt;your domain&gt;&quot;gitlab_rails[&apos;smtp_authentication&apos;] = &quot;login&quot;gitlab_rails[&apos;smtp_enable_starttls_auto&apos;] = truegitlab_rails[&apos;smtp_tls&apos;] = true 邮件服务测试12345678910111213141516171819202122232425262728293031$ docker exec -it gitlab /bin/bash#重启配置$gitlab-ctl reconfigureroot@8176a338c3ef:/# gitlab-rails console------------------------------------------------------------------------------------- GitLab: 11.6.5 (237bddc) GitLab Shell: 8.4.3 postgresql: 9.6.11-------------------------------------------------------------------------------------Loading production environment (Rails 5.0.7)irb(main):001:0&gt; Notify.test_email('1348555156@qq.com','subject','content').deliver_nowNotify#test_email: processed outbound mail in 173.5msSent mail to 1348555156@qq.com (2970.6ms)Date: Tue, 22 Jan 2019 14:40:57 +0000From: Gitlab &lt;zhangleili@cnkeep.cn&gt;Reply-To: Gitlab &lt;noreply@172.16.22.135&gt;To: 1348555156@qq.comMessage-ID: &lt;5c472b798ae1b_ae3fba61cca5f08927@8176a338c3ef.mail&gt;Subject: subjectMime-Version: 1.0Content-Type: text/html; charset=UTF-8Content-Transfer-Encoding: 7bitAuto-Submitted: auto-generatedX-Auto-Response-Suppress: All&lt;!DOCTYPE html PUBLIC \"-//W3C//DTD HTML 4.0 Transitional//EN\" \"http://www.w3.org/TR/REC-html40/loose.dtd\"&gt;&lt;html&gt;&lt;body&gt;&lt;p&gt;content&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;=&gt; #&lt;Mail::Message:70069490324460, Multipart: false, Headers: &lt;Date: Tue, 22 Jan 2019 14:40:57 +0000&gt;, &lt;From: Gitlab &lt;zhangleili@cnkeep.cn&gt;&gt;, &lt;Reply-To: Gitlab &lt;noreply@172.16.22.135&gt;&gt;, &lt;To: 1348555156@qq.com&gt;, &lt;Message-ID: &lt;5c472b798ae1b_ae3fba61cca5f08927@8176a338c3ef.mail&gt;&gt;, &lt;Subject: subject&gt;, &lt;Mime-Version: 1.0&gt;, &lt;Content-Type: text/html; charset=UTF-8&gt;, &lt;Content-Transfer-Encoding: 7bit&gt;, &lt;Auto-Submitted: auto-generated&gt;, &lt;X-Auto-Response-Suppress: All&gt;&gt;irb(main):002:0&gt;","tags":[{"name":"Git","slug":"Git","permalink":"https://cnkeep.github.io/tags/Git/"}]},{"title":"git的原理","date":"2018-03-15T15:27:00.000Z","path":"2018/03/15/01-git的原理/","text":"git的内部原理Git的内部原理&emsp;&emsp;Git本质上是一个内容寻址文件系统，从内部来看它是一个key-value数据库，可以插入任意内容，并返回一个键值，可以通过该键值在任何时候再取出该内容。接下来我们初始化一个本地仓库看看细节原理。 原理解析目录结构首先我们初始化一个本地仓库后，发现生成一个.git文件夹，git的存储和操作都是在操作这些文件(还记得它是一个文件系统吗)我们来看看目录结构以及每个目录的作用：12345678910111213141516├─HEAD 保存当前指向的分支├─config 保存当前项目的git配置选项├─index 保存暂存区信息├─COMMIT_EDITMSG 保存提交记录├─hooks├─logs 提交记录│ ├─HEAD│ └─refs├─info│ └─exclude├─objects git的核心数据存储│ ├─info│ └─pack└─refs 存储指向数据 (分支) 的提交对象的指针 ├─heads └─tags 上面这些目录中最重要的就是index和HEAD, objects和refs, logs这几个部分 原理分析接下来我们从add和commit命令来看git的运行原理。 为什么是内容寻址文件系统 &emsp;&emsp;我们前面提到Git是一个内容寻址文件系统，那么为什么这么说呢？其实Git是采用Key-Value数据存储原理，为每个数据通过SHA-1计算出一个键值key，采用key的前2位分目录创建目录，key的剩余部分为文件名，将原来的内容压缩作为文本内容存储。这样只要我们知道一个key就能轻易找到文件并还原出原文件内容，这就是.git/objects目录的内容结构。我们可以使用git cat-file -p key查看文件内容。 git的三个区域 介绍之前我们先要了解git相关的三个目录区域： 工作区(working directory): 对应我们真是操作的文件目录 暂存区(staged或者叫index): 对应.git/index 本地仓库(repository): 对应.git/objects目录它们之间的关系如图所示： git中的对象 Git中的对象有Commit, Tree, Blob, Tag四种对象，前三种最重要，我们先介绍着几个对象的作用，后面再讲Git是如何通过这些对象完成版本控制的。 Commit: 记录提交记录的信息，包含author, message, parent(前一个commit对象指针)，Tree对象指针 Tree: 用于记录目录结构的一组指针，包含文件类型，文件指针(Tree或者Blob),文件名 Blob: 存储压缩后的内容它们之间的关系如图所示： 正向操作 接下来就最常见的正常操作add，commit命令来看Git的工作原理12345678910111213141516171819202122232425262728293031323334353637383940414243$ git init #初始化一个本地仓库 $ vi readme.txt #创建一个文件$ This is readme. $ find .git/objects/ -type f #查看对象库，为空$ git ls-files --stage #查看暂存区信息，即.git/index内容$ git add readme.txt #文件放入暂存区，这一步会将readme.txt放入对象库并生成key存储在.git/index中$ find .git/objects/ -type f #查看对象库，生成新的文件.git/objects/52/cb6cdb81a64344370c918a301eb153035f915a$ git cat-file -t 52cb6cdb81a64344370c918a301eb153035f915a #查看对象类型blob #blob对象，存储文件内容$ git cat-file -p 52cb6cdb81a64344370c918a301eb153035f915a #查看文件内容This is readme. #文件内容与我们写入的一致$ git ls-file --stage #查看缓存区100644 52cb6cdb81a64344370c918a301eb153035f915a 0 readme.txt #记录着blob对象$ git commit -m &quot;add readme.txt&quot; #提交暂存区到本地仓库中$ find .git/objects/ -type f #查看对象库.git/objects/52/cb6cdb81a64344370c918a301eb153035f915a #Blob对象.git/objects/9b/6f349ad11c58d0e930dd12134a3e536bf5b057 #Tree对象.git/objects/d6/1479babc75f198416b5fd3caece3495b976391 #Commit对象$ vi readme.txtThis is readme.add line.$ git status #查看工作区和暂存区的文件状态，可以知道那些已经被暂存，那些没有On branch masterChanges not staged for commit: (use &quot;git add &lt;file&gt;...&quot; to update what will be committed) (use &quot;git checkout -- &lt;file&gt;...&quot; to discard changes in working directory) modified: readme.txtno changes added to commit (use &quot;git add&quot; and/or &quot;git commit -a&quot;)$ git add readme.txt $ git commit -m &quot;change readme.txt&quot;$ find .git/objects/ -type f #查看对象库.git/objects/3a/3cf98cad23cb084fa1753bee60e9f97d08e317 #Tree对象.git/objects/52/cb6cdb81a64344370c918a301eb153035f915a #Blob对象，readme.txt:v1.git/objects/62/c1331ab1c19f93dd776e894404aebc6de14b85 #Blob对象，readme.txt:v2.git/objects/9b/6f349ad11c58d0e930dd12134a3e536bf5b057 #Tree对象，.git/objects/a6/08354310f5aec4436a99feab92c96c35fa895c #Commit对象，commit:2.git/objects/d6/1479babc75f198416b5fd3caece3495b976391 #Commit对象, commit:1 通过上面的操作我们可以分析出原理： git add: 通过将文件生成新的Blob对象，并提交到暂存区，写.git/index文件 git commit:通过暂存区生成Tree对象，Commit对象，提交到版本库，写.git/logs目录 git会为每一个的更改都保存一个副本经过一系列操作，最终形成如下结构: 逆向操作 所谓逆向操作，就是指撤销之前的操作，来分析一下Git的处理原理, 即checkout和reset命令。我们以撤销readme.txt的修改为例。12345678910111213141516171819202122232425262728293031323334353637383940414243444546$ git log #查看当前的commit结点commit a608354310f5aec4436a99feab92c96c35fa895c (HEAD -&gt; master)Author: TIME69 &lt;zhangleili924@gmail.com&gt;Date: Wed Sep 19 01:48:36 2018 +0800 change readme.txtcommit d61479babc75f198416b5fd3caece3495b976391Author: TIME69 &lt;zhangleili924@gmail.com&gt;Date: Wed Sep 19 01:38:42 2018 +0800 add readme.txt$ git rest HEAD^ #将版本库回退到上一个版本，此操作只是将HEAD指针指向前一个Commit对象而已,并将暂存区会退到上一个版本Unstaged changes after reset: #可以看到撤销了暂存区的更改M readme.txt$ git log #查看当前Head, 看到上次更改撤销了commit d61479babc75f198416b5fd3caece3495b976391 (HEAD -&gt; master)Author: TIME69 &lt;zhangleili924@gmail.com&gt;Date: Wed Sep 19 01:38:42 2018 +0800 add readme.txt $ git ls-files --stage #查看暂存区100644 52cb6cdb81a64344370c918a301eb153035f915a 0 readme.txt$ git cat-file -p 52cb6cdb81a64344370c918a301eb153035f915a #暂存区内容已经回退，但是工作区呢？This is readme.$ git diff --stage #查看暂存区和版本库的差异，发现没有差异，是相同的$ cat readme.txt #发现工作区没有回退This is readme.add line.$ git status #On branch masterChanges not staged for commit: (use &quot;git add &lt;file&gt;...&quot; to update what will be committed) (use &quot;git checkout -- &lt;file&gt;...&quot; to discard changes in working directory) modified: readme.txtno changes added to commit (use &quot;git add&quot; and/or &quot;git commit -a&quot;)$ git checkout readme.txt #将工作区回退到上一个版本$ cat readme.txt #发现工作区内容变回来了This is readme. 看似很完美，但是这时候有人会说了那我不想撤销了咋办，我找不到记录了吗？怎么会，Git帮你存着呢。所有的提交记录Git都记录下来了(保存在.git/logs目录下的相应文件里)，不用怕丢，我们可以通过git reflog命令查看立即记录。1234$ git reflogd61479b (master) HEAD@&#123;6&#125;: reset: moving to HEAD^a608354 HEAD@&#123;7&#125;: commit: change readme.txtd61479b (master) HEAD@&#123;8&#125;: commit (initial): add readme.txt 我们知道Git操作的都是指针而已，那我们只要把HEAD指针指向下一个Commit结点不就完了，完美！1234$ git reset a608354 #暂存区Unstaged changes after reset:M readme.txt$ git checkout readme.txt #工作区 最终我们解决的问题，只要搞懂了底层机理是不是一下简单了许多，我们把工作区，暂存区，版本库之间的转换总结为下面这一张图： 内存压缩 我们上面说到git会对每一次的提交都保存一个副本Blob对象，那我们想一下那不是有很多冗余，很占存储空间，那能不能像svn那样存储变化的部分呢？其实Git已经考虑到了这一点，所以其内部有一个碎片处理的gc操作，类似于svn，但却有所区别。12345678910111213141516171819202122232425$ git gc #手动执行碎片处理Counting objects: 5, done.Delta compression using up to 4 threads.Compressing objects: 100% (3/3), done.Writing objects: 100% (5/5), done.Total 5 (delta 1), reused 0 (delta 0)$ find .git/objects/ -type f #查看对象库，发现pack目录多了文件，之前的object文件不见了.git/objects/info/packs.git/objects/pack/pack-57cbdc16b7fd157d77451245ef44ba68ea567e14.idx.git/objects/pack/pack-57cbdc16b7fd157d77451245ef44ba68ea567e14.pack$ git verify-pack -v .git/objects/pack/pack-57cbdc16b7fd157d77451245ef44ba68ea567e14.idx #查看pack文件a2d7df5381d7c3086ba998c6e7528725082e7e92 commit 218 157 12a608354310f5aec4436a99feab92c96c35fa895c commit 73 79 169 1 a2d7df5381d7c3086ba998c6e7528725082e7e92d61479babc75f198416b5fd3caece3495b976391 commit 167 125 24862c1331ab1c19f93dd776e894404aebc6de14b85 blob 26 33 373a06a3e45fb33f83522f2459baf4b2d6ebfc196bf tree 38 49 406ffd655096935b0b00e7eac2190ac0e61ea978e2e blob 33 40 4553a3cf98cad23cb084fa1753bee60e9f97d08e317 tree 38 49 4959b6f349ad11c58d0e930dd12134a3e536bf5b057 tree 38 48 54452cb6cdb81a64344370c918a301eb153035f915a blob 16 24 592non delta: 8 objectschain length = 1: 1 object.git/objects/pack/pack-57cbdc16b7fd157d77451245ef44ba68ea567e14.pack: ok 我们看看这两个文件是什么作用： .pack 是包文件，这个文件包含了从文件系统中移除的所有对象的内容 .idx是索引文件，这个文件包含了包文件的偏移信息值得注意的是，git不同于svn的是最后一个版本保存的是完成的文件内容，之前的版本保存的是差异部分，因为git认为这样更高效。 总结我们分析了Git的内部机理，我们来总结一下： Git是一个内容寻址文件系统, 会对每一份内容生成校验和，以便通过校验和再次获取原有数据 Git本质是一个Key-Value数据存储方式 Git存在三个区：工作区，暂存区，版本库，我们操作git就是操作这三个区域 Git理论上会针对每一次修改创建一个副本，但是为了减少存储空间会压缩采用存储差异 Git内部包含这些对象: Tag, Commit(记录提交信息，上一次提交的指针,Tree指针), Tree(记录文件列表，Tree+Blob), Blob(内容对象)","tags":[{"name":"Git","slug":"Git","permalink":"https://cnkeep.github.io/tags/Git/"}]},{"title":"ssh协议配置免密码提交","date":"2018-03-07T00:34:00.000Z","path":"2018/03/07/06-ssh协议配置免密码提交/","text":"ssh协议配置免密码提交 标签：Git, ssh, windows 介绍我们平时在使用git进行版本控制时，为了避免每次都输入密码，可以为账号添加ssh key,这样就可以避免输入账号密码的繁琐步骤。 配置ssh key 笔者这里介绍windows环境(linux环境配置类似)，笔者本地配置了多个远程仓库服务，所以这里是一种通用的配置方式 1.生成公钥123456789101112131415161718192021222324252627282930#任意目录打开git bash##为账号生成公钥$ ssh-keygen -t rsa -C &apos;1348555156@qq.com&apos;Generating public/private rsa key pair.##指定公钥文件的生成位置，这里配置为用户的主目录（前面括号中提示的位置,.ssh为ssh相关的配置目录），文件名随意指定Enter file in which to save the key (/c/Users/zll/.ssh/id_rsa): /c/Users/zll/.ssh/personal_rsa## 键入密码，可选Enter passphrase (empty for no passphrase):Enter same passphrase again:Your identification has been saved in /c/Users/zll/.ssh/personal_rsa.Your public key has been saved in /c/Users/zll/.ssh/personal_rsa.pub.The key fingerprint is:SHA256:e2fhKJwQ6vlWGjeHHXIUmoDefqnpGWBGt2XcieqiINs zhangleili@wxchina.comThe key&apos;s randomart image is:+---[RSA 2048]----+| .. .. || . o =.. || ...o B.o || ..o.*. o || =.+ S* .. || + +ooBooo . ||o + oO=oo + ||.+ . o=o o o ||. E o+ |+----[SHA256]-----+## 查看公钥$ cat /c/Users/zll/.ssh/personal_rsa.pubssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDRxhEozNNvtwL+CauYrs5fXznTvHpjzneNUG4RtEVrCkIxn8ld4W9VuFJ9UbC3cKIxB43svDiQqdnDPM08A8RfGDX0Jt686orkk7DAJerEEewoYollxcz79CHy17DhJ58yyz7zhVWW9sht/H6hEZuulliTtZIvtieH3xnL0zZVt/VWKk42pC+L/gEPWNDI0nkgmkVpKHaTxX503+Vh6tGJFf92Xg+t7UnqE+zz2WvZ4XTtX1Fli1y3ES7xS7ZwU5LwreDSrgT0u8EIk5HLCwcS5//9ZHJgkBGY3cdrgN0BUviNdWnYd4fI+4/Qb0C+acuMb/1ow1VxEKlE+6YMr6B7 XXXXXX@wxchina.com 2.服务端配置ssh key 登录到服务端，将上一步中的cat **.pub的输出内容粘贴复制到服务端 3.配置不同的服务端账号和地址 1.配置用户 1234567891011121314151617181920# 配置用户$ git config --global --add user.name &quot;&lt;username&gt;&quot;$ git config --global --add user.email &quot;&lt;email&gt;&quot;或者vi ~/.gitconfig##增加name和email[user] name = ******* email = ********* name = ******** email = *********[http] sslVerify = false[url &quot;https://&quot;] insteadOf = git://[credential] helper = store 2.配置服务端地址123456789101112131415161718192021222324252627#进入用户主目录，第一步中有提示$ cd /c/Users/zll/.ssh/#新建config配置文件$ touch config$ vi config# 配置github.comHost github.com User cnkeep HostName github.com IdentityFile C:\\Users\\zll\\.ssh\\id_rsa PreferredAuthentications publickey # 配置私有gitlabHost 172.16.22.135 User zhangleili #指定用户，在~/.gitconfig中配置的用户名 HostName 172.16.22.135 #指定服务器地址 Port 4422 #指定服务器端口(默认22) IdentityFile C:\\Users\\zll\\.ssh\\personal_rsa #指定公钥的文件 PreferredAuthentications publickey #指定验证策略# 码云Host gitee.com HostName gitee.com IdentityFile C:\\Users\\zll\\.ssh\\mayun_rsa PreferredAuthentications publickey 3.测试配置是否可用123456$ ssh -T git@github.comWarning: Permanently added the RSA host key for IP address &apos;52.74.223.119&apos; to the list of known hosts.Hi cnkeep! You&apos;ve successfully authenticated, but GitHub does not provide shell access.$ ssh -T git@172.16.22.135 Welcome to GitLab, @cnkeep! 4.ssh方式拉取项目12345678910$ git clone ssh://git@172.16.22.135:4422/developer/test.gitCloning into &apos;test&apos;...The authenticity of host &apos;[172.16.22.135]:4422 ([172.16.22.135]:4422)&apos; can&apos;t be established.ECDSA key fingerprint is SHA256:yvJ524f2Cxd60O4onSsE4K8TAtgWQISzWM+g6wi+H7Y.Are you sure you want to continue connecting (yes/no)? yesWarning: Permanently added &apos;[172.16.22.135]:4422&apos; (ECDSA) to the list of known hosts.remote: Enumerating objects: 3, done.remote: Counting objects: 100% (3/3), done.remote: Total 3 (delta 0), reused 0 (delta 0)Receiving objects: 100% (3/3), done. 结束语 这些都是笔者在搭建私有gitlab是亲自测试过的操作，可能因为不同环境有所差异。如有错误欢迎指正！","tags":[{"name":"Git","slug":"Git","permalink":"https://cnkeep.github.io/tags/Git/"}]},{"title":"Https协议配置免输密码","date":"2018-03-05T21:29:00.000Z","path":"2018/03/06/05-Https协议配置免输密码/","text":"Https方式clone的项目每次都要输入用户名密码使用git从远程仓库clone下来的项目时，连接如果是https://, 而不是git@git (ssh)的形式时，我们每次git pull/push到远程仓库时，总提示需要输入账号和密码，太麻烦了，有没有办法呢？ 有！ 解决方案： 12git bash进入项目目录，输入：git config --global credential.helper store 完成配置后，再操作一次git pull, 然后提示输入账号密码，这次输入后就不需要再次输入密码了！","tags":[{"name":"Git","slug":"Git","permalink":"https://cnkeep.github.io/tags/Git/"}]},{"title":"提交规范","date":"2018-03-04T17:03:00.000Z","path":"2018/03/05/04-提交规范/","text":"Git的提交规范参考： 阮一峰-Commit message 和 Change log 编写指南 并非严格按照此规范执行，可以实际情况实际定制规则，主要为了规范化。 提交格式12345&lt;type&gt;(&lt;scope&gt;) : &lt;subject&gt; &lt;空行&gt; &lt;body&gt; &lt;空行&gt; &lt;footer&gt; 其中 type 的值可以有很多，下面有几个我们常用到的 feature :新功能 fixbug:修复bug doc : 文档改变 style : 代码格式改变 refactor :某个已有功能重构 performance:性能优化 test :增加测试 chore: 修改了改变构建流程、或者增加依赖库、工具等 build :改变了build工具 如 grunt换成了 npm revert: 撤销上一次的 commit scope :用来说明此次修改的影响范围 可以随便填写任何东西,我推荐使用下列 all ：表示影响面大 ，如修改了网络框架 会对真个程序产生影响 loation： 表示影响小，某个小小的功能 module：表示会影响某个模块 如登录模块、首页模块 、用户管理模块等等 subject: 用来简要描述本次改动，概述就好了body:具体的修改信息 应该尽量详细footer: 附加信息，例如fix #*;issue #;ref #*** 1234567891011示例： [feature](user): 新增用户管理 description: 针对用户…… 示例： [fixbug](sqlmapper): issue #1252 sql error desctiption: 修复sql问题 使用插件校验提交信息(validate-commit-msg)依赖：安装npm, 安装node, ghooks, validate-commit-msg 1.跳转到项目根目录下 2.安装ghooks123456npm install ghooks --save-dev``` &gt; 3.安装[validate-commit-msg](https://github.com/conventional-changelog-archived-repos/validate-commit-msg) ```textnpm install --save-dev validate-commit-msg 4.生成package.json1npm init --yes 5.添加hooks配置123456在package.json中增加：&quot;config&quot;: &#123; &quot;ghooks&quot;: &#123; &quot;commit-msg&quot;: &quot;validate-commit-msg&quot; &#125; &#125; 6.测试 规范化辅助插件(commitizen) 1.全局安装commitizennode模块1npm install -g commitizen 2.在项目目录下运行命令1commitizen init cz-conventional-changelog --save --save-exact 3.此时可能会报找不到package.json的错误,使用下面命令来自动生成一个项目的package,然后在运行2中的命令.1npm init --yes 4.运行完以上一律使用git cz 代替git commit来提交代码,同时会显示一下选项来自动生成符合格式的commit message.123456789101112131415$ git czcz-cli@2.10.1, cz-conventional-changelog@2.1.0Line 1 will be cropped at 100 characters. All other lines will be wrapped after 100 characters.? Select the type of change that you&apos;re committing: (Use arrow keys)&gt; feat: A new feature fix: A bug fix docs: Documentation only changes style: Changes that do not affect the meaning of the code (white-space, formatting, missing semi-colons, etc) refactor: A code change that neither fixes a bug nor adds a feature perf: A code change that improves performance test: Adding missing tests or correcting existing tests(Move up and down to reveal more choices)","tags":[{"name":"Git","slug":"Git","permalink":"https://cnkeep.github.io/tags/Git/"}]},{"title":"git的常见命令","date":"2018-03-03T16:42:00.000Z","path":"2018/03/04/03-git的常见命令/","text":"Git的简单使用前言&emsp;&emsp;开发过程当中难以避免的要使用到版本控制工具，以前使用的SVN, 但随着Git的出现，它以及其优良的特性迅速火热，本节我们就来简单了解一下。 介绍&emsp;&emsp;Git是一个开源的分布式版本控制系统(本质是一个内容寻址文件系统，后面章节会做介绍)，用于敏捷高效地处理任何或小或大的项目。Git 与常用的版本控制工具 CVS, Subversion 等不同，它采用了分布式版本库的方式，不必服务器端软件支持（这点很重要，他可以在服务端无法访问的时候提交到本地仓库，等网络恢复后推送到远程仓库）。 Git 与 SVN的区别 简单使用初始化命令 init 初始化仓库1$ git init clone 克隆仓库12$ git clone https://github.com/repname$ git clone https://github.com/repname myrep #myrep作为本地仓库名 config 配置1234$ git config --list #查看配置$ git config --system user.name #系统级配置$ git config --global user.name #全局配置，系统用户级$ git config user.name #仓库级配置 remote 远程仓库12345$ git remote -v #查看远程仓库详细信息$ git remote show origin #查看远程仓库$ git remote add pb https://github.com/pb #添加远程仓库映射$ git remote rename pb paul #重命名远程仓库映射$ git remote rm paul #移除远程仓库映射关系 ### 基础命令 status 查看文件状态123$ git status$ git status -s #状态简览$ git ls-files --stage #查看缓存区索引文件内容，即.git/index add 暂存已修改文件12$ git add filename$ git add -A #暂存所有已修改文件 commit 提交更新12$ git commit -m &quot;commit message&quot;$ git commit -a -m &quot;commit message&quot; #跳过暂存区，直接提交 push 推送到远程仓库12$ git push$ git push origin master #将本地master分支推动到远程origin仓库 fetch 从远程仓库拉取数据1$ git fetch [remote-name] merge 合并分支123$ git merge --no-ff #推荐的合并方式，会做作一个新的提交，便于历史查询 $ git merge hotfix #把 hotfix 分支，合并到当前分支$ git mergetool #图形化解决冲突的工具 pull ( fetch + merge ) 常用命令 diff 查看修改12$ git diff #比较 暂存区－工作区$ git diff --staged #比较 仓库－暂存区 log 查看提交历史1234567$ git log$ git log --stat #展示提交的简略统计信息$ git log --oneline #简要显示$ git log --grep #类似grep命令，支持正则查找$ git log --since/after/before/until #只是时间点查找$ git log $ git reflog #回退版本后看不到之后的历史记录，此命令可以完成该功能 撤销操作12345$ git commit --amend #重新提交$ git reset HEAD filename #取消暂存的文件$ git reset -soft #暂存区-&gt;工作区, 类似于checkout$ git reset --mixed #版本库-&gt;暂存区 $ git reset --hard #版本库-&gt;暂存区-&gt;工作区 checkout1234567$ git checkout &lt;filename&gt;# 会覆盖工作区文件# 如果暂存区有改动的文件，则从暂存区到工作区# 如果暂存区无改动的文件，则从仓库到工作区#新建分子并切换至新分支$ git checkout -b &lt;branch_name&gt; tag 打标签1234567$ git tag #查看标签$ git tag -a v1.4 -m &quot;my version 1.4&quot; #创建附注标签$ git tag v1.4 #创建轻量标签$ git tag -a v1.2 9fceb02 #对某次提交后期打标签$ git push origin v1.5 #上传某个标签，GIT 默认不会 push 标签到远程仓库$ git push origin --tags #上传所有不在远程仓库的标签$ git checkout -b version2 v2.0.0 #检出标签 rm 移除文件12$ git rm filename #个人感觉效果同 rm$ git rm --cached filename #移除暂存区中的文件 分支命令 branch 创建分支123456789$ git branch #查看分支，前面带星号*的，是当前分支$ git branch testing #创建 testing 分支$ git branch -d testing #删除 testing 分支$ git branch -v #查看每个分支最后一次提交$ git branch --merged #查看已合并到当前分支的分支$ git branch --no-merged #查看未合并到当前分支的分支$ git branch -r #查看远程分支$ git push origin &lt;local_branch&gt;:&lt;remote_branch&gt; #推送本地分支到远程分支，不存在时新建远程分支$ git push origin :&lt;remote_branch&gt; #与上一条不同的时本地分支留空了，这将会删除远程分支 checkout 切换分支12$ git checkout testing$ git checkout -b iss53 #创建分支，并切换到新创建的分支 底层命令 cat-file 读取 GIT 仓库对象1234$ git cat-file -p f8a67de1d4bf0d6dbaaaf8990ffe8394e5fa88ee #查看对象内容$ git cat-file -p master^&#123;tree&#125; #master 分支上最新的提交所指向的 tree 对象$ git cat-file -t f8a67de1d4bf0d6dbaaaf8990ffe8394e5fa88ee #查看对象类型$ git cat-file -s f8a67de1d4bf0d6dbaaaf8990ffe8394e5fa88ee #查看对象大小 gc 生成包文件1234$ git gc#作用：完整保存最新版文件，历史版本文件保存差异#GIT 会根据情况自己执行，一般不需要手动之行$ git verify-pack -v .git/objects/pack/pack-57cbdc16b7fd157d77451245ef44ba68ea567e14.idx 查看对象 $ find .git/object/ -type f #所有对象列表 $ git rev-list --objects --all #blob列表","tags":[{"name":"Git","slug":"Git","permalink":"https://cnkeep.github.io/tags/Git/"}]},{"title":"git和svn有什么区别","date":"2018-03-01T16:40:00.000Z","path":"2018/03/02/02-git和svn有什么区别/","text":"Git和Svn的区别以前有Svn这种工具来进行版本控制，为什么还要用Git呢，两者有什么区别? 其实Git和Svn都是版本控制工具，但是Git更倾向于分布式，而且效率高，功能更强大。 Git和Svn的主要差别： 在Git 中的绝大多数操作都只需要访问本地文件和资源，不必联网就可以看到所有的历史版本记录，而SVN 却需要联网。 &nbsp;&nbsp;因为 Git 在本地磁盘上就保存着所有当前项目的历史更新，所以处理起来速度飞快，但我们需要浏览项目的历史更新摘要，Git 不用跑到外面的服务器上去取数据回来，而直接从本地数据库读取后展示给你看。如果想要看当前版本的文件和一个月前的版本之间有何差异，Git 会取出一个月前的快照和当前文件作一次差异运算。 SVN 断开网络或者断开VPN就无法commit代码，但是Git 可以先commit到本地仓库, svn断网就傻了 Git 克隆一个完整项目的速度非常快，SVN 非常慢。 Git 只关心文件数据的整体是否发生变化，而SVN这类版本控制系统则只关心文件内容的具体差异。 &nbsp;&nbsp;这类系统（如SVN）每次记录有哪些文件作了更新，以及都更新了哪些行的什么内容，然而Git 并不保存这些前后变化的差异数据。实际上，Git更像是把变化的文件作快照后，记录在一个微型的文件系统中。每次提交更新时，它会纵览一遍所有文件的指纹信息并对文件作一快照，然后保存一个指向这次快照的索引。为提高性能，若文件没有变化，Git 不会再次保存，而只对上次保存的快照作一链接。 Git分支管理比svn强大太多 在 SVN 这类的版本控制系统上，分支（branch）是一个完整的目录，且这个目录拥有完整的实际文件。如果工作成员想要开启新的分支，那将会影响“全世界”！每个人都会拥有和你一样的分支。如果你的分支是用来对系统模块进行安全检查测试的，那将会像传染病一样，你改一个分支，还得让其他人重新切分支重新下载，而且这些代码很可能对稳定版本还是具有破坏性的。 在 Git上，每个工作成员可以任意在自己的本地版本库开启无限个分支。举例：当我想尝试破坏自己的程序（安检测试），并且想保留这些被修改的文件供日后使用，我可以开一个分支，做我喜欢的事。完全不需担心妨碍其他工作成员。只要我不合并及提交到主要版本库，没有一个工作成员会被影响。等到我不需要这个分支时， 我只要把它从我的本地版本库删除即可，无痛无痒。","tags":[{"name":"Git","slug":"Git","permalink":"https://cnkeep.github.io/tags/Git/"}]},{"title":"git是什么","date":"2018-03-01T13:23:00.000Z","path":"2018/03/01/01-git是什么/","text":"Git是什么？ 作为一个Coder, 在编写代码过程中不可避免的要提交和修改文件，如何没有版本控制工具，我们想象一下，不小心删除了代码，恢复不了，昨天改了哪些，总会遇到这样对我问题，哪有没有一种工具能保存我们的历史修改记录，方便我们自由的切换到任意一个版本呢？ &nbsp;&nbsp;Git就是这样一个分布式版本控制工具, 能帮助我们记录文件的历史修改记录，可以自由的切换到文件的历史版本，再也不怕文件丢了！当然git远不止做了这些，它还提供了各种对比，合并，统计的功能。","tags":[{"name":"Git","slug":"Git","permalink":"https://cnkeep.github.io/tags/Git/"}]}]